{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lia\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797\n",
      "1797\n"
     ]
    }
   ],
   "source": [
    "print(len(digits.data))\n",
    "print(digits.target.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "wine = fetch_openml(name='wine', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n",
      "178\n"
     ]
    }
   ],
   "source": [
    "print(len(wine.data))\n",
    "print(wine.target.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of wine data via division of each feature by its max value\n",
    "digits_data_norm = []\n",
    "for col in digits.data:\n",
    "    col_norm = col/np.max(col)\n",
    "    digits_data_norm.append(col_norm)\n",
    "\n",
    "digits.data = np.asarray(digits_data_norm)\n",
    "\n",
    "\n",
    "wine_data_norm = []\n",
    "for col in wine.data.T:\n",
    "    col_norm = col/np.amax(col)\n",
    "    wine_data_norm.append(col_norm)\n",
    "    \n",
    "wine.data = np.asarray(wine_data_norm).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation for digits dataset\n",
    "\n",
    "digitsTrainingSetSize = int(np.ceil(0.8 * len(digits.data)))\n",
    "digitsValidationSetSize = int(len(digits.data) - digitsTrainingSetSize)\n",
    "\n",
    "xDigitsTrainingSets = []\n",
    "yDigitsTrainingSets = []\n",
    "xDigitsValidationSets = []\n",
    "yDigitsValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data[foldIndex*digitsValidationSetSize:((foldIndex*digitsValidationSetSize)+digitsValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(digits.target[index+(foldIndex*digitsValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(digits.target[index])\n",
    "            \n",
    "    xDigitsTrainingSets.append(xTrainingSet)\n",
    "    yDigitsTrainingSets.append(yTrainingSet)\n",
    "    xDigitsValidationSets.append(xValidationSet)\n",
    "    yDigitsValidationSets.append(yValidationSet)\n",
    "    \n",
    "# 5-fold cross validation for wine dataset\n",
    "\n",
    "wineTrainingSetSize = int(np.ceil(0.8 * len(wine.data)))\n",
    "wineValidationSetSize = int(len(wine.data) - wineTrainingSetSize)\n",
    "\n",
    "xWineTrainingSets = []\n",
    "yWineTrainingSets = []\n",
    "xWineValidationSets = []\n",
    "yWineValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "    for index, data in enumerate(wine.data[foldIndex*wineValidationSetSize:((foldIndex*wineValidationSetSize)+wineValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(wine.target[index+(foldIndex*wineValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "    \n",
    "    for index, data in enumerate(wine.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(wine.target[index])\n",
    "            \n",
    "    xWineTrainingSets.append(xTrainingSet)\n",
    "    yWineTrainingSets.append(yTrainingSet)\n",
    "    xWineValidationSets.append(xValidationSet)\n",
    "    yWineValidationSets.append(yValidationSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of y for digits dataset\n",
    "\n",
    "numberOfDigitsTargets = 10\n",
    "numberOfWineTargets = 3\n",
    "\n",
    "for index, fold in enumerate(yDigitsTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yDigitsValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsValidationSets[index] = encodedFold\n",
    "\n",
    "# one-hot encoding of y for wine dataset\n",
    "\n",
    "for index, fold in enumerate(yWineTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yWineValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineValidationSets[index] = encodedFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomIndices(arr, batch_size):\n",
    "    indices = []\n",
    "    \n",
    "    if batch_size > len(arr):\n",
    "        print(\"Error: batch size larger than size of dataset.\")\n",
    "        return\n",
    "    \n",
    "    while batch_size > 0:\n",
    "        x = np.floor(np.random.random() * len(arr))\n",
    "        if x not in indices:\n",
    "            indices.append(int(x))\n",
    "            batch_size -= 1\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent class\n",
    " \n",
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, batch_size, learning_rate=0.5, momentum=0.9, max_iterations=20, epsilon=1e-8, iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iterations = max_iterations\n",
    "        self.epsilon = epsilon\n",
    "        self.deltas = []\n",
    "        \n",
    "        self.iters = iters\n",
    "        \n",
    "    def run(self, gradient_fn, x, y, w):\n",
    "        print(\"##############################\")\n",
    "        t = 1\n",
    "        \n",
    "        iterations = 0\n",
    "        eps = True\n",
    "        \n",
    "        weight_history = []\n",
    "        error_history = []\n",
    "        \n",
    "        gradients = []\n",
    "        \n",
    "        for number_of_targets in range(len(y[0])):\n",
    "            weight_history.append([])\n",
    "            error_history.append([])\n",
    "            gradients.append(np.inf)\n",
    "        \n",
    "        while eps and iterations < self.max_iterations and t < self.iters:\n",
    "            gradients = gradient_fn(x, y, w, self.batch_size)\n",
    "            \n",
    "            for c in range(len(y[0])):\n",
    "                if np.linalg.norm(gradients[c]) < self.epsilon:\n",
    "                    eps = False\n",
    "                    break\n",
    "            \n",
    "            for c in range(len(y[0])):\n",
    "                if(t==1):\n",
    "                    w[c] = w[c] - self.learning_rate * gradients[c]\n",
    "                else:\n",
    "                    delta_w = (self.momentum)*(self.deltas[-(len(y[0]))]) + (1-self.momentum)*gradients[c]\n",
    "                    w[c] = w[c] - (self.learning_rate)*(delta_w)\n",
    "                self.deltas.append(w[c])\n",
    "            \n",
    "            a = np.asarray(x)\n",
    "            b = np.asarray(w)\n",
    "\n",
    "#             if self.add_bias:\n",
    "#                 x = np.column_stack([x,np.ones(N)])\n",
    "    \n",
    "            yh=[]\n",
    "            for i, x_c in enumerate(a):\n",
    "                yh_x=[]\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  b[c] @ x_c\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  b[i] @ x_c\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "                    yh_x.append(yh_c)\n",
    "                yh.append(yh_x)\n",
    "                \n",
    "            step_cost = 0\n",
    "                \n",
    "            def cost(yh, y):\n",
    "                return y * np.log1p(np.exp(-yh)) + (1-yh) * np.log1p(np.exp(yh))\n",
    "                \n",
    "            for sample_index, yh_x in enumerate(yh):\n",
    "                c = np.argmax(y[sample_index])\n",
    "                cst = cost(yh_x[c], y[sample_index][c])\n",
    "                step_cost += cst\n",
    "            \n",
    "            print(step_cost)\n",
    "            for c in range(len(b)):\n",
    "                weight_history[c].append(w[c])\n",
    "                error_history.append(step_cost)\n",
    "            \n",
    "            if t > 1 and error_history[-1] > error_history[-2]:\n",
    "                iterations += 1\n",
    "            else:\n",
    "                iterations = 0\n",
    "                \n",
    "            t += 1\n",
    "            \n",
    "        #TODO return best w\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, add_bias=True):\n",
    "        self.add_bias = add_bias\n",
    "        pass\n",
    "            \n",
    "    def fit(self, x, y, optimizer):\n",
    "        # TODO: add bias\n",
    "        \n",
    "        def gradient(x, y, w, batch_size):\n",
    "            gradients = np.zeros(len(w)).tolist()\n",
    "\n",
    "            indices = getRandomIndices(x, batch_size)\n",
    "\n",
    "            for index in indices:\n",
    "                a = np.asarray(x[index])\n",
    "                b = np.asarray(y[index])\n",
    "\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  w[c] @ a\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  w[i] @ a\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "\n",
    "                    y_c = b[c]\n",
    "                    \n",
    "                    cost_c = np.dot(yh_c - y_c, a)\n",
    "                    \n",
    "                    gradients[c] += cost_c\n",
    "\n",
    "            return gradients\n",
    "        \n",
    "        w0 = []\n",
    "        for c in range(len(y[0])):\n",
    "            w0.append(np.zeros(len(x[0])))\n",
    "            \n",
    "        self.w = optimizer.run(gradient, x, y, w0)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        a = np.asarray(x)\n",
    "        b = np.asarray(self.w)\n",
    "        \n",
    "#       if self.add_bias:\n",
    "#           x = np.column_stack([x,np.ones(N)])\n",
    "\n",
    "        yh=[]\n",
    "        for i, x_c in enumerate(a):\n",
    "            yh_x=[]\n",
    "            for c in range(len(b)):\n",
    "                w_x =  b[c] @ x_c\n",
    "                num = np.exp(w_x)\n",
    "\n",
    "                den = 0\n",
    "                for i in range(len(b)):\n",
    "                    w_x =  b[i] @ x_c\n",
    "                    den += np.exp(w_x)\n",
    "\n",
    "                yh_c = num/den\n",
    "                yh_x.append(yh_c)\n",
    "            yh.append(yh_x)\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "1883.612813811298\n",
      "1877.939438410482\n",
      "1872.685284731791\n",
      "1867.023763082022\n",
      "1860.4249530262375\n",
      "1852.7979845051866\n",
      "1846.8937316999413\n",
      "1841.4775920159782\n",
      "1834.3462872901678\n",
      "1826.6090613080485\n",
      "1819.2568471012619\n",
      "1815.0733049444636\n",
      "1806.4076806606301\n",
      "1795.2850455477044\n",
      "1790.5597508455544\n",
      "1783.9605360508945\n",
      "1775.8446540126556\n",
      "1768.520966231943\n",
      "1761.655104853976\n",
      "1755.376367983873\n",
      "1748.1248705465548\n",
      "1740.5170865299326\n",
      "1736.6481460858604\n",
      "1733.055018515412\n",
      "1725.3534447687225\n",
      "1714.4827051501802\n",
      "1719.055371372735\n",
      "1711.5302783185916\n",
      "1704.9575969856728\n",
      "1698.599335767757\n",
      "1686.6038947393922\n",
      "1679.980714297988\n",
      "1669.5714951218165\n",
      "1663.7811106809036\n",
      "1653.0562500819549\n",
      "1656.0579853853383\n",
      "1651.0625910099352\n",
      "1642.6999250278168\n",
      "1625.1092738779175\n",
      "1613.8324870008123\n",
      "1602.0805418978844\n",
      "1601.8834647498184\n",
      "1596.171516016506\n",
      "1599.1429446595116\n",
      "1595.6772074578078\n",
      "1581.8570305355101\n",
      "1568.4231230555938\n",
      "1562.9081965322573\n",
      "1559.0344229958725\n",
      "1549.7650246525166\n",
      "1546.8504677231886\n",
      "1541.2374410869186\n",
      "1531.8663659797282\n",
      "1527.0071172559578\n",
      "1522.6727593645574\n",
      "1515.6726434659909\n",
      "1504.9133244521086\n",
      "1500.5368259522447\n",
      "1495.7158468113844\n",
      "1491.7621614225652\n",
      "1491.4440507139377\n",
      "1482.2052903283109\n",
      "1478.5329212066802\n",
      "1477.5749640974143\n",
      "1468.109164670803\n",
      "1463.7581534988235\n",
      "1467.1830366934034\n",
      "1463.1158420710844\n",
      "1455.880931290438\n",
      "1454.6007192626312\n",
      "1445.4455534752074\n",
      "1436.4362078436998\n",
      "1434.044064150284\n",
      "1425.4595678915778\n",
      "1425.6635645423564\n",
      "1418.2485543436708\n",
      "1407.2744381089612\n",
      "1407.7556516598256\n",
      "1400.422610752279\n",
      "1401.3433160330512\n",
      "1401.0866341137314\n",
      "1393.9351420346334\n",
      "1385.9222637750547\n",
      "1378.1164223191315\n",
      "1377.5654467578993\n",
      "1372.9037327821773\n",
      "1367.784107806172\n",
      "1361.1452936556395\n",
      "1357.7461421394137\n",
      "1353.521522074507\n",
      "1348.6614666221321\n",
      "1347.7570465159388\n",
      "1345.2944916259191\n",
      "1341.142524322368\n",
      "1341.790001991402\n",
      "1338.1839619739148\n",
      "1330.532723520215\n",
      "1330.3378535786146\n",
      "1329.4021471478934\n",
      "1322.257990822008\n",
      "1316.264877246217\n",
      "1317.8295217652355\n",
      "1310.764803132579\n",
      "1315.0293657461382\n",
      "1308.202988574995\n",
      "1303.9375515703823\n",
      "1297.2717058067146\n",
      "1297.1894347467332\n",
      "1293.8977549358292\n",
      "1291.953900394265\n",
      "1290.2936463699994\n",
      "1285.5830140118137\n",
      "1277.9064507979938\n",
      "1271.726853773302\n",
      "1272.8918935404722\n",
      "1269.8511652514653\n",
      "1267.8528145128887\n",
      "1264.8190351361739\n",
      "1265.9100292638877\n",
      "1261.348256520625\n",
      "1259.4467392440758\n",
      "1259.3908178949491\n",
      "1258.9482424974296\n",
      "1257.4796664745866\n",
      "1256.0963197210451\n",
      "1255.0368263640187\n",
      "1246.8760403311082\n",
      "1250.8252046358282\n",
      "1246.7414684104726\n",
      "1249.1016603454793\n",
      "1244.08211300006\n",
      "1235.6871330512301\n",
      "1235.5606038490405\n",
      "1231.7084061326352\n",
      "1228.1594554853423\n",
      "1229.8125076779156\n",
      "1226.2257914167096\n",
      "1224.628984516699\n",
      "1223.4540547301347\n",
      "1221.7980944486642\n",
      "1216.602208062861\n",
      "1217.7184887081357\n",
      "1218.119284048328\n",
      "1220.9924671098183\n",
      "1214.99683328104\n",
      "1210.7228979905647\n",
      "1213.1150119935428\n",
      "1209.796731462121\n",
      "1207.7188723873837\n",
      "1202.902163508665\n",
      "1196.595378587056\n",
      "1196.0432339747435\n",
      "1198.1566243322416\n",
      "1197.9474804723332\n",
      "1194.9854177147001\n",
      "1192.0183780215789\n",
      "1188.1466609010229\n",
      "1184.9205729426394\n",
      "1183.1706316479062\n",
      "1185.5686963941125\n",
      "1183.693371731597\n",
      "1182.79253351308\n",
      "1181.471351064183\n",
      "1176.6142547252116\n",
      "1177.7366562425898\n",
      "1174.6701566009508\n",
      "1174.3603964761194\n",
      "1173.2744336767462\n",
      "1173.9376380394524\n",
      "1180.774377179363\n",
      "1174.5765611456643\n",
      "1169.250148826222\n",
      "1167.961360503195\n",
      "1167.7129815666697\n",
      "1166.439854762213\n",
      "1168.9035354927455\n",
      "1170.6026767129122\n",
      "1167.5187110514464\n",
      "1172.506952927194\n",
      "1166.218712795495\n",
      "1160.9523914776892\n",
      "1159.2363440856097\n",
      "1157.428232387606\n",
      "1157.7716866925093\n",
      "1157.270872176966\n",
      "1161.270930167804\n",
      "1161.0628590525353\n",
      "1153.4962608609906\n",
      "1155.0903523762383\n",
      "1150.7038337018446\n",
      "1151.3458278608932\n",
      "1155.2675428555276\n",
      "1152.708128603676\n",
      "1147.6917534606728\n",
      "1149.6363188265\n",
      "1149.1631097618479\n",
      "1145.437124539791\n",
      "1137.9846334000065\n",
      "1135.9216396883085\n",
      "1135.2908885703505\n",
      "1131.7821820899785\n",
      "1132.3775408063855\n",
      "1132.0972149093388\n",
      "1130.1311457914142\n",
      "1127.6902423127979\n",
      "1128.8101996504763\n",
      "1128.1804444090699\n",
      "1126.390087123823\n",
      "1128.5646045995557\n",
      "1124.8785043911842\n",
      "1125.0033898754693\n",
      "1124.7293440123171\n",
      "1125.1749081885016\n",
      "1126.4923825590936\n",
      "1127.5986333601145\n",
      "1125.111521980634\n",
      "1126.2630407396425\n",
      "1129.4711224681148\n",
      "1129.37854077225\n",
      "1126.3962775581176\n",
      "1134.3925769340601\n",
      "1121.6259016934314\n",
      "1122.5798446910806\n",
      "1122.1135182138223\n",
      "1125.1709598899588\n",
      "1118.17703549228\n",
      "1113.6836841284935\n",
      "1112.479394153187\n",
      "1109.305637312694\n",
      "1109.8027590698568\n",
      "1111.1359537636708\n",
      "1108.482748642892\n",
      "1108.472659178202\n",
      "1106.3890213015572\n",
      "1103.6137210962931\n",
      "1102.9097476798722\n",
      "1101.4348237035654\n",
      "1099.2538963915624\n",
      "1096.8437554414404\n",
      "1096.2728517010332\n",
      "1097.5089271754289\n",
      "1097.5176586397954\n",
      "1096.7028257996535\n",
      "1098.83685869303\n",
      "1100.1058745095042\n",
      "1096.632287848811\n",
      "1095.799915689267\n",
      "1095.6431855163562\n",
      "1093.7294260489255\n",
      "1091.182469271052\n",
      "1089.4436952803378\n",
      "1088.1934776968963\n",
      "1089.49706580303\n",
      "1088.350854126847\n",
      "1089.4144516074398\n",
      "1085.53393227263\n",
      "1088.7450843930774\n",
      "1084.6663214030386\n",
      "1082.7547589756568\n",
      "1081.8118703963119\n",
      "1084.480639193959\n",
      "1085.1885718582905\n",
      "1081.6971027185239\n",
      "1080.5296230286372\n",
      "1079.5003311311655\n",
      "1079.8935276486623\n",
      "1080.1044105532976\n",
      "1084.5572914525949\n",
      "1080.8874433553735\n",
      "1082.2732873547818\n",
      "1079.7179085081893\n",
      "1075.7858329254227\n",
      "1076.9569081879722\n",
      "1076.4813345455584\n",
      "1080.231184312587\n",
      "1078.2547979237886\n",
      "1077.8650788097445\n",
      "1075.155810094987\n",
      "1073.6084464910239\n",
      "1074.9588226885762\n",
      "1073.3795540499864\n",
      "1075.0593692349191\n",
      "1073.7167087282596\n",
      "1076.5042157651744\n",
      "1074.1348955254655\n",
      "1074.3283928196224\n",
      "1072.2698277476504\n",
      "1069.582992105168\n",
      "1067.436348609015\n",
      "1067.9337397374543\n",
      "1066.7488648231526\n",
      "1065.9302631577023\n",
      "1065.7346845982056\n",
      "1064.682054637481\n",
      "1065.6713896127703\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-55bc49e298dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mlogisticRegressionModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mlogisticRegressionModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myDigitsTrainingSets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradientDescentModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0myh_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogisticRegressionModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxDigitsTrainingSets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0myh_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogisticRegressionModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxDigitsValidationSets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9975998cfbfd>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, optimizer)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mw0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-5f26cdad902b>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, gradient_fn, x, y, w)\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                         \u001b[0mw_x\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mx_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                         \u001b[0mden\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0myh_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def accurate(a, b):\n",
    "    return np.argmax(a) == np.argmax(b)\n",
    "\n",
    "def cost(yh, y):\n",
    "    return y * np.log1p(np.exp(-yh)) + (1-yh) * np.log1p(np.exp(yh))\n",
    "\n",
    "# TODO: grid-search to find lowest cost combination of model hyper-parameters\n",
    "\n",
    "batch_size = 30\n",
    "learning_rate = 0.01\n",
    "momentum = 0.2\n",
    "\n",
    "digits_training_accuracy = 0\n",
    "digits_training_cost = 0\n",
    "digits_validation_accuracy = 0\n",
    "digits_validation_cost = 0\n",
    "\n",
    "for fold_index, fold in enumerate(xDigitsTrainingSets):\n",
    "    gradientDescentModel = GradientDescent(batch_size, learning_rate, momentum)\n",
    "    logisticRegressionModel = LogisticRegression(False)\n",
    "    \n",
    "    logisticRegressionModel.fit(fold, yDigitsTrainingSets[fold_index], gradientDescentModel)\n",
    "    yh_training = logisticRegressionModel.predict(xDigitsTrainingSets[fold_index])\n",
    "    yh_validation = logisticRegressionModel.predict(xDigitsValidationSets[fold_index])\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh_training):\n",
    "        if accurate(yh_x, yDigitsTrainingSets[fold_index][sample_index]):\n",
    "            digits_training_accuracy += 1\n",
    "        c = np.argmax(yDigitsTrainingSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yDigitsTrainingSets[fold_index][sample_index][c])\n",
    "        digits_training_cost += cst\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh_validation):\n",
    "        if accurate(yh_x, yDigitsValidationSets[fold_index][sample_index]):\n",
    "            digits_validation_accuracy += 1\n",
    "        c = np.argmax(yDigitsValidationSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yDigitsValidationSets[fold_index][sample_index][c])\n",
    "        digits_validation_cost += cst\n",
    "        \n",
    "digits_training_accuracy /= 4*len(digits.data)\n",
    "digits_training_cost /= 4\n",
    "digits_validation_accuracy /= len(digits.data)\n",
    "\n",
    "wine_training_accuracy = 0\n",
    "wine_training_cost = 0\n",
    "wine_validation_accuracy = 0\n",
    "wine_validation_cost = 0\n",
    "\n",
    "for fold_index, fold in enumerate(xWineTrainingSets):\n",
    "    gradientDescentModel = GradientDescent(batch_size, learning_rate, momentum)\n",
    "    logisticRegressionModel = LogisticRegression(False)\n",
    "    \n",
    "    logisticRegressionModel.fit(fold, yWineTrainingSets[fold_index], gradientDescentModel)\n",
    "    yh_training = logisticRegressionModel.predict(xWineTrainingSets[fold_index])\n",
    "    yh_validation = logisticRegressionModel.predict(xWineValidationSets[fold_index])\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh_training):\n",
    "        if accurate(yh_x, yWineTrainingSets[fold_index][sample_index]):\n",
    "            wine_training_accuracy += 1\n",
    "        c = np.argmax(yWineTrainingSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yWineTrainingSets[fold_index][sample_index][c])\n",
    "        wine_training_cost += cst\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh_validation):\n",
    "        if accurate(yh_x, yWineValidationSets[fold_index][sample_index]):\n",
    "            wine_validation_accuracy += 1\n",
    "        c = np.argmax(yWineValidationSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yWineValidationSets[fold_index][sample_index][c])\n",
    "        wine_validation_cost += cst\n",
    "\n",
    "wine_training_accuracy /= 4*len(wine.data)\n",
    "wine_training_cost /= 4\n",
    "wine_validation_accuracy /= len(wine.data)\n",
    "        \n",
    "print(\"Model hyper-parameters:\")\n",
    "print(\"\\tMini-batch size:\", batch_size)\n",
    "print(\"\\tLearning rate:\", learning_rate)\n",
    "print(\"\\tMomentum:\", momentum)\n",
    "print(\"Digits training accuracy:\", digits_training_accuracy)\n",
    "print(\"Digits training cost:\", digits_training_cost)\n",
    "print(\"Digits validation accuracy:\", digits_validation_accuracy)\n",
    "print(\"Digits validation cost:\", digits_validation_cost)\n",
    "print(\"Wine training accuracy:\", wine_training_accuracy)\n",
    "print(\"Wine training cost:\", wine_training_cost)\n",
    "print(\"Wine validation accuracy:\", wine_validation_accuracy)\n",
    "print(\"Wine validation cost:\", wine_validation_cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: comparison against another classifier (e.g. KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
