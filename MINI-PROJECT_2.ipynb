{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lia\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797\n",
      "1797\n"
     ]
    }
   ],
   "source": [
    "print(len(digits.data))\n",
    "print(digits.target.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "wine = fetch_openml(name='wine', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n",
      "178\n"
     ]
    }
   ],
   "source": [
    "print(len(wine.data))\n",
    "print(wine.target.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalising wine data\n",
    "wine_data_norm = []\n",
    "for col in wine.data.T:\n",
    "    col_norm = col/np.amax(col)\n",
    "    wine_data_norm.append(col_norm)\n",
    "    \n",
    "wine.data = np.asarray(wine_data_norm).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation for digits dataset\n",
    "\n",
    "digitsTrainingSetSize = int(np.ceil(0.8 * len(digits.data)))\n",
    "digitsValidationSetSize = int(len(digits.data) - digitsTrainingSetSize)\n",
    "\n",
    "xDigitsTrainingSets = []\n",
    "yDigitsTrainingSets = []\n",
    "xDigitsValidationSets = []\n",
    "yDigitsValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data[foldIndex*digitsValidationSetSize:((foldIndex*digitsValidationSetSize)+digitsValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(digits.target[index])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(digits.target[index])\n",
    "            \n",
    "    xDigitsTrainingSets.append(xTrainingSet)\n",
    "    yDigitsTrainingSets.append(yTrainingSet)\n",
    "    xDigitsValidationSets.append(xValidationSet)\n",
    "    yDigitsValidationSets.append(yValidationSet)\n",
    "    \n",
    "# 5-fold cross validation for wine dataset\n",
    "\n",
    "wineTrainingSetSize = int(np.ceil(0.8 * len(wine.data)))\n",
    "wineValidationSetSize = int(len(wine.data) - wineTrainingSetSize)\n",
    "\n",
    "xWineTrainingSets = []\n",
    "yWineTrainingSets = []\n",
    "xWineValidationSets = []\n",
    "yWineValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "    for index, data in enumerate(wine.data[foldIndex*wineValidationSetSize:((foldIndex*wineValidationSetSize)+wineValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(wine.target[index+(foldIndex*wineValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "    \n",
    "    for index, data in enumerate(wine.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(wine.target[index])\n",
    "            \n",
    "    xWineTrainingSets.append(xTrainingSet)\n",
    "    yWineTrainingSets.append(yTrainingSet)\n",
    "    xWineValidationSets.append(xValidationSet)\n",
    "    yWineValidationSets.append(yValidationSet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of y for digits dataset\n",
    "\n",
    "numberOfDigitsTargets = 10\n",
    "numberOfWineTargets = 3\n",
    "\n",
    "for index, fold in enumerate(yDigitsTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yDigitsValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsValidationSets[index] = encodedFold\n",
    "\n",
    "# one-hot encoding of y for wine dataset\n",
    "\n",
    "for index, fold in enumerate(yWineTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yWineValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineValidationSets[index] = encodedFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomIndices(arr, batch_size):\n",
    "    indices = []\n",
    "    \n",
    "    if batch_size > len(arr):\n",
    "        print(\"Error: batch size larger than size of dataset.\")\n",
    "        return\n",
    "    \n",
    "    while batch_size > 0:\n",
    "        x = np.floor(np.random.random() * len(arr))\n",
    "        if x not in indices:\n",
    "            indices.append(int(x))\n",
    "            batch_size -= 1\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent class\n",
    " \n",
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, batch_size, learning_rate=0.5, momentum=0.9, max_iters=20, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iters = max_iters\n",
    "        self.epsilon = epsilon\n",
    "            \n",
    "    def run(self, gradient_fn, x, y, w):\n",
    "        # TODO: use epsilon\n",
    "        \n",
    "        grad = np.inf\n",
    "        t = 1\n",
    "        \n",
    "        while np.linalg.norm(grad) > self.epsilon and t < self.max_iters:\n",
    "            # TODO: implement momentum here\n",
    "            \n",
    "            print(\"gradient descent step:\", t)\n",
    "            \n",
    "            gradients = gradient_fn(x, y, w, self.batch_size)\n",
    "            \n",
    "            for c in range(len(y[0])):\n",
    "                w[c] = w[c] - self.learning_rate * gradients[c]\n",
    "#                 print(\"w for class: \", w[c])\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, add_bias=True):\n",
    "        self.add_bias = add_bias\n",
    "        pass\n",
    "            \n",
    "    def fit(self, x, y, optimizer):\n",
    "        # TODO: add bias\n",
    "        \n",
    "        def gradient(x, y, w, batch_size):\n",
    "            gradients = np.zeros(len(w)).tolist()\n",
    "\n",
    "            indices = getRandomIndices(x, batch_size)\n",
    "\n",
    "            for index in indices:\n",
    "                a = np.asarray(x[index])\n",
    "                b = np.asarray(y[index])\n",
    "                \n",
    "#                 print(\"x:\", a.astype(int))\n",
    "#                 print(\"y:\", b)\n",
    "\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  w[c] @ a\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  w[i] @ a\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "\n",
    "                    y_c = b[c]\n",
    "                    \n",
    "                    # TODO: may change, see slide 27 of logistic slideshow\n",
    "                    cost_c = np.dot(yh_c - y_c, a)\n",
    "                    \n",
    "                    gradients[c] += cost_c\n",
    "                    \n",
    "#                     print(\"class:\", c)\n",
    "#                     print(\"softmax numerator:\", num)\n",
    "#                     print(\"softmax denominator:\", den)\n",
    "#                     print(\"y hat for class:\", yh_c)\n",
    "#                     print(\"y actual for class:\", y_c)\n",
    "#                     print(\"x gradient:\", cost_c)\n",
    "#                     print(\"new gradient for class:\", gradients[c])\n",
    "\n",
    "            return gradients\n",
    "        \n",
    "        w0 = []\n",
    "        for c in range(len(y[0])):\n",
    "            w0.append(np.zeros(len(x[0])))\n",
    "            \n",
    "        self.w = optimizer.run(gradient, x, y, w0)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        a = np.asarray(x)\n",
    "        b = np.asarray(self.w)\n",
    "        \n",
    "#         if self.add_bias:\n",
    "#             x = np.column_stack([x,np.ones(N)])\n",
    "\n",
    "        yh=[]\n",
    "        for i,x_c in enumerate(a):\n",
    "            yh_x=[]\n",
    "            for c in range(len(b)):\n",
    "                w_x =  b[c] @ x_c\n",
    "                num = np.exp(w_x)\n",
    "\n",
    "                den = 0\n",
    "                for i in range(len(b)):\n",
    "                    w_x =  b[i] @ x_c\n",
    "                    den += np.exp(w_x)\n",
    "\n",
    "                yh_c = num/den\n",
    "                yh_x.append(yh_c)\n",
    "            yh.append(yh_x)\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient descent step: 1\n",
      "gradient descent step: 2\n",
      "gradient descent step: 3\n",
      "gradient descent step: 4\n",
      "gradient descent step: 5\n",
      "gradient descent step: 6\n",
      "gradient descent step: 7\n",
      "gradient descent step: 8\n",
      "gradient descent step: 9\n",
      "gradient descent step: 10\n",
      "gradient descent step: 11\n",
      "gradient descent step: 12\n",
      "gradient descent step: 13\n",
      "gradient descent step: 14\n",
      "gradient descent step: 15\n",
      "gradient descent step: 16\n",
      "gradient descent step: 17\n",
      "gradient descent step: 18\n",
      "gradient descent step: 19\n",
      "[0.9949122315732559, 0.005087766932441624, 1.4943024604269124e-09] [1.0, 0.0, 0.0]\n",
      "[0.9937763402665426, 0.006223647483410678, 1.2250046729543357e-08] [1.0, 0.0, 0.0]\n",
      "[0.996826508395333, 0.0031734862538432035, 5.3508237167773514e-09] [1.0, 0.0, 0.0]\n",
      "[0.9999319057699267, 6.809375572756675e-05, 4.743456760935382e-10] [1.0, 0.0, 0.0]\n",
      "[0.9359907105153912, 0.06400896388630334, 3.2559830558754403e-07] [1.0, 0.0, 0.0]\n",
      "[0.9997085166449766, 0.0002914799957747018, 3.3592486474551668e-09] [1.0, 0.0, 0.0]\n",
      "[0.9976595916697754, 0.002340398087656034, 1.0242568683981207e-08] [1.0, 0.0, 0.0]\n",
      "[0.9973199608903106, 0.0026800110428973098, 2.8066792079071964e-08] [1.0, 0.0, 0.0]\n",
      "[0.9953371027889546, 0.0046628823123441325, 1.4898701396291113e-08] [1.0, 0.0, 0.0]\n",
      "[0.9981337298147781, 0.0018662659564790873, 4.2287428836112435e-09] [1.0, 0.0, 0.0]\n",
      "[0.9993829056033681, 0.0006170937400492043, 6.565826605392932e-10] [1.0, 0.0, 0.0]\n",
      "[0.9961485731096416, 0.0038513592118143663, 6.767854410977892e-08] [1.0, 0.0, 0.0]\n",
      "[0.9980359766529922, 0.0019640031831253146, 2.016388242711172e-08] [1.0, 0.0, 0.0]\n",
      "[0.9953710495727902, 0.004628949243209392, 1.1840002437717272e-09] [1.0, 0.0, 0.0]\n",
      "[0.9998819353783075, 0.00011806443498198317, 1.8671067017445366e-10] [1.0, 0.0, 0.0]\n",
      "[0.9991132149674975, 0.0008867605051379136, 2.4527364680939195e-08] [1.0, 0.0, 0.0]\n",
      "[0.9985531647499285, 0.0014467991771606966, 3.6072910675134016e-08] [1.0, 0.0, 0.0]\n",
      "[0.9961918126209627, 0.003808138743574875, 4.8635462465307604e-08] [1.0, 0.0, 0.0]\n",
      "[0.9999460461346112, 5.3952995330835094e-05, 8.700578884751868e-10] [1.0, 0.0, 0.0]\n",
      "[0.9956477501346205, 0.004352217410079137, 3.245530034775072e-08] [1.0, 0.0, 0.0]\n",
      "[0.9845836535488482, 0.015416343884145855, 2.5670059420000714e-09] [1.0, 0.0, 0.0]\n",
      "[0.9762090684223165, 0.02379072017759379, 2.1140008965660216e-07] [1.0, 0.0, 0.0]\n",
      "[0.975108244542904, 0.024891752576379562, 2.880716385026489e-09] [1.0, 0.0, 0.0]\n",
      "[0.9693453789086954, 0.03065459564644777, 2.5444856709071173e-08] [1.0, 0.0, 0.0]\n",
      "[0.8957160637773354, 0.10428391989182847, 1.6330836154380678e-08] [1.0, 0.0, 0.0]\n",
      "[0.7839193205046935, 0.2160805275174266, 1.5197787995502954e-07] [1.0, 0.0, 0.0]\n",
      "[0.996009118340946, 0.003990857473490336, 2.4185563771165955e-08] [1.0, 0.0, 0.0]\n",
      "[0.9954887469980501, 0.004511112758535541, 1.4024341443329426e-07] [1.0, 0.0, 0.0]\n",
      "[0.9507745162272178, 0.04922547172283454, 1.2049947560382349e-08] [1.0, 0.0, 0.0]\n",
      "[0.9887644846700069, 0.01123550149700524, 1.3832987958838905e-08] [1.0, 0.0, 0.0]\n",
      "[0.9961044672849494, 0.0038955265292450717, 6.185805524346044e-09] [1.0, 0.0, 0.0]\n",
      "[0.999718731441258, 0.0002812633456470915, 5.213094838031121e-09] [1.0, 0.0, 0.0]\n",
      "[0.9487581448765139, 0.0512417912243815, 6.389910477356764e-08] [1.0, 0.0, 0.0]\n",
      "[0.9900994659747949, 0.009900483755063597, 5.027014144653117e-08] [1.0, 0.0, 0.0]\n",
      "[0.9862209077299873, 0.013778969206988437, 1.2306302429915047e-07] [1.0, 0.0, 0.0]\n",
      "gradient descent step: 1\n",
      "gradient descent step: 2\n",
      "gradient descent step: 3\n",
      "gradient descent step: 4\n",
      "gradient descent step: 5\n",
      "gradient descent step: 6\n",
      "gradient descent step: 7\n",
      "gradient descent step: 8\n",
      "gradient descent step: 9\n",
      "gradient descent step: 10\n",
      "gradient descent step: 11\n",
      "gradient descent step: 12\n",
      "gradient descent step: 13\n",
      "gradient descent step: 14\n",
      "gradient descent step: 15\n",
      "gradient descent step: 16\n",
      "gradient descent step: 17\n",
      "gradient descent step: 18\n",
      "gradient descent step: 19\n",
      "[0.9964183635584949, 5.408260176855474e-07, 0.003581095615487479] [1.0, 0.0, 0.0]\n",
      "[0.9797126949990211, 7.343890342676236e-07, 0.020286570611944575] [1.0, 0.0, 0.0]\n",
      "[0.9853693853424785, 4.815636350701572e-07, 0.014630133093886465] [1.0, 0.0, 0.0]\n",
      "[0.9957979855612098, 8.06088586164812e-07, 0.004201208350203975] [1.0, 0.0, 0.0]\n",
      "[0.9914864926455685, 2.5286460275133926e-07, 0.008513254489828663] [1.0, 0.0, 0.0]\n",
      "[0.9978174243128056, 2.681194735357687e-07, 0.002182307567720933] [1.0, 0.0, 0.0]\n",
      "[0.9486059264285389, 9.058067527075201e-07, 0.051393167764708404] [1.0, 0.0, 0.0]\n",
      "[0.9998133350314611, 1.6182355386907458e-08, 0.00018664878618350697] [1.0, 0.0, 0.0]\n",
      "[0.7994579975625508, 5.193094282615404e-06, 0.20053680934316662] [1.0, 0.0, 0.0]\n",
      "[0.9980282608780945, 4.3153257512877794e-07, 0.0019713075893304432] [1.0, 0.0, 0.0]\n",
      "[0.9200460609681248, 1.9138530706315607e-07, 0.0799537476465681] [1.0, 0.0, 0.0]\n",
      "[0.9979484382256127, 7.714496685578414e-08, 0.002051484629420435] [1.0, 0.0, 0.0]\n",
      "[0.9995885956174428, 5.6123054852907296e-08, 0.0004113482595023423] [1.0, 0.0, 0.0]\n",
      "[0.992451986454692, 1.4538549399796304e-07, 0.00754786815981385] [1.0, 0.0, 0.0]\n",
      "[0.9987864749851517, 5.0582282664959795e-09, 0.0012135199566200164] [1.0, 0.0, 0.0]\n",
      "[0.9998884421190691, 1.3934488347733325e-08, 0.00011154394644238785] [1.0, 0.0, 0.0]\n",
      "[0.9996294682896499, 3.0049532275628985e-08, 0.00037050166081764336] [1.0, 0.0, 0.0]\n",
      "[0.999800462624543, 3.3835723990003622e-09, 0.00019953399188461134] [1.0, 0.0, 0.0]\n",
      "[0.9949714350810064, 1.3389955506118387e-08, 0.00502855152903807] [1.0, 0.0, 0.0]\n",
      "[0.997532113837734, 8.208413569049558e-08, 0.00246780407813038] [1.0, 0.0, 0.0]\n",
      "[0.9979340768717034, 5.6270062904763696e-08, 0.0020658668582336755] [1.0, 0.0, 0.0]\n",
      "[0.9983317439827417, 6.172613536652055e-08, 0.0016681942911228598] [1.0, 0.0, 0.0]\n",
      "[0.9979438707713583, 1.8648698357494246e-08, 0.002056110579943448] [1.0, 0.0, 0.0]\n",
      "[0.9998135004110367, 3.1161008589173886e-09, 0.00018649647286254835] [1.0, 0.0, 0.0]\n",
      "[0.6414234072199131, 0.00022391246678729075, 0.3583526803132997] [0.0, 1.0, 0.0]\n",
      "[0.06701047937793643, 1.4314518603009781e-05, 0.9329752061034606] [0.0, 1.0, 0.0]\n",
      "[0.031665057724939175, 6.366191688840087e-06, 0.968328576083372] [0.0, 1.0, 0.0]\n",
      "[0.8038730592542379, 3.258206132644514e-05, 0.19609435868443564] [0.0, 1.0, 0.0]\n",
      "[0.9983987314160636, 6.9792262801526434e-06, 0.00159428935765625] [0.0, 1.0, 0.0]\n",
      "[0.3078943563748205, 0.00022612462993682904, 0.6918795189952427] [0.0, 1.0, 0.0]\n",
      "[0.9642414799794146, 8.02347494607832e-06, 0.03575049654563943] [0.0, 1.0, 0.0]\n",
      "[0.9992624463613317, 4.111195449532298e-06, 0.0007334424432187726] [0.0, 1.0, 0.0]\n",
      "[0.9561229753704659, 5.201448665480646e-05, 0.04382501014287931] [0.0, 1.0, 0.0]\n",
      "[0.21714007245363653, 9.55874089599128e-06, 0.7828503688054674] [0.0, 1.0, 0.0]\n",
      "[0.9921778005533554, 1.8275853714609155e-05, 0.00780392359293015] [0.0, 1.0, 0.0]\n",
      "gradient descent step: 1\n",
      "gradient descent step: 2\n",
      "gradient descent step: 3\n",
      "gradient descent step: 4\n",
      "gradient descent step: 5\n",
      "gradient descent step: 6\n",
      "gradient descent step: 7\n",
      "gradient descent step: 8\n",
      "gradient descent step: 9\n",
      "gradient descent step: 10\n",
      "gradient descent step: 11\n",
      "gradient descent step: 12\n",
      "gradient descent step: 13\n",
      "gradient descent step: 14\n",
      "gradient descent step: 15\n",
      "gradient descent step: 16\n",
      "gradient descent step: 17\n",
      "gradient descent step: 18\n",
      "gradient descent step: 19\n",
      "[0.545546812188369, 0.19440082828573754, 0.2600523595258935] [0.0, 1.0, 0.0]\n",
      "[0.6137139650223987, 0.38567791111064165, 0.0006081238669595518] [0.0, 1.0, 0.0]\n",
      "[0.5216685594252081, 0.4403317001587983, 0.03799974041599377] [0.0, 1.0, 0.0]\n",
      "[0.9088295364561884, 0.0909610590056027, 0.00020940453820899056] [0.0, 1.0, 0.0]\n",
      "[0.9631038978319235, 0.03671295858649763, 0.00018314358157878858] [0.0, 1.0, 0.0]\n",
      "[0.5523473138538684, 0.3736498013713434, 0.07400288477478825] [0.0, 1.0, 0.0]\n",
      "[0.7964017183134261, 0.19748058951060107, 0.006117692175972843] [0.0, 1.0, 0.0]\n",
      "[0.2985123074001816, 0.4777977814836802, 0.2236899111161382] [0.0, 1.0, 0.0]\n",
      "[0.8525073643850657, 0.14353183919921278, 0.003960796415721424] [0.0, 1.0, 0.0]\n",
      "[0.2736973347687837, 0.7174897140662012, 0.008812951165015131] [0.0, 1.0, 0.0]\n",
      "[0.4907149339859234, 0.5079851225854134, 0.0012999434286632143] [0.0, 1.0, 0.0]\n",
      "[0.8997943509814413, 0.09892329600814442, 0.0012823530104142593] [0.0, 1.0, 0.0]\n",
      "[0.46515785958732964, 0.5178708947667516, 0.01697124564591865] [0.0, 1.0, 0.0]\n",
      "[0.04750671611954819, 0.22420661557918617, 0.7282866683012655] [0.0, 1.0, 0.0]\n",
      "[0.7627975640421977, 0.23600742172022451, 0.0011950142375777885] [0.0, 1.0, 0.0]\n",
      "[0.6661984750957543, 0.33109240069372803, 0.0027091242105176756] [0.0, 1.0, 0.0]\n",
      "[0.2866122932087064, 0.6722573479594646, 0.0411303588318291] [0.0, 1.0, 0.0]\n",
      "[0.32376183599073216, 0.6584355547463481, 0.01780260926291985] [0.0, 1.0, 0.0]\n",
      "[0.47841236073765964, 0.4579498155959574, 0.06363782366638292] [0.0, 1.0, 0.0]\n",
      "[0.4472807000341863, 0.5422260057219147, 0.010493294243899064] [0.0, 1.0, 0.0]\n",
      "[0.2611334604123134, 0.6410990414116747, 0.09776749817601185] [0.0, 1.0, 0.0]\n",
      "[0.2016561536820484, 0.6679210581619212, 0.13042278815603042] [0.0, 1.0, 0.0]\n",
      "[0.17378629848511315, 0.5781227830658695, 0.24809091844901726] [0.0, 1.0, 0.0]\n",
      "[0.4011593979618191, 0.5969067851577491, 0.0019338168804318304] [0.0, 1.0, 0.0]\n",
      "[0.708114040034428, 0.29027933852352894, 0.0016066214420430878] [0.0, 1.0, 0.0]\n",
      "[0.8801613654848116, 0.11919540082060616, 0.0006432336945823242] [0.0, 1.0, 0.0]\n",
      "[0.5627664227251324, 0.36828940900100526, 0.06894416827386227] [0.0, 1.0, 0.0]\n",
      "[0.7424373970739812, 0.25632055896492706, 0.0012420439610919146] [0.0, 1.0, 0.0]\n",
      "[0.95849004330797, 0.04143867474479838, 7.128194723161613e-05] [0.0, 1.0, 0.0]\n",
      "[0.36131147634394806, 0.6375496764094947, 0.0011388472465573333] [0.0, 1.0, 0.0]\n",
      "[0.8869535836372967, 0.11080876776282939, 0.002237648599873758] [0.0, 1.0, 0.0]\n",
      "[0.6308803825256294, 0.3475048972712646, 0.02161472020310604] [0.0, 1.0, 0.0]\n",
      "[0.46866213404844, 0.5196707122001456, 0.011667153751414515] [0.0, 1.0, 0.0]\n",
      "[0.38110399329082884, 0.5995438229689246, 0.01935218374024654] [0.0, 1.0, 0.0]\n",
      "[0.7662716732083139, 0.23044202577146616, 0.0032863010202199673] [0.0, 1.0, 0.0]\n",
      "gradient descent step: 1\n",
      "gradient descent step: 2\n",
      "gradient descent step: 3\n",
      "gradient descent step: 4\n",
      "gradient descent step: 5\n",
      "gradient descent step: 6\n",
      "gradient descent step: 7\n",
      "gradient descent step: 8\n",
      "gradient descent step: 9\n",
      "gradient descent step: 10\n",
      "gradient descent step: 11\n",
      "gradient descent step: 12\n",
      "gradient descent step: 13\n",
      "gradient descent step: 14\n",
      "gradient descent step: 15\n",
      "gradient descent step: 16\n",
      "gradient descent step: 17\n",
      "gradient descent step: 18\n",
      "gradient descent step: 19\n",
      "[5.132752220802421e-07, 0.9999994864262395, 2.9853844638523536e-10] [0.0, 1.0, 0.0]\n",
      "[3.006362125342865e-05, 0.99996993620571, 1.7303655654449364e-10] [0.0, 1.0, 0.0]\n",
      "[4.7565696439134035e-06, 0.9999952420989473, 1.33140882437989e-09] [0.0, 1.0, 0.0]\n",
      "[1.8163867898314874e-05, 0.9999818360867284, 4.537337354944471e-11] [0.0, 1.0, 0.0]\n",
      "[0.00017198428613159514, 0.9998280157122752, 1.5931704148453674e-12] [0.0, 1.0, 0.0]\n",
      "[0.0007864742208313835, 0.9992135257329445, 4.622399330497774e-11] [0.0, 1.0, 0.0]\n",
      "[3.0909429863540974e-05, 0.9999690904078388, 1.6229765101997855e-10] [0.0, 1.0, 0.0]\n",
      "[1.9220598328657725e-06, 0.9999980775907289, 3.494382887454428e-10] [0.0, 1.0, 0.0]\n",
      "[7.407410731300296e-06, 0.9999925925375447, 5.172390645270594e-11] [0.0, 1.0, 0.0]\n",
      "[6.341077819715147e-06, 0.9999936588892817, 3.2898472003722495e-11] [0.0, 1.0, 0.0]\n",
      "[3.602026770890639e-07, 0.9999996397969223, 4.0060331105548474e-13] [0.0, 1.0, 0.0]\n",
      "[2.104195043007096e-05, 0.9999789579768108, 7.275917382839026e-11] [0.0, 1.0, 0.0]\n",
      "[3.910771462401929e-06, 0.9999960892088069, 1.973070862420586e-11] [0.0, 1.0, 0.0]\n",
      "[4.083613822946837e-05, 0.9999588730410269, 2.908207435873495e-07] [0.0, 1.0, 0.0]\n",
      "[2.6441261199754835e-05, 0.9999735582968736, 4.419266034427752e-10] [0.0, 1.0, 0.0]\n",
      "[0.0002671322852982289, 0.9997328676870748, 2.7627028094075186e-11] [0.0, 1.0, 0.0]\n",
      "[2.274840460861069e-05, 0.9999772515952553, 1.3599578781393874e-13] [0.0, 1.0, 0.0]\n",
      "[1.2024627974139174e-06, 0.99999879739135, 1.458526859305962e-10] [0.0, 1.0, 0.0]\n",
      "[8.513046500348745e-05, 0.9999148680995332, 1.4354632708108802e-09] [0.0, 1.0, 0.0]\n",
      "[0.0002482828390309274, 0.9997517171374628, 2.3506269422383588e-11] [0.0, 1.0, 0.0]\n",
      "[3.100122961017555e-05, 0.9999689987205147, 4.9875141952352174e-11] [0.0, 1.0, 0.0]\n",
      "[7.417803683392607e-05, 0.9999258219000088, 6.315716242924963e-11] [0.0, 1.0, 0.0]\n",
      "[4.953760990689096e-07, 0.9999995045512364, 7.266430567068622e-11] [0.0, 1.0, 0.0]\n",
      "[4.499790949747833e-06, 0.9999955001809687, 2.808150640696835e-11] [0.0, 1.0, 0.0]\n",
      "[2.4210957117494805e-05, 0.999975779147061, 9.895821560388788e-09] [0.0, 1.0, 0.0]\n",
      "[0.00026092926535692564, 0.9997389967213575, 7.401328555805849e-08] [0.0, 0.0, 1.0]\n",
      "[0.0001426793945182462, 0.9998564113656464, 9.092398353570741e-07] [0.0, 0.0, 1.0]\n",
      "[6.274222285328584e-05, 0.9999362304377651, 1.0273393816032824e-06] [0.0, 0.0, 1.0]\n",
      "[0.00024754544415052615, 0.9997516855423612, 7.690134883193925e-07] [0.0, 0.0, 1.0]\n",
      "[2.139327935858738e-05, 0.9999784643672915, 1.4235334987441403e-07] [0.0, 0.0, 1.0]\n",
      "[2.7000461399175098e-05, 0.9999713203860849, 1.6791525157454468e-06] [0.0, 0.0, 1.0]\n",
      "[1.151473591681097e-05, 0.9999811330904663, 7.3521736168059905e-06] [0.0, 0.0, 1.0]\n",
      "[9.254120105693071e-07, 0.999997746185074, 1.3284029154039087e-06] [0.0, 0.0, 1.0]\n",
      "[7.653453137322915e-06, 0.9999912694291387, 1.0771177238811536e-06] [0.0, 0.0, 1.0]\n",
      "[2.380836246176459e-06, 0.9999975952943622, 2.3869391561200497e-08] [0.0, 0.0, 1.0]\n",
      "gradient descent step: 1\n",
      "gradient descent step: 2\n",
      "gradient descent step: 3\n",
      "gradient descent step: 4\n",
      "gradient descent step: 5\n",
      "gradient descent step: 6\n",
      "gradient descent step: 7\n",
      "gradient descent step: 8\n",
      "gradient descent step: 9\n",
      "gradient descent step: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient descent step: 11\n",
      "gradient descent step: 12\n",
      "gradient descent step: 13\n",
      "gradient descent step: 14\n",
      "gradient descent step: 15\n",
      "gradient descent step: 16\n",
      "gradient descent step: 17\n",
      "gradient descent step: 18\n",
      "gradient descent step: 19\n",
      "[1.3557525721040914e-06, 0.054908657402630914, 0.945089986844797] [0.0, 0.0, 1.0]\n",
      "[1.904385260204277e-05, 0.024422186802755173, 0.9755587693446427] [0.0, 0.0, 1.0]\n",
      "[5.561470478277989e-07, 0.0602024432896972, 0.939797000563255] [0.0, 0.0, 1.0]\n",
      "[1.757729824046178e-06, 0.020122630628781224, 0.9798756116413947] [0.0, 0.0, 1.0]\n",
      "[4.1779484590259285e-05, 0.0029426992123830187, 0.9970155213030267] [0.0, 0.0, 1.0]\n",
      "[5.933222656255749e-06, 0.018092905932752975, 0.9819011608445907] [0.0, 0.0, 1.0]\n",
      "[1.4420245444616145e-06, 0.007454252312763295, 0.9925443056626923] [0.0, 0.0, 1.0]\n",
      "[2.5651416055919255e-06, 0.003200644704558639, 0.9967967901538358] [0.0, 0.0, 1.0]\n",
      "[6.469151485945369e-06, 0.005661789653438617, 0.9943317411950754] [0.0, 0.0, 1.0]\n",
      "[8.105494944403842e-06, 0.0027692010575486965, 0.9972226934475069] [0.0, 0.0, 1.0]\n",
      "[1.1251508550897804e-05, 0.008238640331697203, 0.9917501081597518] [0.0, 0.0, 1.0]\n",
      "[3.074581241681217e-05, 0.004508639759418542, 0.9954606144281647] [0.0, 0.0, 1.0]\n",
      "[9.886707657580661e-06, 0.05243272007313169, 0.9475573932192107] [0.0, 0.0, 1.0]\n",
      "[3.319332491449242e-06, 0.0026466062101863723, 0.9973500744573222] [0.0, 0.0, 1.0]\n",
      "[7.283271752340022e-06, 0.030781083159929567, 0.9692116335683181] [0.0, 0.0, 1.0]\n",
      "[6.026572661810986e-07, 0.0020819799212006868, 0.9979174174215332] [0.0, 0.0, 1.0]\n",
      "[3.3732151488541184e-06, 0.004267807314682789, 0.9957288194701683] [0.0, 0.0, 1.0]\n",
      "[5.480728421440355e-07, 0.009359947088646868, 0.990639504838511] [0.0, 0.0, 1.0]\n",
      "[7.081945051205492e-06, 0.008272951945537713, 0.9917199661094112] [0.0, 0.0, 1.0]\n",
      "[1.0729912523180933e-05, 0.010084088837504133, 0.9899051812499727] [0.0, 0.0, 1.0]\n",
      "[4.111666363736521e-06, 0.006897444371257333, 0.993098443962379] [0.0, 0.0, 1.0]\n",
      "[1.7420228275290586e-06, 0.01825934509817541, 0.9817389128789971] [0.0, 0.0, 1.0]\n",
      "[4.7213073698087247e-07, 0.03894605320907195, 0.9610534746601911] [0.0, 0.0, 1.0]\n",
      "[7.534002538641398e-06, 0.014349066076147696, 0.9856433999213137] [0.0, 0.0, 1.0]\n",
      "[6.543856232116314e-06, 0.006428953325804136, 0.9935645028179638] [0.0, 0.0, 1.0]\n",
      "[6.555132618542235e-07, 0.012393511926733939, 0.9876058325600042] [0.0, 0.0, 1.0]\n",
      "[2.560007773585862e-06, 0.0028449094569273294, 0.9971525305352992] [0.0, 0.0, 1.0]\n",
      "[1.3847759327859531e-05, 0.002680906188035352, 0.9973052460526368] [0.0, 0.0, 1.0]\n",
      "[4.631276292291507e-06, 0.01236372904926958, 0.9876316396744381] [0.0, 0.0, 1.0]\n",
      "[4.146629861472468e-06, 0.003526713279928168, 0.9964691400902103] [0.0, 0.0, 1.0]\n",
      "[6.529013151069399e-06, 0.027403046157657013, 0.9725904248291919] [0.0, 0.0, 1.0]\n",
      "[6.116522120406704e-06, 0.006202646083434272, 0.9937912373944453] [0.0, 0.0, 1.0]\n",
      "[9.366574661901213e-06, 0.0051999434300215195, 0.9947906899953166] [0.0, 0.0, 1.0]\n",
      "[1.0899345789858377e-06, 0.0012162788346927924, 0.9987826312307283] [0.0, 0.0, 1.0]\n",
      "[3.0433246168198187e-06, 0.006255741415709874, 0.9937412152596733] [0.0, 0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def cost(yh, y):\n",
    "    return y * np.log1p(np.exp(-yh)) + (1-yh) * np.log1p(np.exp(yh))\n",
    "\n",
    "for fold_index, fold in enumerate(xWineTrainingSets):\n",
    "    gradientDescentModel = GradientDescent(10)\n",
    "    logisticRegressionModel = LogisticRegression(False)\n",
    "    \n",
    "    logisticRegressionModel.fit(fold, yWineTrainingSets[fold_index], gradientDescentModel)\n",
    "    yh = logisticRegressionModel.predict(xWineValidationSets[fold_index])\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh):\n",
    "        print(yh_x, yWineValidationSets[fold_index][sample_index])\n",
    "        pass\n",
    "\n",
    "\n",
    "# for i, yh_x in enumerate(yh):\n",
    "    \n",
    "#     print()\n",
    "#     y_index = -1\n",
    "#     for j, c in enumerate(yWineTrainingSets[0][i]):\n",
    "#         if int(c) == 1:\n",
    "#             y_index = j\n",
    "            \n",
    "#     print(y_index)\n",
    "        \n",
    "#     #cst = cost(yh_x)\n",
    "#     print(yh_x, yWineTrainingSets[0][i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#xWineTrainingSets = []\n",
    "#yWineTrainingSets = []\n",
    "#xWineValidationSets = []\n",
    "#yWineValidationSets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
