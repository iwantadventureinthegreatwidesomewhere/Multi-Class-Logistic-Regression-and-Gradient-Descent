{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lia\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797\n",
      "1797\n"
     ]
    }
   ],
   "source": [
    "print(len(digits.data))\n",
    "print(digits.target.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "wine = fetch_openml(name='wine', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n",
      "178\n"
     ]
    }
   ],
   "source": [
    "print(len(wine.data))\n",
    "print(wine.target.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of wine data via division of each feature by its max value\n",
    "digits_data_norm = []\n",
    "for col in digits.data:\n",
    "    col_norm = col/np.max(col)\n",
    "    digits_data_norm.append(col_norm)\n",
    "\n",
    "digits.data = np.asarray(digits_data_norm)\n",
    "\n",
    "\n",
    "wine_data_norm = []\n",
    "for col in wine.data.T:\n",
    "    col_norm = col/np.amax(col)\n",
    "    wine_data_norm.append(col_norm)\n",
    "    \n",
    "wine.data = np.asarray(wine_data_norm).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation for digits dataset\n",
    "\n",
    "digitsTrainingSetSize = int(np.ceil(0.8 * len(digits.data)))\n",
    "digitsValidationSetSize = int(len(digits.data) - digitsTrainingSetSize)\n",
    "\n",
    "xDigitsTrainingSets = []\n",
    "yDigitsTrainingSets = []\n",
    "xDigitsValidationSets = []\n",
    "yDigitsValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data[foldIndex*digitsValidationSetSize:((foldIndex*digitsValidationSetSize)+digitsValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(digits.target[index+(foldIndex*digitsValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(digits.target[index])\n",
    "            \n",
    "    xDigitsTrainingSets.append(xTrainingSet)\n",
    "    yDigitsTrainingSets.append(yTrainingSet)\n",
    "    xDigitsValidationSets.append(xValidationSet)\n",
    "    yDigitsValidationSets.append(yValidationSet)\n",
    "    \n",
    "# 5-fold cross validation for wine dataset\n",
    "\n",
    "wineTrainingSetSize = int(np.ceil(0.8 * len(wine.data)))\n",
    "wineValidationSetSize = int(len(wine.data) - wineTrainingSetSize)\n",
    "\n",
    "xWineTrainingSets = []\n",
    "yWineTrainingSets = []\n",
    "xWineValidationSets = []\n",
    "yWineValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "    for index, data in enumerate(wine.data[foldIndex*wineValidationSetSize:((foldIndex*wineValidationSetSize)+wineValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(wine.target[index+(foldIndex*wineValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "    \n",
    "    for index, data in enumerate(wine.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(wine.target[index])\n",
    "            \n",
    "    xWineTrainingSets.append(xTrainingSet)\n",
    "    yWineTrainingSets.append(yTrainingSet)\n",
    "    xWineValidationSets.append(xValidationSet)\n",
    "    yWineValidationSets.append(yValidationSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of y for digits dataset\n",
    "\n",
    "numberOfDigitsTargets = 10\n",
    "numberOfWineTargets = 3\n",
    "\n",
    "for index, fold in enumerate(yDigitsTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yDigitsValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsValidationSets[index] = encodedFold\n",
    "\n",
    "# one-hot encoding of y for wine dataset\n",
    "\n",
    "for index, fold in enumerate(yWineTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yWineValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineValidationSets[index] = encodedFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomIndices(arr, batch_size):\n",
    "    indices = []\n",
    "    \n",
    "    if batch_size > len(arr):\n",
    "        print(\"Error: batch size larger than size of dataset.\")\n",
    "        return\n",
    "    \n",
    "    while batch_size > 0:\n",
    "        x = np.floor(np.random.random() * len(arr))\n",
    "        if x not in indices:\n",
    "            indices.append(int(x))\n",
    "            batch_size -= 1\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent class\n",
    " \n",
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, batch_size, learning_rate=0.5, momentum=0.9, max_iterations=10, epsilon=1e-8, iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iterations = max_iterations\n",
    "        self.epsilon = epsilon\n",
    "        self.deltas = []\n",
    "        \n",
    "        self.iters = iters\n",
    "        \n",
    "    def run(self, gradient_fn, x, y, w):\n",
    "        print(\"##############################\")\n",
    "        t = 1\n",
    "        \n",
    "        min_cost = np.inf\n",
    "        iterations = 0        \n",
    "        weight_history = []\n",
    "        error_history = []\n",
    "                \n",
    "        for number_of_targets in range(len(y[0])):\n",
    "            weight_history.append([])\n",
    "            error_history.append([])\n",
    "        \n",
    "        while iterations < self.max_iterations and t < self.iters:\n",
    "            \n",
    "            gradients = gradient_fn(x, y, w, self.batch_size)   \n",
    "            \n",
    "            for c in range(len(y[0])):\n",
    "                if(t==1):\n",
    "                    w[c] = w[c] - self.learning_rate * gradients[c]\n",
    "                else:\n",
    "                    delta_w = (self.momentum)*(self.deltas[-(len(y[0]))]) + (1-self.momentum)*gradients[c]\n",
    "                    w[c] = w[c] - (self.learning_rate)*(delta_w)\n",
    "                self.deltas.append(w[c])\n",
    "            \n",
    "            a = np.asarray(x)\n",
    "            b = np.asarray(w)\n",
    "\n",
    "#             if self.add_bias:\n",
    "#                 x = np.column_stack([x,np.ones(N)])\n",
    "    \n",
    "            yh=[]\n",
    "            for i, x_c in enumerate(a):\n",
    "                yh_x=[]\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  b[c] @ x_c\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  b[i] @ x_c\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "                    yh_x.append(yh_c)\n",
    "                yh.append(yh_x)\n",
    "                \n",
    "            step_cost = 0\n",
    "                \n",
    "            def cost(yh, y):\n",
    "                return y * np.log1p(np.exp(-yh)) + (1-yh) * np.log1p(np.exp(yh))\n",
    "                \n",
    "            for sample_index, yh_x in enumerate(yh):\n",
    "                c = np.argmax(y[sample_index])\n",
    "                cst = cost(yh_x[c], y[sample_index][c])\n",
    "                step_cost += cst\n",
    "            \n",
    "            #print(step_cost)\n",
    "            for c in range(len(b)):\n",
    "                weight_history[c].append(w[c])\n",
    "                error_history.append(step_cost)\n",
    "            \n",
    "            if step_cost < min_cost:\n",
    "                min_cost = step_cost\n",
    "                iterations = 0\n",
    "                print(f'min={min_cost}')\n",
    "            else:\n",
    "                iterations += 1\n",
    "                print(f'{iterations}')\n",
    "            \n",
    "            t += 1\n",
    "            \n",
    "        #TODO return best w\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, add_bias=True):\n",
    "        self.add_bias = add_bias\n",
    "        pass\n",
    "            \n",
    "    def fit(self, x, y, optimizer):\n",
    "        # TODO: add bias\n",
    "        \n",
    "        def gradient(x, y, w, batch_size):\n",
    "            gradients = np.zeros(len(w)).tolist()\n",
    "\n",
    "            indices = getRandomIndices(x, batch_size)\n",
    "\n",
    "            for index in indices:\n",
    "                a = np.asarray(x[index])\n",
    "                b = np.asarray(y[index])\n",
    "\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  w[c] @ a\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  w[i] @ a\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "\n",
    "                    y_c = b[c]\n",
    "                    \n",
    "                    cost_c = np.dot(yh_c - y_c, a)\n",
    "                    \n",
    "                    gradients[c] += cost_c\n",
    "\n",
    "            return gradients\n",
    "        \n",
    "        w0 = []\n",
    "        for c in range(len(y[0])):\n",
    "            w0.append(np.zeros(len(x[0])))\n",
    "            \n",
    "        self.w = optimizer.run(gradient, x, y, w0)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        a = np.asarray(x)\n",
    "        b = np.asarray(self.w)\n",
    "        \n",
    "#       if self.add_bias:\n",
    "#           x = np.column_stack([x,np.ones(N)])\n",
    "\n",
    "        yh=[]\n",
    "        for i, x_c in enumerate(a):\n",
    "            yh_x=[]\n",
    "            for c in range(len(b)):\n",
    "                w_x =  b[c] @ x_c\n",
    "                num = np.exp(w_x)\n",
    "\n",
    "                den = 0\n",
    "                for i in range(len(b)):\n",
    "                    w_x =  b[i] @ x_c\n",
    "                    den += np.exp(w_x)\n",
    "\n",
    "                yh_c = num/den\n",
    "                yh_x.append(yh_c)\n",
    "            yh.append(yh_x)\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "min=1835.5417664561026\n",
      "min=1833.5733962199313\n",
      "min=1809.5965232774824\n",
      "min=1778.0540341198387\n",
      "min=1730.2856216983266\n",
      "min=1729.3915517834735\n",
      "min=1688.3848858247081\n",
      "min=1656.8854783185477\n",
      "min=1608.8914519374234\n",
      "min=1601.3534948071804\n",
      "min=1582.5839749851891\n",
      "min=1552.0508247023538\n",
      "1\n",
      "min=1491.8853387272784\n",
      "min=1449.194146969982\n",
      "1\n",
      "2\n",
      "min=1413.2312751156205\n",
      "min=1405.5593718515124\n",
      "min=1396.81191378177\n",
      "min=1368.0709278634288\n",
      "min=1346.1882321194719\n",
      "min=1345.0948021876918\n",
      "1\n",
      "min=1310.6798727736857\n",
      "min=1304.8997836377503\n",
      "1\n",
      "min=1278.239521505106\n",
      "1\n",
      "min=1265.0113880508068\n",
      "min=1242.9757674958948\n",
      "min=1236.551191966257\n",
      "1\n",
      "min=1217.8038904382151\n",
      "1\n",
      "min=1209.7333853140574\n",
      "1\n",
      "2\n",
      "min=1190.379266464448\n",
      "min=1188.0529764320406\n",
      "1\n",
      "min=1186.8947504439532\n",
      "min=1168.3518706016948\n",
      "min=1164.309355470776\n",
      "1\n",
      "min=1149.414403509449\n",
      "1\n",
      "min=1147.168456575329\n",
      "1\n",
      "min=1137.1049495423197\n",
      "1\n",
      "min=1128.8189656112563\n",
      "min=1124.5900110216442\n",
      "min=1117.3113000618366\n",
      "min=1105.2354273296344\n",
      "1\n",
      "min=1104.8362535651036\n",
      "min=1092.9621325622725\n",
      "min=1091.0250702724338\n",
      "1\n",
      "min=1074.9751878405852\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "min=1063.475224710141\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "min=1058.509290128263\n",
      "1\n",
      "min=1057.4378123712947\n",
      "1\n",
      "2\n",
      "min=1047.5706554156232\n",
      "1\n",
      "2\n",
      "3\n",
      "min=1046.9170164240427\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "min=1044.0745181070438\n",
      "min=1043.9769007108766\n",
      "1\n",
      "2\n",
      "min=1031.4346603161232\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "min=1027.0939915712013\n",
      "min=1024.0352187205542\n",
      "min=1022.7866761369976\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "min=1021.9887937297299\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "min=1016.5365929120497\n",
      "min=1015.7238730018206\n",
      "1\n",
      "min=1014.0882221080228\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "##############################\n",
      "min=1854.4326692134316\n",
      "min=1819.5345507823379\n",
      "min=1798.0817251256099\n",
      "min=1792.5456078192474\n",
      "min=1727.7429716647505\n",
      "min=1716.9479646485172\n",
      "min=1674.4712528583339\n",
      "min=1664.0250245043237\n",
      "min=1657.2356203748918\n",
      "min=1584.9136304296683\n",
      "min=1549.3579813719691\n",
      "min=1514.3293143624599\n",
      "min=1496.7535753315044\n",
      "min=1472.8485266532102\n",
      "1\n",
      "min=1426.9813585846314\n",
      "min=1395.7308973743905\n",
      "min=1367.6051852168798\n",
      "1\n",
      "min=1365.1970827504435\n",
      "min=1326.5133110588242\n",
      "1\n",
      "min=1294.8960895547143\n",
      "min=1274.3494621210018\n",
      "min=1257.0392058611062\n",
      "min=1256.8694873269162\n",
      "1\n",
      "min=1233.1567554391077\n",
      "min=1229.0772390857771\n",
      "min=1215.9551521013395\n",
      "min=1212.7781036997774\n",
      "min=1209.5592435716976\n",
      "1\n",
      "min=1193.3227921165746\n",
      "min=1186.7272162579163\n",
      "min=1166.846275397342\n",
      "1\n",
      "2\n",
      "min=1156.2886670368493\n",
      "1\n",
      "min=1135.3865591925312\n",
      "1\n",
      "min=1131.239755726427\n",
      "min=1129.8470734465443\n",
      "min=1120.1015703052594\n",
      "1\n",
      "min=1117.6387094976428\n",
      "1\n",
      "min=1112.9284505064973\n",
      "1\n",
      "min=1100.1169807914273\n",
      "min=1096.6058002331715\n",
      "min=1093.7621243945266\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "min=1078.8757122242307\n",
      "1\n",
      "min=1076.5410997991933\n",
      "min=1067.5429838686312\n",
      "min=1058.7411117023146\n",
      "1\n",
      "2\n",
      "3\n",
      "min=1053.4046982672296\n",
      "min=1047.4498165082452\n",
      "1\n",
      "2\n",
      "min=1045.7223899126618\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "min=1038.333138996704\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "min=1026.337349119274\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "##############################\n",
      "min=1858.2048130899645\n",
      "min=1833.9908355963707\n",
      "min=1785.0974652086416\n",
      "min=1779.7890759982977\n",
      "min=1736.3133240107254\n",
      "min=1709.1906858230905\n",
      "min=1702.1625668967267\n",
      "min=1660.0120401393074\n",
      "min=1638.450920188299\n",
      "min=1597.3771996429748\n",
      "min=1590.4517062774812\n",
      "min=1554.1613770903623\n",
      "min=1523.1177384394168\n",
      "min=1486.2461831649466\n",
      "min=1461.7854857123696\n",
      "min=1434.8723330582604\n",
      "min=1434.4502625989671\n",
      "min=1422.7965020533313\n",
      "min=1420.1856726084661\n",
      "min=1364.7660195756187\n",
      "min=1352.0054812693895\n",
      "1\n",
      "min=1313.2860789079946\n",
      "1\n",
      "2\n",
      "min=1308.4409848371388\n",
      "min=1292.5972915431803\n",
      "min=1288.9989756604011\n",
      "min=1264.2642925696218\n",
      "1\n",
      "2\n",
      "min=1238.4701061763737\n",
      "min=1238.0849286101575\n",
      "min=1211.7232851488407\n",
      "1\n",
      "min=1193.896130520113\n",
      "1\n",
      "2\n",
      "min=1174.4037006430858\n",
      "1\n",
      "min=1170.9610811016073\n",
      "1\n",
      "min=1153.2997838916442\n",
      "min=1150.1805756184167\n",
      "1\n",
      "min=1145.8279182232118\n",
      "1\n",
      "min=1133.4410627771665\n",
      "1\n",
      "2\n",
      "3\n",
      "min=1120.8609399784148\n",
      "min=1119.2204944177504\n",
      "min=1111.7008054074836\n",
      "1\n",
      "2\n",
      "3\n",
      "min=1095.1560046428829\n",
      "1\n",
      "2\n",
      "min=1092.9432945295441\n",
      "min=1092.5672749336534\n",
      "min=1078.792535025591\n",
      "1\n",
      "2\n",
      "3\n",
      "min=1069.8054449767865\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "min=1065.3494702807418\n",
      "min=1058.229650590733\n",
      "min=1051.7582779135075\n",
      "1\n",
      "min=1050.3045552680048\n",
      "min=1048.2068023429679\n",
      "1\n",
      "min=1044.7838249454937\n",
      "min=1037.9600377578981\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "##############################\n",
      "min=1852.947555308809\n",
      "min=1832.3820013018305\n",
      "min=1798.547843572143\n",
      "min=1763.9877089763604\n",
      "min=1756.37010675155\n",
      "min=1743.420564950019\n",
      "min=1699.939843079312\n",
      "min=1640.5209483047922\n",
      "1\n",
      "min=1589.367751892789\n",
      "min=1565.8718870803368\n",
      "min=1539.6198343972737\n",
      "min=1525.1620140451446\n",
      "min=1508.6757888372351\n",
      "min=1483.9315553110855\n",
      "min=1468.562570630823\n",
      "min=1442.7703918295447\n",
      "min=1440.1961552650412\n",
      "1\n",
      "min=1390.104797938105\n",
      "min=1367.325931999903\n",
      "1\n",
      "min=1327.3078264232724\n",
      "min=1318.441198964164\n",
      "min=1309.6926930751497\n",
      "min=1303.9002113935207\n",
      "min=1283.7209535548677\n",
      "min=1274.6345810628725\n",
      "1\n",
      "min=1261.4106264002494\n",
      "1\n",
      "min=1253.324112881843\n",
      "min=1253.1970109765637\n",
      "min=1248.3801856374891\n",
      "min=1214.3853567051797\n",
      "1\n",
      "2\n",
      "min=1208.754812162202\n",
      "min=1196.2558616058773\n",
      "min=1194.2040558590822\n",
      "min=1181.1254183193976\n",
      "min=1176.906070391523\n",
      "1\n",
      "min=1164.0760674008563\n",
      "1\n",
      "min=1153.4073955019994\n",
      "min=1153.0884243957676\n",
      "1\n",
      "min=1152.7008194188516\n",
      "min=1139.614181113382\n",
      "min=1138.3500654503327\n",
      "min=1124.647338747507\n",
      "1\n",
      "2\n",
      "min=1120.707904724344\n",
      "1\n",
      "min=1108.7015229110007\n",
      "1\n",
      "min=1106.0590599628815\n",
      "1\n",
      "2\n",
      "3\n",
      "min=1092.820140810361\n",
      "1\n",
      "min=1089.1123122516624\n",
      "1\n",
      "min=1077.8276712285467\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "min=1070.5915660979663\n",
      "1\n",
      "min=1067.326195469163\n",
      "min=1060.9650864600296\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "min=1058.147242253043\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "min=1052.664957352164\n",
      "1\n",
      "min=1049.077560022987\n",
      "1\n",
      "2\n",
      "min=1040.720483437953\n",
      "min=1038.3826914436424\n",
      "1\n",
      "2\n",
      "min=1037.8652005777797\n",
      "min=1034.302266746325\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "##############################\n",
      "min=1846.6652890926057\n",
      "min=1812.554070972093\n",
      "min=1784.5810368384648\n",
      "min=1772.251888614175\n",
      "min=1739.4399098812971\n",
      "min=1699.4949027079826\n",
      "min=1663.7882246885306\n",
      "min=1660.183905043303\n",
      "min=1629.1997389964404\n",
      "min=1605.3552751168222\n",
      "min=1586.1912585582022\n",
      "min=1547.0616401273007\n",
      "min=1535.1407466185256\n",
      "min=1484.4459437178898\n",
      "min=1458.0179418917894\n",
      "1\n",
      "min=1404.384474596139\n",
      "min=1395.0720743957288\n",
      "min=1372.1980312489839\n",
      "min=1365.8726578138974\n",
      "1\n",
      "min=1353.5536419279424\n",
      "min=1343.8963361809126\n",
      "min=1304.354375984604\n",
      "min=1290.347271616201\n",
      "min=1287.397887213786\n",
      "min=1270.417195064183\n",
      "1\n",
      "min=1258.5848067450052\n",
      "min=1245.7198819837588\n",
      "1\n",
      "2\n",
      "min=1221.5424197965951\n",
      "min=1213.0546276366006\n",
      "min=1209.3508152319503\n",
      "min=1194.4100838530476\n",
      "min=1184.021069862824\n",
      "min=1178.9365728939472\n",
      "1\n",
      "2\n",
      "min=1149.098957501704\n",
      "1\n",
      "2\n",
      "min=1135.3651677304094\n",
      "min=1124.3716272778324\n",
      "1\n",
      "2\n",
      "min=1117.5734541763882\n",
      "1\n",
      "min=1103.2164793935372\n",
      "1\n",
      "min=1091.2396894942844\n",
      "min=1085.272390009954\n",
      "1\n",
      "min=1084.397905793118\n",
      "min=1074.61861443461\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "min=1073.2679512531172\n",
      "min=1067.3399408534056\n",
      "min=1060.062012523277\n",
      "1\n",
      "2\n",
      "min=1051.9970182854522\n",
      "min=1049.9163084493118\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "min=1046.1588853350336\n",
      "min=1043.3115261802366\n",
      "1\n",
      "2\n",
      "3\n",
      "min=1035.3738673135451\n",
      "min=1026.5416333881085\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "min=1021.5023610229133\n",
      "1\n",
      "min=1017.5962891076189\n",
      "1\n",
      "2\n",
      "3\n",
      "min=1016.9537828255184\n",
      "1\n",
      "2\n",
      "min=1014.9091198313004\n",
      "min=1012.2573623483912\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "min=1011.6591276759041\n",
      "min=1004.1451469761622\n",
      "1\n",
      "2\n",
      "min=1003.7354593429872\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "min=1002.9740788771834\n",
      "1\n",
      "2\n",
      "min=996.0565550531161\n",
      "min=995.2721282973265\n",
      "1\n",
      "min=992.2385699766538\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "min=991.5631085330464\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "##############################\n",
      "min=131.15338230475825\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "##############################\n",
      "min=156.33473133424727\n",
      "min=154.5740527134515\n",
      "1\n",
      "min=148.79421608770426\n",
      "min=146.3591204402452\n",
      "1\n",
      "min=145.96453961167862\n",
      "min=142.3862924255549\n",
      "min=142.1222725321629\n",
      "1\n",
      "min=140.72878462808598\n",
      "min=140.65583500053174\n",
      "min=135.14916480822131\n",
      "1\n",
      "min=134.67512726181468\n",
      "1\n",
      "min=134.50193041433738\n",
      "min=132.09015615264832\n",
      "min=131.27963849545552\n",
      "min=127.29468962299866\n",
      "1\n",
      "2\n",
      "3\n",
      "min=126.17609169733191\n",
      "min=125.78921447652897\n",
      "min=124.1836428017273\n",
      "1\n",
      "min=123.5140084624647\n",
      "min=120.78841924584826\n",
      "1\n",
      "2\n",
      "min=120.41719975471426\n",
      "min=119.15011812920315\n",
      "min=118.71549123520813\n",
      "1\n",
      "min=118.55839062848194\n",
      "1\n",
      "min=116.3510666054419\n",
      "min=115.94899235928271\n",
      "min=115.23666328769022\n",
      "min=115.06727393698161\n",
      "min=114.3795769489095\n",
      "1\n",
      "2\n",
      "min=112.79266445694353\n",
      "min=112.06994424783558\n",
      "min=111.57210131028823\n",
      "min=111.41817510242933\n",
      "min=110.66550190752025\n",
      "min=110.4124895557622\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "min=109.22335334349836\n",
      "1\n",
      "2\n",
      "min=108.23130460852632\n",
      "min=108.0876711662463\n",
      "1\n",
      "2\n",
      "min=106.86425613935309\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "min=105.56145759306871\n",
      "1\n",
      "2\n",
      "3\n",
      "min=104.63622590181716\n",
      "1\n",
      "2\n",
      "3\n",
      "min=104.57925125604658\n",
      "min=104.4067730124916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min=103.5725947458009\n",
      "1\n",
      "min=103.33671209316074\n",
      "min=103.19227561467662\n",
      "1\n",
      "min=102.83902700415707\n",
      "1\n",
      "min=102.48065621474991\n",
      "1\n",
      "2\n",
      "min=101.95157701154098\n",
      "1\n",
      "min=101.65236140115121\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "min=101.05609451213539\n",
      "1\n",
      "min=100.85211160297813\n",
      "1\n",
      "2\n",
      "3\n",
      "min=100.81633434205811\n",
      "min=100.41992746449033\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "min=100.22893634691886\n",
      "1\n",
      "2\n",
      "min=100.12751295961564\n",
      "min=100.09305974091335\n",
      "min=100.05107230446112\n",
      "min=99.85684395107887\n",
      "min=99.780627683683\n",
      "min=99.24109521606312\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "min=98.69111906120078\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "min=98.50784408073518\n",
      "1\n",
      "2\n",
      "min=98.50736375674056\n",
      "1\n",
      "min=98.12267607069137\n",
      "min=97.99744749163732\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "##############################\n",
      "min=149.934103268672\n",
      "1\n",
      "2\n",
      "3\n",
      "min=140.90163230770114\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "min=136.450435369526\n",
      "1\n",
      "min=129.26844931704937\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "min=128.149485898078\n",
      "1\n",
      "min=125.46053969033471\n",
      "min=123.87700643192174\n",
      "min=119.95023024499542\n",
      "min=117.95600105967122\n",
      "1\n",
      "2\n",
      "3\n",
      "min=114.62733550529178\n",
      "1\n",
      "2\n",
      "min=111.93884066862674\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "min=110.33311232690846\n",
      "1\n",
      "min=108.98333938895212\n",
      "1\n",
      "min=108.29347972763053\n",
      "1\n",
      "2\n",
      "min=106.97400964190139\n",
      "1\n",
      "min=104.99956310032496\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "min=104.67027499690148\n",
      "1\n",
      "min=104.262570593943\n",
      "1\n",
      "2\n",
      "min=102.6450339456931\n",
      "min=102.26225221713968\n",
      "1\n",
      "2\n",
      "min=101.20243994490873\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "min=100.54485794574114\n",
      "min=99.49645221706373\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "##############################\n",
      "min=147.82074332892915\n",
      "1\n",
      "2\n",
      "3\n",
      "min=143.3055593809061\n",
      "1\n",
      "min=141.6406095344994\n",
      "1\n",
      "2\n",
      "min=137.95812839969025\n",
      "1\n",
      "2\n",
      "min=134.3138340458167\n",
      "1\n",
      "2\n",
      "min=133.93048905005205\n",
      "min=130.55813031238048\n",
      "1\n",
      "2\n",
      "min=128.49060204336334\n",
      "1\n",
      "min=122.71734566365252\n",
      "1\n",
      "2\n",
      "3\n",
      "min=122.18580895236865\n",
      "1\n",
      "min=121.73368110963203\n",
      "min=118.63510347806908\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "min=114.89227788778095\n",
      "1\n",
      "2\n",
      "min=114.73853779793002\n",
      "1\n",
      "2\n",
      "min=112.3237553834214\n",
      "min=111.32392740969223\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "min=110.47379400721759\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "min=110.27788737707883\n",
      "1\n",
      "min=108.01175889067457\n",
      "1\n",
      "2\n",
      "3\n",
      "min=108.00737687986991\n",
      "min=107.603147960311\n",
      "1\n",
      "min=106.93386064569161\n",
      "1\n",
      "min=106.27159809971333\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "min=104.80701420868233\n",
      "1\n",
      "min=104.79889294992944\n",
      "1\n",
      "min=104.13945466954785\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "min=103.97623651659511\n",
      "min=103.75552972372007\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "min=102.65579325917429\n",
      "1\n",
      "min=102.04138720465932\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "min=101.80092193413351\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "min=101.38052641288728\n",
      "1\n",
      "2\n",
      "3\n",
      "min=101.2171144492923\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "min=100.94000511035874\n",
      "min=100.68787067858733\n",
      "1\n",
      "min=100.20633324784832\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "##############################\n",
      "min=140.02007120233748\n",
      "1\n",
      "2\n",
      "3\n",
      "min=137.5725617053442\n",
      "1\n",
      "min=137.16807834147048\n",
      "1\n",
      "min=135.64863457168056\n",
      "1\n",
      "2\n",
      "min=133.4336561608887\n",
      "1\n",
      "min=131.0397466610286\n",
      "1\n",
      "min=129.23655480883198\n",
      "1\n",
      "2\n",
      "min=129.07020400760166\n",
      "min=126.12536810183735\n",
      "min=125.78338699476714\n",
      "1\n",
      "min=124.44893146337752\n",
      "1\n",
      "min=122.10358968116014\n",
      "1\n",
      "min=121.11404531548557\n",
      "1\n",
      "2\n",
      "min=120.18326337136585\n",
      "1\n",
      "min=119.97228102154679\n",
      "min=117.13344138378993\n",
      "min=116.23800105405638\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "min=115.70664759496579\n",
      "min=115.36305639187233\n",
      "min=114.26214749601722\n",
      "min=114.01106582553216\n",
      "min=113.67289742703971\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "min=112.11401177701234\n",
      "1\n",
      "2\n",
      "3\n",
      "min=111.98751460988731\n",
      "1\n",
      "2\n",
      "3\n",
      "min=111.97458634264622\n",
      "min=110.79596490230762\n",
      "min=109.96271275237963\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "min=108.08518244084962\n",
      "1\n",
      "2\n",
      "3\n",
      "min=107.7541725614556\n",
      "min=107.01076386698894\n",
      "1\n",
      "min=106.42515655545053\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "min=106.12146110587132\n",
      "1\n",
      "min=105.91604588563557\n",
      "min=105.64491563699134\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "min=105.22998747355824\n",
      "min=104.18408722935183\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "min=104.11641129473865\n",
      "min=103.98201821816828\n",
      "1\n",
      "2\n",
      "3\n",
      "min=103.55568339908768\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "Model hyper-parameters:\n",
      "\tMini-batch size: 30\n",
      "\tLearning rate: 0.04\n",
      "\tMomentum: 0.2\n",
      "Digits training accuracy: 0.9264051196438509\n",
      "Digits training cost: 1293.631413035125\n",
      "Digits validation accuracy: 0.8809126321647189\n",
      "Digits validation cost: 1367.475323466375\n",
      "Wine training accuracy: 0.9073033707865169\n",
      "Wine training cost: 137.1895392620342\n",
      "Wine validation accuracy: 0.6629213483146067\n",
      "Wine validation cost: 161.01770979339642\n"
     ]
    }
   ],
   "source": [
    "def accurate(a, b):\n",
    "    return np.argmax(a) == np.argmax(b)\n",
    "\n",
    "def cost(yh, y):\n",
    "    return y * np.log1p(np.exp(-yh)) + (1-yh) * np.log1p(np.exp(yh))\n",
    "\n",
    "# TODO: grid-search to find lowest cost combination of model hyper-parameters\n",
    "\n",
    "batch_size = 30\n",
    "learning_rate = 0.04\n",
    "momentum = 0.2\n",
    "\n",
    "digits_training_accuracy = 0\n",
    "digits_training_cost = 0\n",
    "digits_validation_accuracy = 0\n",
    "digits_validation_cost = 0\n",
    "\n",
    "for fold_index, fold in enumerate(xDigitsTrainingSets):\n",
    "    gradientDescentModel = GradientDescent(batch_size, learning_rate, momentum)\n",
    "    logisticRegressionModel = LogisticRegression(False)\n",
    "    \n",
    "    logisticRegressionModel.fit(fold, yDigitsTrainingSets[fold_index], gradientDescentModel)\n",
    "    yh_training = logisticRegressionModel.predict(xDigitsTrainingSets[fold_index])\n",
    "    yh_validation = logisticRegressionModel.predict(xDigitsValidationSets[fold_index])\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh_training):\n",
    "        if accurate(yh_x, yDigitsTrainingSets[fold_index][sample_index]):\n",
    "            digits_training_accuracy += 1\n",
    "        c = np.argmax(yDigitsTrainingSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yDigitsTrainingSets[fold_index][sample_index][c])\n",
    "        digits_training_cost += cst\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh_validation):\n",
    "        if accurate(yh_x, yDigitsValidationSets[fold_index][sample_index]):\n",
    "            digits_validation_accuracy += 1\n",
    "        c = np.argmax(yDigitsValidationSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yDigitsValidationSets[fold_index][sample_index][c])\n",
    "        digits_validation_cost += cst\n",
    "        \n",
    "digits_training_accuracy /= 4*len(digits.data)\n",
    "digits_training_cost /= 4\n",
    "digits_validation_accuracy /= len(digits.data)\n",
    "\n",
    "wine_training_accuracy = 0\n",
    "wine_training_cost = 0\n",
    "wine_validation_accuracy = 0\n",
    "wine_validation_cost = 0\n",
    "\n",
    "for fold_index, fold in enumerate(xWineTrainingSets):\n",
    "    gradientDescentModel = GradientDescent(batch_size, learning_rate, momentum)\n",
    "    logisticRegressionModel = LogisticRegression(False)\n",
    "    \n",
    "    logisticRegressionModel.fit(fold, yWineTrainingSets[fold_index], gradientDescentModel)\n",
    "    yh_training = logisticRegressionModel.predict(xWineTrainingSets[fold_index])\n",
    "    yh_validation = logisticRegressionModel.predict(xWineValidationSets[fold_index])\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh_training):\n",
    "        if accurate(yh_x, yWineTrainingSets[fold_index][sample_index]):\n",
    "            wine_training_accuracy += 1\n",
    "        c = np.argmax(yWineTrainingSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yWineTrainingSets[fold_index][sample_index][c])\n",
    "        wine_training_cost += cst\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh_validation):\n",
    "        if accurate(yh_x, yWineValidationSets[fold_index][sample_index]):\n",
    "            wine_validation_accuracy += 1\n",
    "        c = np.argmax(yWineValidationSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yWineValidationSets[fold_index][sample_index][c])\n",
    "        wine_validation_cost += cst\n",
    "\n",
    "wine_training_accuracy /= 4*len(wine.data)\n",
    "wine_training_cost /= 4\n",
    "wine_validation_accuracy /= len(wine.data)\n",
    "        \n",
    "print(\"Model hyper-parameters:\")\n",
    "print(\"\\tMini-batch size:\", batch_size)\n",
    "print(\"\\tLearning rate:\", learning_rate)\n",
    "print(\"\\tMomentum:\", momentum)\n",
    "print(\"Digits training accuracy:\", digits_training_accuracy)\n",
    "print(\"Digits training cost:\", digits_training_cost)\n",
    "print(\"Digits validation accuracy:\", digits_validation_accuracy)\n",
    "print(\"Digits validation cost:\", digits_validation_cost)\n",
    "print(\"Wine training accuracy:\", wine_training_accuracy)\n",
    "print(\"Wine training cost:\", wine_training_cost)\n",
    "print(\"Wine validation accuracy:\", wine_validation_accuracy)\n",
    "print(\"Wine validation cost:\", wine_validation_cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: comparison against another classifier (e.g. KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
