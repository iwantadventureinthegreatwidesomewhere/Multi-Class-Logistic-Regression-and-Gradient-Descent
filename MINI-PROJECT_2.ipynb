{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lia\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797\n",
      "1797\n"
     ]
    }
   ],
   "source": [
    "print(len(digits.data))\n",
    "print(digits.target.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "wine = fetch_openml(name='wine', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n",
      "178\n"
     ]
    }
   ],
   "source": [
    "print(len(wine.data))\n",
    "print(wine.target.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalising wine data\n",
    "wine_data_norm = []\n",
    "for col in wine.data.T:\n",
    "    col_norm = col/np.amax(col)\n",
    "    wine_data_norm.append(col_norm)\n",
    "    \n",
    "wine.data = np.asarray(wine_data_norm).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation for digits dataset\n",
    "\n",
    "digitsTrainingSetSize = int(np.ceil(0.8 * len(digits.data)))\n",
    "digitsValidationSetSize = int(len(digits.data) - digitsTrainingSetSize)\n",
    "\n",
    "xDigitsTrainingSets = []\n",
    "yDigitsTrainingSets = []\n",
    "xDigitsValidationSets = []\n",
    "yDigitsValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data[foldIndex*digitsValidationSetSize:((foldIndex*digitsValidationSetSize)+digitsValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(digits.target[index])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(digits.target[index])\n",
    "            \n",
    "    xDigitsTrainingSets.append(xTrainingSet)\n",
    "    yDigitsTrainingSets.append(yTrainingSet)\n",
    "    xDigitsValidationSets.append(xValidationSet)\n",
    "    yDigitsValidationSets.append(yValidationSet)\n",
    "    \n",
    "# 5-fold cross validation for wine dataset\n",
    "\n",
    "wineTrainingSetSize = int(np.ceil(0.8 * len(wine.data)))\n",
    "wineValidationSetSize = int(len(wine.data) - wineTrainingSetSize)\n",
    "\n",
    "xWineTrainingSets = []\n",
    "yWineTrainingSets = []\n",
    "xWineValidationSets = []\n",
    "yWineValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "\n",
    "    for index, data in enumerate(wine.data[foldIndex*wineValidationSetSize:((foldIndex*wineValidationSetSize)+wineValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(wine.target[index])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "\n",
    "    for index, data in enumerate(wine.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(wine.target[index])\n",
    "            \n",
    "    xWineTrainingSets.append(xTrainingSet)\n",
    "    yWineTrainingSets.append(yTrainingSet)\n",
    "    xWineValidationSets.append(xValidationSet)\n",
    "    yWineValidationSets.append(yValidationSet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of y for digits dataset\n",
    "\n",
    "numberOfDigitsTargets = 10\n",
    "numberOfWineTargets = 3\n",
    "\n",
    "for index, fold in enumerate(yDigitsTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yDigitsValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsValidationSets[index] = encodedFold\n",
    "\n",
    "# one-hot encoding of y for wine dataset\n",
    "\n",
    "for index, fold in enumerate(yWineTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yWineValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineValidationSets[index] = encodedFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomIndices(arr, batch_size):\n",
    "    indices = []\n",
    "    \n",
    "    if batch_size > len(arr):\n",
    "        print(\"Error: batch size larger than size of dataset.\")\n",
    "        return\n",
    "    \n",
    "    while batch_size > 0:\n",
    "        x = np.floor(np.random.random() * len(arr))\n",
    "        if x not in indices:\n",
    "            indices.append(int(x))\n",
    "            batch_size -= 1\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent class\n",
    " \n",
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, batch_size, learning_rate=0.5, momentum=0.9, max_iters=10, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iters = max_iters\n",
    "        self.epsilon = epsilon\n",
    "            \n",
    "    def run(self, gradient_fn, x, y, w):\n",
    "        # TODO: use epsilon\n",
    "        \n",
    "        grad = np.inf\n",
    "        t = 1\n",
    "        \n",
    "        while np.linalg.norm(grad) > self.epsilon and t < self.max_iters:\n",
    "            # TODO: implement momentum here\n",
    "            \n",
    "            print(\"gradient descent step:\", t)\n",
    "            \n",
    "            gradients = gradient_fn(x, y, w, self.batch_size)\n",
    "            \n",
    "            for c in range(len(y[0])):\n",
    "                w[c] = w[c] - self.learning_rate * gradients[c]\n",
    "                print(\"w for class: \", w[c])\n",
    "            \n",
    "            print(\"###################################################\")\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, add_bias=True):\n",
    "        self.add_bias = add_bias\n",
    "        pass\n",
    "            \n",
    "    def fit(self, x, y, optimizer):\n",
    "        # TODO: add bias\n",
    "        \n",
    "        def gradient(x, y, w, batch_size):\n",
    "            gradients = np.zeros(len(w)).tolist()\n",
    "\n",
    "            indices = getRandomIndices(x, batch_size)\n",
    "            \n",
    "            print(\"indices for batch:\", indices)\n",
    "\n",
    "            for index in indices:\n",
    "                a = np.asarray(x[index])\n",
    "                b = np.asarray(y[index])\n",
    "                \n",
    "                print(\"x:\", a.astype(int))\n",
    "                print(\"y:\", b)\n",
    "                \n",
    "                # do max normalization on input for\n",
    "                # numerical stability during softmax\n",
    "                \n",
    "#                 max_x = np.amax(a)\n",
    "#                 a = a - max_x\n",
    "\n",
    "                print(\"x:\", a)\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  w[c] @ a\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  w[i] @ a\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "\n",
    "                    y_c = b[c]\n",
    "                    cost_c = np.dot(yh_c - y_c, a)\n",
    "                    \n",
    "                    gradients[c] += cost_c\n",
    "                    \n",
    "                    print(\"class:\", c)\n",
    "                    print(\"softmax numerator:\", num)\n",
    "                    print(\"softmax denominator:\", den)\n",
    "                    print(\"y hat for class:\", yh_c)\n",
    "                    print(\"y actual for class:\", y_c)\n",
    "                    print(\"x gradient:\", cost_c)\n",
    "                    print(\"new gradient for class:\", gradients[c])\n",
    "\n",
    "            return gradients\n",
    "        \n",
    "#        w0 = np.random.rand(len(y[0]),len(x[0])).tolist()\n",
    "        \n",
    "        w0 = []\n",
    "        for c in range(len(y[0])):\n",
    "            w0.append(np.zeros(len(x[0])))\n",
    "            \n",
    "        self.w = optimizer.run(gradient, x, y, w0)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # TODO: not tested yet, so not sure if it works\n",
    "        \n",
    "        if self.add_bias:\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "        yh = x@self.w\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientDescentModel = GradientDescent(2)\n",
    "logisticRegressionModel = LogisticRegression(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient descent step: 1\n",
      "indices for batch: [53, 90]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.78489548 0.35517241 0.76160991 0.72       0.51851852 0.50257732\n",
      " 0.33267717 0.72727273 0.37709497 0.21538462 0.58479532 0.6875\n",
      " 0.4047619 ]\n",
      "class: 0\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.26163183 0.1183908  0.25386997 0.24       0.17283951 0.16752577\n",
      " 0.11089239 0.24242424 0.12569832 0.07179487 0.19493177 0.22916667\n",
      " 0.13492063]\n",
      "new gradient for class: [0.26163183 0.1183908  0.25386997 0.24       0.17283951 0.16752577\n",
      " 0.11089239 0.24242424 0.12569832 0.07179487 0.19493177 0.22916667\n",
      " 0.13492063]\n",
      "class: 1\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.52326365 -0.23678161 -0.50773994 -0.48       -0.34567901 -0.33505155\n",
      " -0.22178478 -0.48484848 -0.25139665 -0.14358974 -0.38986355 -0.45833333\n",
      " -0.26984127]\n",
      "new gradient for class: [-0.52326365 -0.23678161 -0.50773994 -0.48       -0.34567901 -0.33505155\n",
      " -0.22178478 -0.48484848 -0.25139665 -0.14358974 -0.38986355 -0.45833333\n",
      " -0.26984127]\n",
      "class: 2\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.26163183 0.1183908  0.25386997 0.24       0.17283951 0.16752577\n",
      " 0.11089239 0.24242424 0.12569832 0.07179487 0.19493177 0.22916667\n",
      " 0.13492063]\n",
      "new gradient for class: [0.26163183 0.1183908  0.25386997 0.24       0.17283951 0.16752577\n",
      " 0.11089239 0.24242424 0.12569832 0.07179487 0.19493177 0.22916667\n",
      " 0.13492063]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.81389076 0.37241379 0.67182663 0.7        0.52469136 0.67010309\n",
      " 0.52165354 0.56060606 0.37709497 0.21230769 0.50292398 0.82\n",
      " 0.225     ]\n",
      "class: 0\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.27129692 0.12413793 0.22394221 0.23333333 0.17489712 0.2233677\n",
      " 0.17388451 0.18686869 0.12569832 0.07076923 0.16764133 0.27333333\n",
      " 0.075     ]\n",
      "new gradient for class: [0.53292875 0.24252874 0.47781218 0.47333333 0.34773663 0.39089347\n",
      " 0.2847769  0.42929293 0.25139665 0.1425641  0.3625731  0.5025\n",
      " 0.20992063]\n",
      "class: 1\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.54259384 -0.24827586 -0.44788442 -0.46666667 -0.34979424 -0.4467354\n",
      " -0.34776903 -0.37373737 -0.25139665 -0.14153846 -0.33528265 -0.54666667\n",
      " -0.15      ]\n",
      "new gradient for class: [-1.0658575  -0.48505747 -0.95562436 -0.94666667 -0.69547325 -0.78178694\n",
      " -0.56955381 -0.85858586 -0.5027933  -0.28512821 -0.7251462  -1.005\n",
      " -0.41984127]\n",
      "class: 2\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.27129692 0.12413793 0.22394221 0.23333333 0.17489712 0.2233677\n",
      " 0.17388451 0.18686869 0.12569832 0.07076923 0.16764133 0.27333333\n",
      " 0.075     ]\n",
      "new gradient for class: [0.53292875 0.24252874 0.47781218 0.47333333 0.34773663 0.39089347\n",
      " 0.2847769  0.42929293 0.25139665 0.1425641  0.3625731  0.5025\n",
      " 0.20992063]\n",
      "w for class:  [-0.26646437 -0.12126437 -0.23890609 -0.23666667 -0.17386831 -0.19544674\n",
      " -0.14238845 -0.21464646 -0.12569832 -0.07128205 -0.18128655 -0.25125\n",
      " -0.10496032]\n",
      "w for class:  [0.53292875 0.24252874 0.47781218 0.47333333 0.34773663 0.39089347\n",
      " 0.2847769  0.42929293 0.25139665 0.1425641  0.3625731  0.5025\n",
      " 0.20992063]\n",
      "w for class:  [-0.26646437 -0.12126437 -0.23890609 -0.23666667 -0.17386831 -0.19544674\n",
      " -0.14238845 -0.21464646 -0.12569832 -0.07128205 -0.18128655 -0.25125\n",
      " -0.10496032]\n",
      "###################################################\n",
      "gradient descent step: 2\n",
      "indices for batch: [76, 4]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.84423466 0.41896552 0.67182663 0.7        0.54320988 0.65721649\n",
      " 0.44685039 0.39393939 0.34078212 0.15384615 0.52631579 0.695\n",
      " 0.19345238]\n",
      "class: 0\n",
      "softmax numerator: 0.26789617465488136\n",
      "softmax denominator: 14.469509024737514\n",
      "y hat for class: 0.018514531087190166\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.01563061 0.00775695 0.01243855 0.01296017 0.01005728 0.01216806\n",
      " 0.00827323 0.0072936  0.00630942 0.00284839 0.00974449 0.0128676\n",
      " 0.00358168]\n",
      "new gradient for class: [0.01563061 0.00775695 0.01243855 0.01296017 0.01005728 0.01216806\n",
      " 0.00827323 0.0072936  0.00630942 0.00284839 0.00974449 0.0128676\n",
      " 0.00358168]\n",
      "class: 1\n",
      "softmax numerator: 13.933716675427751\n",
      "softmax denominator: 14.469509024737514\n",
      "y hat for class: 0.9629709378256197\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.03126122 -0.0155139  -0.02487711 -0.02592034 -0.02011455 -0.02433611\n",
      " -0.01654645 -0.01458721 -0.01261884 -0.00569678 -0.01948898 -0.0257352\n",
      " -0.00716336]\n",
      "new gradient for class: [-0.03126122 -0.0155139  -0.02487711 -0.02592034 -0.02011455 -0.02433611\n",
      " -0.01654645 -0.01458721 -0.01261884 -0.00569678 -0.01948898 -0.0257352\n",
      " -0.00716336]\n",
      "class: 2\n",
      "softmax numerator: 0.26789617465488136\n",
      "softmax denominator: 14.469509024737514\n",
      "y hat for class: 0.018514531087190166\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.01563061 0.00775695 0.01243855 0.01296017 0.01005728 0.01216806\n",
      " 0.00827323 0.0072936  0.00630942 0.00284839 0.00974449 0.0128676\n",
      " 0.00358168]\n",
      "new gradient for class: [0.01563061 0.00775695 0.01243855 0.01296017 0.01005728 0.01216806\n",
      " 0.00827323 0.0072936  0.00630942 0.00284839 0.00974449 0.0128676\n",
      " 0.00358168]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [1. 0. 0.]\n",
      "x: [0.95886716 0.68793103 0.77708978 0.44       0.79012346 0.77319588\n",
      " 0.5984252  0.3030303  0.58100559 0.39230769 0.52046784 0.8825\n",
      " 0.45238095]\n",
      "class: 0\n",
      "softmax numerator: 0.2160988248342669\n",
      "softmax denominator: 21.84606902388812\n",
      "y hat for class: 0.009891886022971379\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.94938216 -0.6811261  -0.7694029  -0.43564757 -0.78230765 -0.76554751\n",
      " -0.59250564 -0.30003276 -0.57525835 -0.38842703 -0.51531943 -0.87377041\n",
      " -0.44790605]\n",
      "new gradient for class: [-0.93375155 -0.67336915 -0.75696434 -0.4226874  -0.77225037 -0.75337946\n",
      " -0.58423242 -0.29273916 -0.56894892 -0.38557864 -0.50557494 -0.86090281\n",
      " -0.44432437]\n",
      "class: 1\n",
      "softmax numerator: 21.41387137421959\n",
      "softmax denominator: 21.84606902388812\n",
      "y hat for class: 0.9802162279540574\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.93989715 0.67432116 0.76171602 0.43129514 0.77449183 0.75789915\n",
      " 0.58658609 0.29703522 0.5695111  0.38454637 0.51017102 0.86504082\n",
      " 0.44343115]\n",
      "new gradient for class: [0.90863593 0.65880726 0.73683891 0.4053748  0.75437728 0.73356303\n",
      " 0.57003964 0.28244801 0.55689226 0.37884959 0.49068204 0.83930562\n",
      " 0.43626779]\n",
      "class: 2\n",
      "softmax numerator: 0.2160988248342669\n",
      "softmax denominator: 21.84606902388812\n",
      "y hat for class: 0.009891886022971379\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.009485   0.00680494 0.00768688 0.00435243 0.00781581 0.00764837\n",
      " 0.00591955 0.00299754 0.00574724 0.00388066 0.00514841 0.00872959\n",
      " 0.0044749 ]\n",
      "new gradient for class: [0.02511561 0.01456189 0.02012544 0.0173126  0.01787309 0.01981642\n",
      " 0.01419278 0.01029114 0.01205666 0.00672905 0.0148929  0.02159719\n",
      " 0.00805658]\n",
      "w for class:  [ 0.2004114   0.21542021  0.13957608 -0.02532297  0.21225687  0.18124299\n",
      "  0.14972776 -0.06827689  0.15877614  0.12150727  0.07150092  0.17920141\n",
      "  0.11720187]\n",
      "w for class:  [ 7.86107810e-02 -8.68748961e-02  1.09392724e-01  2.70645935e-01\n",
      " -2.94520156e-02  2.41119534e-02 -2.42916192e-04  2.88068922e-01\n",
      " -2.70494830e-02 -4.68606912e-02  1.17232080e-01  8.28471885e-02\n",
      " -8.21326033e-03]\n",
      "w for class:  [-0.27902218 -0.12854531 -0.24896881 -0.24532297 -0.18280486 -0.20535495\n",
      " -0.14948484 -0.21979204 -0.13172666 -0.07464658 -0.188733   -0.26204859\n",
      " -0.10898861]\n",
      "###################################################\n",
      "gradient descent step: 3\n",
      "indices for batch: [14, 51]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [1. 0. 0.]\n",
      "x: [0.93998651 0.29827586 0.70278638 0.58       0.66666667 0.74226804\n",
      " 0.69685039 0.48484848 0.58100559 0.68461538 0.65497076 0.775\n",
      " 0.75      ]\n",
      "class: 0\n",
      "softmax numerator: 3.102976913145828\n",
      "softmax denominator: 4.956842476663931\n",
      "y hat for class: 0.6259986932718109\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.35155618 -0.11155556 -0.26284302 -0.21692076 -0.2493342  -0.27760922\n",
      " -0.26062296 -0.18133397 -0.21729685 -0.25604705 -0.24495992 -0.28985101\n",
      " -0.28050098]\n",
      "new gradient for class: [-0.35155618 -0.11155556 -0.26284302 -0.21692076 -0.2493342  -0.27760922\n",
      " -0.26062296 -0.18133397 -0.21729685 -0.25604705 -0.24495992 -0.28985101\n",
      " -0.28050098]\n",
      "class: 1\n",
      "softmax numerator: 1.6596900485197537\n",
      "softmax denominator: 4.956842476663931\n",
      "y hat for class: 0.3348280798377848\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.31473388 0.09987113 0.23531261 0.19420029 0.22321872 0.24853218\n",
      " 0.23332508 0.16234089 0.19453698 0.22922845 0.2193026  0.25949176\n",
      " 0.25112106]\n",
      "new gradient for class: [0.31473388 0.09987113 0.23531261 0.19420029 0.22321872 0.24853218\n",
      " 0.23332508 0.16234089 0.19453698 0.22922845 0.2193026  0.25949176\n",
      " 0.25112106]\n",
      "class: 2\n",
      "softmax numerator: 0.19417551499835037\n",
      "softmax denominator: 4.956842476663931\n",
      "y hat for class: 0.03917322689040442\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.0368223  0.01168443 0.02753041 0.02272047 0.02611548 0.02907703\n",
      " 0.02729788 0.01899308 0.02275986 0.02681859 0.02565732 0.03035925\n",
      " 0.02937992]\n",
      "new gradient for class: [0.0368223  0.01168443 0.02753041 0.02272047 0.02611548 0.02907703\n",
      " 0.02729788 0.01899308 0.02275986 0.02681859 0.02565732 0.03035925\n",
      " 0.02937992]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.81995954 0.27758621 0.71517028 0.76       0.55555556 0.45876289\n",
      " 0.33267717 0.65151515 0.43575419 0.18846154 0.77777778 0.565\n",
      " 0.29464286]\n",
      "class: 0\n",
      "softmax numerator: 2.2135064344744544\n",
      "softmax denominator: 4.317918729617126\n",
      "y hat for class: 0.512632722633649\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.42033809 0.14229977 0.36661969 0.38960087 0.28479596 0.23517687\n",
      " 0.1705412  0.33398799 0.22338186 0.09661155 0.39871434 0.28963749\n",
      " 0.15104357]\n",
      "new gradient for class: [ 0.06878191  0.03074421  0.10377666  0.17268011  0.03546175 -0.04243235\n",
      " -0.09008176  0.15265402  0.00608501 -0.1594355   0.15375442 -0.00021352\n",
      " -0.12945741]\n",
      "class: 1\n",
      "softmax numerator: 1.8617528604943483\n",
      "softmax denominator: 4.317918729617126\n",
      "y hat for class: 0.43116903700024745\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.46641838 -0.15789963 -0.406811   -0.43231153 -0.3160172  -0.26095853\n",
      " -0.18923707 -0.37060199 -0.24787048 -0.10720276 -0.44242408 -0.32138949\n",
      " -0.16760198]\n",
      "new gradient for class: [-0.1516845  -0.0580285  -0.17149838 -0.23811125 -0.09279848 -0.01242635\n",
      "  0.04408801 -0.2082611  -0.05333349  0.1220257  -0.22312148 -0.06189773\n",
      "  0.08351908]\n",
      "class: 2\n",
      "softmax numerator: 0.24265943464832407\n",
      "softmax denominator: 4.317918729617126\n",
      "y hat for class: 0.05619824036610362\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.04608028 0.01559986 0.04019131 0.04271066 0.03122124 0.02578167\n",
      " 0.01869587 0.03661401 0.02448862 0.01059121 0.04370974 0.03175201\n",
      " 0.01655841]\n",
      "new gradient for class: [0.08290259 0.02728428 0.06772172 0.06543113 0.05733673 0.0548587\n",
      " 0.04599375 0.05560708 0.04724848 0.0374098  0.06936706 0.06211126\n",
      " 0.04593833]\n",
      "w for class:  [ 0.16602045  0.2000481   0.08768775 -0.11166302  0.194526    0.20245917\n",
      "  0.19476864 -0.14460389  0.15573363  0.20122502 -0.00537629  0.17930817\n",
      "  0.18193057]\n",
      "w for class:  [ 1.54453029e-01 -5.78606485e-02  1.95141917e-01  3.89701558e-01\n",
      "  1.69472253e-02  3.03251292e-02 -2.22869197e-02  3.92199474e-01\n",
      " -3.82737720e-04 -1.07873539e-01  2.28792820e-01  1.13796055e-01\n",
      " -4.99728002e-02]\n",
      "w for class:  [-0.32047347 -0.14218745 -0.28282967 -0.27803853 -0.21147322 -0.2327843\n",
      " -0.17248172 -0.24759558 -0.1553509  -0.09335148 -0.22341653 -0.29310422\n",
      " -0.13195777]\n",
      "###################################################\n",
      "gradient descent step: 4\n",
      "indices for batch: [48, 100]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.87997303 0.66551724 0.71826625 0.75       0.52469136 0.42525773\n",
      " 0.31299213 0.92424242 0.45251397 0.36923077 0.49122807 0.5025\n",
      " 0.30654762]\n",
      "class: 0\n",
      "softmax numerator: 1.9385536557962193\n",
      "softmax denominator: 4.901376896740048\n",
      "y hat for class: 0.3955120564357272\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.34803994 0.26322009 0.28408296 0.29663404 0.20752176 0.16819456\n",
      " 0.12379216 0.36554902 0.17897473 0.14603522 0.19428662 0.19874481\n",
      " 0.12124328]\n",
      "new gradient for class: [0.34803994 0.26322009 0.28408296 0.29663404 0.20752176 0.16819456\n",
      " 0.12379216 0.36554902 0.17897473 0.14603522 0.19428662 0.19874481\n",
      " 0.12124328]\n",
      "class: 1\n",
      "softmax numerator: 2.7770704825780226\n",
      "softmax denominator: 4.901376896740048\n",
      "y hat for class: 0.5665898667015544\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.38138923 -0.28844192 -0.31130387 -0.3250576  -0.22740655 -0.18431101\n",
      " -0.13565396 -0.40057603 -0.19612414 -0.16002836 -0.21290322 -0.21778859\n",
      " -0.13286084]\n",
      "new gradient for class: [-0.38138923 -0.28844192 -0.31130387 -0.3250576  -0.22740655 -0.18431101\n",
      " -0.13565396 -0.40057603 -0.19612414 -0.16002836 -0.21290322 -0.21778859\n",
      " -0.13286084]\n",
      "class: 2\n",
      "softmax numerator: 0.1857527583658068\n",
      "softmax denominator: 4.901376896740048\n",
      "y hat for class: 0.03789807686271845\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.03334929 0.02522182 0.02722091 0.02842356 0.01988479 0.01611645\n",
      " 0.0118618  0.03502701 0.01714941 0.01399314 0.0186166  0.01904378\n",
      " 0.01161757]\n",
      "new gradient for class: [0.03334929 0.02522182 0.02722091 0.02842356 0.01988479 0.01611645\n",
      " 0.0118618  0.03502701 0.01714941 0.01399314 0.0186166  0.01904378\n",
      " 0.01161757]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 0. 1.]\n",
      "x: [0.84962913 0.42413793 0.68111455 0.61666667 0.58024691 0.41752577\n",
      " 0.12992126 0.95454545 0.26256983 0.54615385 0.42690058 0.395\n",
      " 0.41369048]\n",
      "class: 0\n",
      "softmax numerator: 1.814907061769557\n",
      "softmax denominator: 4.582680454067265\n",
      "y hat for class: 0.39603613648400315\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.33648384 0.16797395 0.26974598 0.24422228 0.22979875 0.16535529\n",
      " 0.05145351 0.37803449 0.10398714 0.21629666 0.16906806 0.15643427\n",
      " 0.16383638]\n",
      "new gradient for class: [0.68452378 0.43119404 0.55382894 0.54085633 0.4373205  0.33354985\n",
      " 0.17524567 0.74358352 0.28296187 0.36233188 0.36335468 0.35517908\n",
      " 0.28507966]\n",
      "class: 1\n",
      "softmax numerator: 2.551855011872057\n",
      "softmax denominator: 4.582680454067265\n",
      "y hat for class: 0.5568476871668435\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.47311402 0.23618023 0.37927706 0.34338941 0.32310915 0.23249826\n",
      " 0.07234635 0.53153643 0.1462114  0.30412451 0.2377186  0.21995484\n",
      " 0.23036258]\n",
      "new gradient for class: [ 0.09172479 -0.05226169  0.06797319  0.01833181  0.0957026   0.04818725\n",
      " -0.06330761  0.1309604  -0.04991273  0.14409615  0.02481538  0.00216624\n",
      "  0.09750174]\n",
      "class: 2\n",
      "softmax numerator: 0.21591838042565115\n",
      "softmax denominator: 4.582680454067265\n",
      "y hat for class: 0.04711617634915329\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.80959785 -0.40415417 -0.64902304 -0.58761169 -0.5529079  -0.39785356\n",
      " -0.12379987 -0.90957092 -0.25019855 -0.52042117 -0.40678666 -0.37638911\n",
      " -0.39419896]\n",
      "new gradient for class: [-0.77624857 -0.37893235 -0.62180213 -0.55918813 -0.5330231  -0.38173711\n",
      " -0.11193807 -0.87454391 -0.23304914 -0.50642803 -0.38817006 -0.35734533\n",
      " -0.3825814 ]\n",
      "w for class:  [-0.17624144 -0.01554892 -0.18922672 -0.38209119 -0.02413426  0.03568424\n",
      "  0.1071458  -0.51639565  0.0142527   0.02005908 -0.18705363  0.00171863\n",
      "  0.03939074]\n",
      "w for class:  [ 0.10859063 -0.0317298   0.16115532  0.38053565 -0.03090407  0.0062315\n",
      "  0.00936688  0.32671928  0.02457363 -0.17992161  0.21638513  0.11271293\n",
      " -0.09872367]\n",
      "w for class:  [ 0.06765081  0.04727872  0.0280714   0.00155553  0.05503833 -0.04191574\n",
      " -0.11651268  0.18967638 -0.03882633  0.15986254 -0.0293315  -0.11443156\n",
      "  0.05933293]\n",
      "###################################################\n",
      "gradient descent step: 5\n",
      "indices for batch: [63, 116]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.83412003 0.18448276 0.6501548  0.61666667 0.54320988 0.90721649\n",
      " 0.73818898 0.36363636 0.54469274 0.34615385 0.60818713 0.6925\n",
      " 0.39285714]\n",
      "class: 0\n",
      "softmax numerator: 0.5064451432852014\n",
      "softmax denominator: 3.462451003905159\n",
      "y hat for class: 0.1462678151153626\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.12200491 0.02698389 0.09509672 0.09019849 0.07945412 0.13269657\n",
      " 0.10797329 0.0531883  0.07967102 0.05063117 0.0889582  0.10129046\n",
      " 0.05746236]\n",
      "new gradient for class: [0.12200491 0.02698389 0.09509672 0.09019849 0.07945412 0.13269657\n",
      " 0.10797329 0.0531883  0.07967102 0.05063117 0.0889582  0.10129046\n",
      " 0.05746236]\n",
      "class: 1\n",
      "softmax numerator: 1.9362006445573903\n",
      "softmax denominator: 3.462451003905159\n",
      "y hat for class: 0.5591994348435912\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.36768058 -0.0813201  -0.2865886  -0.27182702 -0.23944722 -0.39990154\n",
      " -0.32539412 -0.16029111 -0.24010087 -0.15258481 -0.26808923 -0.30525439\n",
      " -0.17317165]\n",
      "new gradient for class: [-0.36768058 -0.0813201  -0.2865886  -0.27182702 -0.23944722 -0.39990154\n",
      " -0.32539412 -0.16029111 -0.24010087 -0.15258481 -0.26808923 -0.30525439\n",
      " -0.17317165]\n",
      "class: 2\n",
      "softmax numerator: 1.0198052160625672\n",
      "softmax denominator: 3.462451003905159\n",
      "y hat for class: 0.29453275004104607\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.24567567 0.05433621 0.19149188 0.18162853 0.1599931  0.26720497\n",
      " 0.21742083 0.10710282 0.16042985 0.10195364 0.17913103 0.20396393\n",
      " 0.11570929]\n",
      "new gradient for class: [0.24567567 0.05433621 0.19149188 0.18162853 0.1599931  0.26720497\n",
      " 0.21742083 0.10710282 0.16042985 0.10195364 0.17913103 0.20396393\n",
      " 0.11570929]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 0. 1.]\n",
      "x: [0.862441   0.46034483 0.76780186 0.73333333 0.69135802 0.3814433\n",
      " 0.26771654 0.36363636 0.35195531 0.83076923 0.28070175 0.3675\n",
      " 0.28571429]\n",
      "class: 0\n",
      "softmax numerator: 0.46498348777602\n",
      "softmax denominator: 3.4245158445289925\n",
      "y hat for class: 0.1357807961434542\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.11710293 0.06250599 0.10425275 0.09957258 0.09387314 0.05179267\n",
      " 0.03635076 0.04937483 0.04778877 0.11280251 0.03811391 0.04989944\n",
      " 0.03879451]\n",
      "new gradient for class: [0.23910784 0.08948988 0.19934947 0.18977107 0.17332726 0.18448925\n",
      " 0.14432405 0.10256313 0.12745979 0.16343367 0.12707211 0.1511899\n",
      " 0.09625687]\n",
      "class: 1\n",
      "softmax numerator: 1.6774880527681313\n",
      "softmax denominator: 3.4245158445289925\n",
      "y hat for class: 0.48984677803377275\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.42246394 0.22549843 0.37610527 0.35922097 0.3386595  0.18684877\n",
      " 0.13114008 0.1781261  0.17240417 0.40694963 0.13750085 0.18001869\n",
      " 0.13995622]\n",
      "new gradient for class: [ 0.05478336  0.14417833  0.08951666  0.08739396  0.09921228 -0.21305277\n",
      " -0.19425404  0.01783499 -0.06769669  0.25436482 -0.13058838 -0.1252357\n",
      " -0.03321543]\n",
      "class: 2\n",
      "softmax numerator: 1.2820443039848413\n",
      "softmax denominator: 3.4245158445289925\n",
      "y hat for class: 0.3743724258227731\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.53956687 -0.28800442 -0.48035801 -0.45879355 -0.43253264 -0.23864145\n",
      " -0.16749085 -0.22750094 -0.22019295 -0.51975214 -0.17561476 -0.22991813\n",
      " -0.17875074]\n",
      "new gradient for class: [-0.2938912  -0.2336682  -0.28886613 -0.27716503 -0.27253955  0.02856352\n",
      "  0.04992998 -0.12039812 -0.0597631  -0.41779849  0.00351627 -0.0259542\n",
      " -0.06304144]\n",
      "w for class:  [-0.29579536 -0.06029386 -0.28890145 -0.47697672 -0.11079789 -0.05656038\n",
      "  0.03498377 -0.56767722 -0.0494772  -0.06165776 -0.25058969 -0.07387633\n",
      " -0.00873769]\n",
      "w for class:  [ 0.08119895 -0.10381897  0.11639699  0.33683868 -0.08051022  0.11275789\n",
      "  0.1064939   0.31780178  0.05842198 -0.30710402  0.28167932  0.17533078\n",
      " -0.08211596]\n",
      "w for class:  [ 0.21459641  0.16411282  0.17250446  0.14013804  0.1913081  -0.05619751\n",
      " -0.14147767  0.24987544 -0.00894478  0.36876178 -0.03108963 -0.10145446\n",
      "  0.09085365]\n",
      "###################################################\n",
      "gradient descent step: 6\n",
      "indices for batch: [103, 68]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 0. 1.]\n",
      "x: [0.90964262 0.61896552 0.67801858 0.65       0.54320988 0.41752577\n",
      " 0.09448819 0.87878788 0.24581006 0.43846154 0.47368421 0.455\n",
      " 0.3452381 ]\n",
      "class: 0\n",
      "softmax numerator: 0.2047030725624062\n",
      "softmax denominator: 4.661646000612706\n",
      "y hat for class: 0.04391218735517475\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.0399444  0.02718013 0.02977328 0.02854292 0.02385353 0.01833447\n",
      " 0.00414918 0.0385895  0.01079406 0.01925381 0.02080051 0.01998005\n",
      " 0.01516016]\n",
      "new gradient for class: [0.0399444  0.02718013 0.02977328 0.02854292 0.02385353 0.01833447\n",
      " 0.00414918 0.0385895  0.01079406 0.01925381 0.02080051 0.01998005\n",
      " 0.01516016]\n",
      "class: 1\n",
      "softmax numerator: 1.9439358044880266\n",
      "softmax denominator: 4.661646000612706\n",
      "y hat for class: 0.41700631155444334\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.37932671 0.25811253 0.28273803 0.2710541  0.22652195 0.17411088\n",
      " 0.03940217 0.36646009 0.10250434 0.18284123 0.19752931 0.18973787\n",
      " 0.14396646]\n",
      "new gradient for class: [0.37932671 0.25811253 0.28273803 0.2710541  0.22652195 0.17411088\n",
      " 0.03940217 0.36646009 0.10250434 0.18284123 0.19752931 0.18973787\n",
      " 0.14396646]\n",
      "class: 2\n",
      "softmax numerator: 2.5130071235622724\n",
      "softmax denominator: 4.661646000612706\n",
      "y hat for class: 0.5390815010903819\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.41927111 -0.28529266 -0.3125113  -0.29959702 -0.25037548 -0.19244535\n",
      " -0.04355135 -0.40504959 -0.1132984  -0.20209503 -0.21832982 -0.20971792\n",
      " -0.15912662]\n",
      "new gradient for class: [-0.41927111 -0.28529266 -0.3125113  -0.29959702 -0.25037548 -0.19244535\n",
      " -0.04355135 -0.40504959 -0.1132984  -0.20209503 -0.21832982 -0.20971792\n",
      " -0.15912662]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.79703304 0.29655172 0.58204334 0.65       0.5308642  0.6443299\n",
      " 0.32283465 0.56060606 0.39664804 0.15846154 0.5497076  0.61\n",
      " 0.24702381]\n",
      "class: 0\n",
      "softmax numerator: 0.25965332462398627\n",
      "softmax denominator: 4.20620509550292\n",
      "y hat for class: 0.06173101851395587\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.04920166 0.01830644 0.03593013 0.04012516 0.03277079 0.03977514\n",
      " 0.01992891 0.03460678 0.02448549 0.00978199 0.03393401 0.03765592\n",
      " 0.01524903]\n",
      "new gradient for class: [0.08914606 0.04548657 0.06570341 0.06866808 0.05662432 0.05810961\n",
      " 0.02407809 0.07319628 0.03527955 0.0290358  0.05473452 0.05763597\n",
      " 0.03040919]\n",
      "class: 1\n",
      "softmax numerator: 2.1795007453819353\n",
      "softmax denominator: 4.20620509550292\n",
      "y hat for class: 0.5181632126574515\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.38403984 -0.14288953 -0.28044989 -0.31319391 -0.2557899  -0.31046185\n",
      " -0.15555361 -0.27012062 -0.19111962 -0.0763526  -0.26486935 -0.29392044\n",
      " -0.11902516]\n",
      "new gradient for class: [-0.00471313  0.115223    0.00228813 -0.04213981 -0.02926795 -0.13635096\n",
      " -0.11615144  0.09633947 -0.08861527  0.10648863 -0.06734004 -0.10418257\n",
      "  0.02494131]\n",
      "class: 2\n",
      "softmax numerator: 1.7670510254969982\n",
      "softmax denominator: 4.20620509550292\n",
      "y hat for class: 0.4201057688285927\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.33483818 0.12458309 0.24451977 0.27306875 0.22301911 0.27068671\n",
      " 0.1356247  0.23551384 0.16663413 0.06657061 0.23093533 0.25626452\n",
      " 0.10377613]\n",
      "new gradient for class: [-0.08443293 -0.16070957 -0.06799154 -0.02652827 -0.02735637  0.07824135\n",
      "  0.09207334 -0.16953575  0.05333573 -0.13552443  0.01260552  0.0465466\n",
      " -0.0553505 ]\n",
      "w for class:  [-0.34036839 -0.08303714 -0.32175316 -0.51131076 -0.13911005 -0.08561519\n",
      "  0.02294473 -0.60427536 -0.06711697 -0.07617566 -0.27795695 -0.10269431\n",
      " -0.02394229]\n",
      "w for class:  [ 0.08355552 -0.16143047  0.11525292  0.35790858 -0.06587624  0.18093337\n",
      "  0.16456962  0.26963205  0.10272961 -0.36034834  0.31534934  0.22742207\n",
      " -0.09458661]\n",
      "w for class:  [ 0.25681288  0.24446761  0.20650023  0.15340218  0.20498629 -0.09531818\n",
      " -0.18751435  0.33464331 -0.03561265  0.436524   -0.03739239 -0.12472776\n",
      "  0.11852889]\n",
      "###################################################\n",
      "gradient descent step: 7\n",
      "indices for batch: [24, 127]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.83412003 0.16206897 0.42105263 0.35333333 0.54320988 0.51030928\n",
      " 0.11220472 0.42424242 0.11731844 0.15       0.61403509 0.455\n",
      " 0.30952381]\n",
      "class: 0\n",
      "softmax numerator: 0.292100533217195\n",
      "softmax denominator: 3.9932581118400634\n",
      "y hat for class: 0.0731484229259093\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.06101456 0.01185509 0.03079934 0.02584578 0.03973495 0.03732832\n",
      " 0.0082076  0.03103266 0.00858166 0.01097226 0.0449157  0.03328253\n",
      " 0.02264118]\n",
      "new gradient for class: [0.06101456 0.01185509 0.03079934 0.02584578 0.03973495 0.03732832\n",
      " 0.0082076  0.03103266 0.00858166 0.01097226 0.0449157  0.03328253\n",
      " 0.02264118]\n",
      "class: 1\n",
      "softmax numerator: 1.884681531681604\n",
      "softmax denominator: 3.9932581118400634\n",
      "y hat for class: 0.47196586819507064\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.44044384 -0.08557795 -0.22233016 -0.18657206 -0.28683336 -0.26946072\n",
      " -0.05924792 -0.22401448 -0.06194814 -0.07920512 -0.32423148 -0.24025553\n",
      " -0.16343914]\n",
      "new gradient for class: [-0.44044384 -0.08557795 -0.22233016 -0.18657206 -0.28683336 -0.26946072\n",
      " -0.05924792 -0.22401448 -0.06194814 -0.07920512 -0.32423148 -0.24025553\n",
      " -0.16343914]\n",
      "class: 2\n",
      "softmax numerator: 1.8164760469412646\n",
      "softmax denominator: 3.9932581118400634\n",
      "y hat for class: 0.45488570887902013\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.37942928 0.07372286 0.19153082 0.16072628 0.24709841 0.2321324\n",
      " 0.05104033 0.19298182 0.05336648 0.06823286 0.27931579 0.206973\n",
      " 0.14079796]\n",
      "new gradient for class: [0.37942928 0.07372286 0.19153082 0.16072628 0.24709841 0.2321324\n",
      " 0.05104033 0.19298182 0.05336648 0.06823286 0.27931579 0.206973\n",
      " 0.14079796]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 0. 1.]\n",
      "x: [0.86648685 0.5637931  0.79876161 0.73333333 0.65432099 0.42525773\n",
      " 0.11811024 0.90909091 0.26815642 0.42923077 0.50877193 0.5275\n",
      " 0.33928571]\n",
      "class: 0\n",
      "softmax numerator: 0.14924378434332036\n",
      "softmax denominator: 5.425628226513171\n",
      "y hat for class: 0.02750718960322743\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.02383462 0.01550836 0.02197169 0.02017194 0.01799853 0.01169765\n",
      " 0.00324888 0.02500654 0.00737623 0.01180693 0.01399489 0.01451004\n",
      " 0.0093328 ]\n",
      "new gradient for class: [0.08484918 0.02736345 0.05277102 0.04601772 0.05773348 0.04902596\n",
      " 0.01145648 0.0560392  0.01595789 0.0227792  0.05891058 0.04779257\n",
      " 0.03197397]\n",
      "class: 1\n",
      "softmax numerator: 2.128671225634941\n",
      "softmax denominator: 5.425628226513171\n",
      "y hat for class: 0.3923363593607207\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.3399543  0.22119653 0.31338322 0.28771333 0.25671391 0.16684407\n",
      " 0.04633894 0.35666942 0.10520752 0.16840284 0.19960973 0.20695743\n",
      " 0.13311412]\n",
      "new gradient for class: [-0.10048955  0.13561859  0.09105306  0.10114127 -0.03011944 -0.10261665\n",
      " -0.01290898  0.13265494  0.04325938  0.08919772 -0.12462176 -0.0332981\n",
      " -0.03032501]\n",
      "class: 2\n",
      "softmax numerator: 3.1477132165349087\n",
      "softmax denominator: 5.425628226513171\n",
      "y hat for class: 0.5801564510360517\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.36378891 -0.2367049  -0.33535491 -0.30788527 -0.27471245 -0.17854172\n",
      " -0.04958782 -0.38167595 -0.11258374 -0.18020977 -0.21360461 -0.22146747\n",
      " -0.14244692]\n",
      "new gradient for class: [ 0.01564037 -0.16298204 -0.14382408 -0.14715899 -0.02761404  0.05359068\n",
      "  0.0014525  -0.18869414 -0.05921727 -0.11197691  0.06571117 -0.01449447\n",
      " -0.00164896]\n",
      "w for class:  [-0.38279298 -0.09671887 -0.34813867 -0.53431962 -0.16797679 -0.11012817\n",
      "  0.01721649 -0.63229496 -0.07509591 -0.08756526 -0.30741224 -0.1265906\n",
      " -0.03992927]\n",
      "w for class:  [ 0.13380029 -0.22923976  0.06972639  0.30733795 -0.05081652  0.2322417\n",
      "  0.17102411  0.20330458  0.08109993 -0.4049472   0.37766022  0.24407112\n",
      " -0.0794241 ]\n",
      "w for class:  [ 0.24899269  0.32595863  0.27841227  0.22698167  0.21879331 -0.12211352\n",
      " -0.1882406   0.42899038 -0.00600401  0.49251245 -0.07024798 -0.11748052\n",
      "  0.11935338]\n",
      "###################################################\n",
      "gradient descent step: 8\n",
      "indices for batch: [34, 42]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.82333109 0.20517241 0.54179567 0.56       0.93209877 0.47680412\n",
      " 0.2519685  0.21212121 0.69832402 0.21923077 0.74853801 0.7675\n",
      " 0.42738095]\n",
      "class: 0\n",
      "softmax numerator: 0.20645239667144147\n",
      "softmax denominator: 4.6112747378630745\n",
      "y hat for class: 0.04477122019563602\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.03686154 0.00918582 0.02425685 0.02507188 0.0417312  0.0213471\n",
      " 0.01128094 0.00949693 0.03126482 0.00981523 0.03351296 0.03436191\n",
      " 0.01913437]\n",
      "new gradient for class: [0.03686154 0.00918582 0.02425685 0.02507188 0.0417312  0.0213471\n",
      " 0.01128094 0.00949693 0.03126482 0.00981523 0.03351296 0.03436191\n",
      " 0.01913437]\n",
      "class: 1\n",
      "softmax numerator: 2.285377145537029\n",
      "softmax denominator: 4.6112747378630745\n",
      "y hat for class: 0.4956063725225149\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.41528295 -0.10348766 -0.27327828 -0.28246043 -0.47014468 -0.24049696\n",
      " -0.12709131 -0.10699259 -0.35223019 -0.1105786  -0.3775578  -0.38712211\n",
      " -0.21556823]\n",
      "new gradient for class: [-0.41528295 -0.10348766 -0.27327828 -0.28246043 -0.47014468 -0.24049696\n",
      " -0.12709131 -0.10699259 -0.35223019 -0.1105786  -0.3775578  -0.38712211\n",
      " -0.21556823]\n",
      "class: 2\n",
      "softmax numerator: 2.119445195654604\n",
      "softmax denominator: 4.6112747378630745\n",
      "y hat for class: 0.45962240728184905\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.37842142 0.09430184 0.24902143 0.25738855 0.42841348 0.21914986\n",
      " 0.11581037 0.09749566 0.32096537 0.10076337 0.34404484 0.3527602\n",
      " 0.19643386]\n",
      "new gradient for class: [0.37842142 0.09430184 0.24902143 0.25738855 0.42841348 0.21914986\n",
      " 0.11581037 0.09749566 0.32096537 0.10076337 0.34404484 0.3527602\n",
      " 0.19643386]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.79838166 0.49827586 0.69040248 0.6        0.69135802 0.44329897\n",
      " 0.25984252 0.65151515 0.26536313 0.20384615 0.56140351 0.63\n",
      " 0.29761905]\n",
      "class: 0\n",
      "softmax numerator: 0.16709734250922134\n",
      "softmax denominator: 5.110300616940169\n",
      "y hat for class: 0.03269814342336521\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.0261056  0.0162927  0.02257488 0.01961889 0.02260612 0.01449505\n",
      " 0.00849637 0.02130334 0.00867688 0.00666539 0.01835685 0.02059983\n",
      " 0.00973159]\n",
      "new gradient for class: [0.06296714 0.02547851 0.04683173 0.04469077 0.06433732 0.03584216\n",
      " 0.01977731 0.03080026 0.0399417  0.01648062 0.05186981 0.05496174\n",
      " 0.02886596]\n",
      "class: 1\n",
      "softmax numerator: 2.1190693370841687\n",
      "softmax denominator: 5.110300616940169\n",
      "y hat for class: 0.41466627815585877\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.46731971 -0.29165766 -0.40411585 -0.35120023 -0.40467517 -0.25947784\n",
      " -0.15209459 -0.38135379 -0.15532599 -0.11931803 -0.32860841 -0.36876024\n",
      " -0.17420646]\n",
      "new gradient for class: [-0.88260266 -0.39514532 -0.67739413 -0.63366066 -0.87481984 -0.4999748\n",
      " -0.2791859  -0.48834638 -0.50755617 -0.22989663 -0.70616621 -0.75588235\n",
      " -0.38977469]\n",
      "class: 2\n",
      "softmax numerator: 2.824133937346779\n",
      "softmax denominator: 5.110300616940169\n",
      "y hat for class: 0.552635578420776\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.44121411 0.27536497 0.38154097 0.33158135 0.38206904 0.24498278\n",
      " 0.14359822 0.36005045 0.14664911 0.11265264 0.31025155 0.34816041\n",
      " 0.16447487]\n",
      "new gradient for class: [0.81963553 0.36966681 0.6305624  0.5889699  0.81048252 0.46413264\n",
      " 0.25940859 0.45754611 0.46761447 0.21341601 0.6542964  0.70092061\n",
      " 0.36090874]\n",
      "w for class:  [-0.41427655 -0.10945813 -0.37155453 -0.55666501 -0.20014545 -0.12804925\n",
      "  0.00732783 -0.64769509 -0.09506676 -0.09580557 -0.33334715 -0.15407147\n",
      " -0.05436225]\n",
      "w for class:  [ 0.57510162 -0.0316671   0.40842346  0.62416828  0.3865934   0.48222909\n",
      "  0.31061706  0.44747777  0.33487801 -0.28999888  0.73074332  0.62201229\n",
      "  0.11546324]\n",
      "w for class:  [-0.16082507  0.14112522 -0.03686893 -0.06750327 -0.18644795 -0.35417984\n",
      " -0.31794489  0.20021732 -0.23981125  0.38580445 -0.39739618 -0.46794083\n",
      " -0.06110099]\n",
      "###################################################\n",
      "gradient descent step: 9\n",
      "indices for batch: [35, 69]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.82872556 0.27758621 0.68421053 0.68       0.63580247 0.28350515\n",
      " 0.2007874  0.56060606 0.40782123 0.23461538 0.52982456 0.455\n",
      " 0.51785714]\n",
      "class: 0\n",
      "softmax numerator: 0.15446882008114213\n",
      "softmax denominator: 14.986056033082832\n",
      "y hat for class: 0.010307503170957103\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.00854209 0.00286122 0.0070525  0.0070091  0.00655354 0.00292223\n",
      " 0.00206962 0.00577845 0.00420362 0.0024183  0.00546117 0.00468991\n",
      " 0.00533781]\n",
      "new gradient for class: [0.00854209 0.00286122 0.0070525  0.0070091  0.00655354 0.00292223\n",
      " 0.00206962 0.00577845 0.00420362 0.0024183  0.00546117 0.00468991\n",
      " 0.00533781]\n",
      "class: 1\n",
      "softmax numerator: 14.381437612341502\n",
      "softmax denominator: 14.986056033082832\n",
      "y hat for class: 0.9596546002893229\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.03343526 -0.01119933 -0.02760475 -0.02743487 -0.0256517  -0.01143813\n",
      " -0.00810085 -0.02261788 -0.01645371 -0.00946565 -0.02137598 -0.01835716\n",
      " -0.02089315]\n",
      "new gradient for class: [-0.03343526 -0.01119933 -0.02760475 -0.02743487 -0.0256517  -0.01143813\n",
      " -0.00810085 -0.02261788 -0.01645371 -0.00946565 -0.02137598 -0.01835716\n",
      " -0.02089315]\n",
      "class: 2\n",
      "softmax numerator: 0.45014960066018717\n",
      "softmax denominator: 14.986056033082832\n",
      "y hat for class: 0.03003789653971989\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.02489317 0.00833811 0.02055225 0.02042577 0.01909817 0.0085159\n",
      " 0.00603123 0.01683943 0.01225009 0.00704735 0.01591482 0.01366724\n",
      " 0.01555534]\n",
      "new gradient for class: [0.02489317 0.00833811 0.02055225 0.02042577 0.01909817 0.0085159\n",
      " 0.00603123 0.01683943 0.01225009 0.00704735 0.01591482 0.01366724\n",
      " 0.01555534]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.84356035 0.29827586 0.6130031  0.68333333 0.52469136 0.56701031\n",
      " 0.37795276 0.48484848 0.41340782 0.22615385 0.60818713 0.8925\n",
      " 0.4       ]\n",
      "class: 0\n",
      "softmax numerator: 0.1492531629329919\n",
      "softmax denominator: 22.254500986576247\n",
      "y hat for class: 0.006706650624204979\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.00565746 0.00200043 0.0041112  0.00458288 0.00351892 0.00380274\n",
      " 0.0025348  0.00325171 0.00277258 0.00151673 0.0040789  0.00598569\n",
      " 0.00268266]\n",
      "new gradient for class: [0.01419956 0.00486165 0.0111637  0.01159198 0.01007246 0.00672497\n",
      " 0.00460441 0.00903016 0.0069762  0.00393503 0.00954007 0.0106756\n",
      " 0.00802047]\n",
      "class: 1\n",
      "softmax numerator: 21.79787727412705\n",
      "softmax denominator: 22.254500986576247\n",
      "y hat for class: 0.979481736628261\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.01730839 -0.0061201  -0.01257776 -0.01402081 -0.01076576 -0.01163407\n",
      " -0.00775493 -0.00994825 -0.00848241 -0.00464028 -0.01247894 -0.01831255\n",
      " -0.00820731]\n",
      "new gradient for class: [-0.05074366 -0.01731943 -0.04018251 -0.04145569 -0.03641746 -0.0230722\n",
      " -0.01585578 -0.03256612 -0.02493612 -0.01410594 -0.03385493 -0.03666971\n",
      " -0.02910046]\n",
      "class: 2\n",
      "softmax numerator: 0.3073705495162023\n",
      "softmax denominator: 22.254500986576247\n",
      "y hat for class: 0.013811612747533902\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.01165093 0.00411967 0.00846656 0.00943794 0.00724683 0.00783133\n",
      " 0.00522014 0.00669654 0.00570983 0.00312355 0.00840005 0.01232686\n",
      " 0.00552465]\n",
      "new gradient for class: [0.0365441  0.01245778 0.02901881 0.02986371 0.026345   0.01634723\n",
      " 0.01125137 0.02353597 0.01795992 0.0101709  0.02431486 0.02599411\n",
      " 0.02107998]\n",
      "w for class:  [-0.42137633 -0.11188895 -0.37713638 -0.562461   -0.20518168 -0.13141173\n",
      "  0.00502563 -0.65221017 -0.09855486 -0.09777308 -0.33811718 -0.15940927\n",
      " -0.05837249]\n",
      "w for class:  [ 0.60047345 -0.02300738  0.42851471  0.64489612  0.40480213  0.49376519\n",
      "  0.31854495  0.46376083  0.34734607 -0.28294591  0.74767079  0.64034715\n",
      "  0.13001347]\n",
      "w for class:  [-0.17909712  0.13489634 -0.05137833 -0.08243513 -0.19962046 -0.36235346\n",
      " -0.32357058  0.18844934 -0.24879121  0.380719   -0.40955361 -0.48093788\n",
      " -0.07164099]\n",
      "###################################################\n"
     ]
    }
   ],
   "source": [
    "xtest = np.asarray([[1, 2, 3, 4], [5, 6, 7, 8], [1, 1, 2, 2]])\n",
    "ytest = np.asarray([[0, 1, 0], [1, 0, 1], [0, 0, 1]])\n",
    "\n",
    "logisticRegressionModel.fit(xWineTrainingSets[0], yWineTrainingSets[0], gradientDescentModel)\n",
    "\n",
    "#logisticRegressionModel.fit(xtest, ytest, gradientDescentModel)\n",
    "\n",
    "\n",
    "\n",
    "#xWineTrainingSets = []\n",
    "#yWineTrainingSets = []\n",
    "#xWineValidationSets = []\n",
    "#yWineValidationSets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
