{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lia\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import cv2 as cv\n",
    "from sklearn import svm \n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "wine = fetch_openml(name='wine', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of digits and wine data\n",
    "\n",
    "digits_data_norm = []\n",
    "\n",
    "for col in digits.data:\n",
    "    col_norm = col/np.max(col)\n",
    "    digits_data_norm.append(col_norm)\n",
    "\n",
    "digits.data = np.asarray(digits_data_norm)\n",
    "\n",
    "wine_data_norm = []\n",
    "\n",
    "for col in wine.data.T:\n",
    "    col_norm = col/np.amax(col)\n",
    "    wine_data_norm.append(col_norm)\n",
    "    \n",
    "wine.data = np.asarray(wine_data_norm).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation for digits dataset\n",
    "\n",
    "digitsTrainingSetSize = int(np.ceil(0.8 * len(digits.data)))\n",
    "digitsValidationSetSize = int(len(digits.data) - digitsTrainingSetSize)\n",
    "\n",
    "xDigitsTrainingSets = []\n",
    "yDigitsTrainingSets = []\n",
    "xDigitsValidationSets = []\n",
    "yDigitsValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data[foldIndex*digitsValidationSetSize:((foldIndex*digitsValidationSetSize)+digitsValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(digits.target[index+(foldIndex*digitsValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(digits.target[index])\n",
    "            \n",
    "    xDigitsTrainingSets.append(xTrainingSet)\n",
    "    yDigitsTrainingSets.append(yTrainingSet)\n",
    "    xDigitsValidationSets.append(xValidationSet)\n",
    "    yDigitsValidationSets.append(yValidationSet)\n",
    "    \n",
    "# 5-fold cross validation for wine dataset\n",
    "\n",
    "wineTrainingSetSize = int(np.ceil(0.8 * len(wine.data)))\n",
    "wineValidationSetSize = int(len(wine.data) - wineTrainingSetSize)\n",
    "\n",
    "xWineTrainingSets = []\n",
    "yWineTrainingSets = []\n",
    "xWineValidationSets = []\n",
    "yWineValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "    for index, data in enumerate(wine.data[foldIndex*wineValidationSetSize:((foldIndex*wineValidationSetSize)+wineValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(wine.target[index+(foldIndex*wineValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "    \n",
    "    for index, data in enumerate(wine.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(wine.target[index])\n",
    "            \n",
    "    xWineTrainingSets.append(xTrainingSet)\n",
    "    yWineTrainingSets.append(yTrainingSet)\n",
    "    xWineValidationSets.append(xValidationSet)\n",
    "    yWineValidationSets.append(yValidationSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of y for digits dataset\n",
    "\n",
    "numberOfDigitsTargets = 10\n",
    "numberOfWineTargets = 3\n",
    "\n",
    "for index, fold in enumerate(yDigitsTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yDigitsValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsValidationSets[index] = encodedFold\n",
    "\n",
    "# one-hot encoding of y for wine dataset\n",
    "\n",
    "for index, fold in enumerate(yWineTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yWineValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineValidationSets[index] = encodedFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomIndices(arr, batch_size):\n",
    "    indices = []\n",
    "    \n",
    "    if batch_size > len(arr):\n",
    "        print(\"Error: batch size larger than size of dataset.\")\n",
    "        return\n",
    "    \n",
    "    while batch_size > 0:\n",
    "        x = np.floor(np.random.random() * len(arr))\n",
    "        if x not in indices:\n",
    "            indices.append(int(x))\n",
    "            batch_size -= 1\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent class\n",
    " \n",
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, batch_size, learning_rate=0.5, momentum=0.9, max_termination_condition=25, max_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        self.max_termination_condition = max_termination_condition\n",
    "        self.max_iters = max_iters\n",
    "        self.deltas = []\n",
    "        \n",
    "    def run(self, gradient_fn, x, y, w):\n",
    "        t = 1\n",
    "        \n",
    "        max_accuracy = -1\n",
    "        termination_count = 0        \n",
    "        weight_history = []\n",
    "        error_history = []\n",
    "                \n",
    "        for number_of_targets in range(len(y[0])):\n",
    "            weight_history.append([])\n",
    "        \n",
    "        while termination_count < self.max_termination_condition and t < self.max_iters:\n",
    "            gradients = gradient_fn(x, y, w, self.batch_size)   \n",
    "            \n",
    "            for c in range(len(y[0])):\n",
    "                if(t==1):\n",
    "                    w[c] = w[c] - self.learning_rate * gradients[c]\n",
    "                else:\n",
    "                    delta_w = (self.momentum)*(self.deltas[-(len(y[0]))]) + (1-self.momentum)*gradients[c]\n",
    "                    w[c] = w[c] - (self.learning_rate)*(delta_w)\n",
    "                self.deltas.append(w[c])\n",
    "            \n",
    "            a = np.asarray(x)\n",
    "            b = np.asarray(w)\n",
    "    \n",
    "            yh=[]\n",
    "            for i, x_c in enumerate(a):\n",
    "                yh_x=[]\n",
    "\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  b[c] @ x_c\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  b[i] @ x_c\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "                    yh_x.append(yh_c)\n",
    "                    \n",
    "                yh.append(yh_x)\n",
    "                \n",
    "            step_accuracy = 0\n",
    "                \n",
    "            def accurate(a, b):\n",
    "                return np.argmax(a) == np.argmax(b)\n",
    "                \n",
    "            for sample_index, yh_x in enumerate(yh):\n",
    "                if accurate(yh_x, y[sample_index]):\n",
    "                    step_accuracy += 1\n",
    "                    \n",
    "            step_accuracy /= len(x)\n",
    "            \n",
    "            for c in range(len(b)):\n",
    "                weight_history[c].append(w[c])\n",
    "            \n",
    "            error_history.append(step_accuracy)\n",
    "            \n",
    "            # We use an alternate termination condition that terminates faster as\n",
    "            # the suggested condition ran for a longtime for us (~1hr).\n",
    "            \n",
    "            # We track the best training accuracy encountered, and if the \n",
    "            # next max_termination_condition-number of steps do not have a better accuracy, then\n",
    "            # it terminates.\n",
    "\n",
    "            if step_accuracy > max_accuracy:\n",
    "                max_accuracy = step_accuracy\n",
    "                termination_count = 0\n",
    "                print(f\"\\t\\tStep {t}: new best accuracy of {max_accuracy:.3f}\")\n",
    "            else:\n",
    "                termination_count += 1\n",
    "                print(f\"\\t\\tStep {t}\")\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        # take the weight prior to the last max_termination_condition-number of weights\n",
    "        # (as it is guaranteed to be the best prior to termination).\n",
    "        \n",
    "        index_best = len(error_history)-self.max_termination_condition-1\n",
    "        \n",
    "        w_best = []\n",
    "        \n",
    "        for c in range(len(y[0])):\n",
    "            w_best.append(weight_history[c][index_best])\n",
    "        \n",
    "        return w_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, add_bias=True):\n",
    "        self.add_bias = add_bias\n",
    "        pass\n",
    "            \n",
    "    def fit(self, x, y, optimizer):\n",
    "        def gradient(x, y, w, batch_size):\n",
    "            gradients = np.zeros(len(w)).tolist()\n",
    "\n",
    "            indices = getRandomIndices(x, batch_size)\n",
    "\n",
    "            for index in indices:\n",
    "                a = np.asarray(x[index])\n",
    "                b = np.asarray(y[index])\n",
    "\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  w[c] @ a\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  w[i] @ a\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "\n",
    "                    y_c = b[c]\n",
    "                    \n",
    "                    cost_c = np.dot(yh_c - y_c, a)\n",
    "                    \n",
    "                    gradients[c] += cost_c\n",
    "\n",
    "            return gradients\n",
    "        \n",
    "        if self.add_bias:\n",
    "            x = np.asarray(x)\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "\n",
    "        w0 = []\n",
    "        for c in range(len(y[0])):\n",
    "            w0.append(np.zeros(len(x[0])))\n",
    "            \n",
    "        self.w = optimizer.run(gradient, x, y, w0)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if self.add_bias:\n",
    "            x = np.asarray(x)\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "\n",
    "        a = np.asarray(x)\n",
    "        b = np.asarray(self.w)\n",
    "\n",
    "        yh=[]\n",
    "        \n",
    "        for i, x_c in enumerate(a):\n",
    "            yh_x=[]\n",
    "            \n",
    "            for c in range(len(b)):\n",
    "                w_x =  b[c] @ x_c\n",
    "                num = np.exp(w_x)\n",
    "\n",
    "                den = 0\n",
    "                for i in range(len(b)):\n",
    "                    w_x =  b[i] @ x_c\n",
    "                    den += np.exp(w_x)\n",
    "\n",
    "                yh_c = num/den\n",
    "                yh_x.append(yh_c)\n",
    "                \n",
    "            yh.append(yh_x)\n",
    "        \n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def runLogisticRegression(batch_size, learning_rate, momentum):\n",
    "    def accurate(a, b):\n",
    "        return np.argmax(a) == np.argmax(b)\n",
    "\n",
    "    def cost(yh, y):\n",
    "        return y * np.log1p(np.exp(-yh)) + (1-yh) * np.log1p(np.exp(yh))\n",
    "\n",
    "    print(\"Model hyper-parameters:\")\n",
    "    print(\"\\tMini-batch size:\", batch_size)\n",
    "    print(\"\\tLearning rate:\", learning_rate)\n",
    "    print(\"\\tMomentum:\", momentum)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    digits_training_accuracy = 0\n",
    "    digits_training_cost = 0\n",
    "    digits_validation_accuracy = 0\n",
    "    digits_validation_cost = 0\n",
    "\n",
    "    print(\"Digits gradient descent:\")\n",
    "\n",
    "    for fold_index, fold in enumerate(xDigitsTrainingSets):\n",
    "        print(f\"\\tCross-validation fold {fold_index+1}\")\n",
    "\n",
    "        gradientDescentModel = GradientDescent(batch_size, learning_rate, momentum)\n",
    "        logisticRegressionModel = LogisticRegression(add_bias=True)\n",
    "\n",
    "        logisticRegressionModel.fit(fold, yDigitsTrainingSets[fold_index], gradientDescentModel)\n",
    "        yh_training = logisticRegressionModel.predict(xDigitsTrainingSets[fold_index])\n",
    "        yh_validation = logisticRegressionModel.predict(xDigitsValidationSets[fold_index])\n",
    "\n",
    "        for sample_index, yh_x in enumerate(yh_training):\n",
    "            if accurate(yh_x, yDigitsTrainingSets[fold_index][sample_index]):\n",
    "                digits_training_accuracy += 1\n",
    "            c = np.argmax(yDigitsTrainingSets[fold_index][sample_index])\n",
    "            cst = cost(yh_x[c], yDigitsTrainingSets[fold_index][sample_index][c])\n",
    "            digits_training_cost += cst\n",
    "\n",
    "        for sample_index, yh_x in enumerate(yh_validation):\n",
    "            if accurate(yh_x, yDigitsValidationSets[fold_index][sample_index]):\n",
    "                digits_validation_accuracy += 1\n",
    "            c = np.argmax(yDigitsValidationSets[fold_index][sample_index])\n",
    "            cst = cost(yh_x[c], yDigitsValidationSets[fold_index][sample_index][c])\n",
    "            digits_validation_cost += cst\n",
    "\n",
    "    digits_training_accuracy /= 4*len(digits.data)\n",
    "    digits_training_cost /= 4\n",
    "    digits_validation_accuracy /= len(digits.data)\n",
    "\n",
    "    wine_training_accuracy = 0\n",
    "    wine_training_cost = 0\n",
    "    wine_validation_accuracy = 0\n",
    "    wine_validation_cost = 0\n",
    "\n",
    "    print(\"Wine gradient descent:\")\n",
    "\n",
    "    for fold_index, fold in enumerate(xWineTrainingSets):\n",
    "        print(f\"\\tCross-validation fold {fold_index+1}\")\n",
    "\n",
    "        gradientDescentModel = GradientDescent(batch_size, learning_rate, momentum)\n",
    "        logisticRegressionModel = LogisticRegression(add_bias=True)\n",
    "\n",
    "        logisticRegressionModel.fit(fold, yWineTrainingSets[fold_index], gradientDescentModel)\n",
    "        yh_training = logisticRegressionModel.predict(xWineTrainingSets[fold_index])\n",
    "        yh_validation = logisticRegressionModel.predict(xWineValidationSets[fold_index])\n",
    "\n",
    "        for sample_index, yh_x in enumerate(yh_training):\n",
    "            if accurate(yh_x, yWineTrainingSets[fold_index][sample_index]):\n",
    "                wine_training_accuracy += 1\n",
    "            c = np.argmax(yWineTrainingSets[fold_index][sample_index])\n",
    "            cst = cost(yh_x[c], yWineTrainingSets[fold_index][sample_index][c])\n",
    "            wine_training_cost += cst\n",
    "\n",
    "        for sample_index, yh_x in enumerate(yh_validation):\n",
    "            if accurate(yh_x, yWineValidationSets[fold_index][sample_index]):\n",
    "                wine_validation_accuracy += 1\n",
    "            c = np.argmax(yWineValidationSets[fold_index][sample_index])\n",
    "            cst = cost(yh_x[c], yWineValidationSets[fold_index][sample_index][c])\n",
    "            wine_validation_cost += cst\n",
    "\n",
    "    wine_training_accuracy /= 4*len(wine.data)\n",
    "    wine_training_cost /= 4\n",
    "    wine_validation_accuracy /= len(wine.data)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(f\"Digits training accuracy: {digits_training_accuracy*100:.1f}%\")\n",
    "    print(f\"Digits training cost: {digits_training_cost:.3f}\")\n",
    "    print(f\"Digits validation accuracy: {digits_validation_accuracy*100:.1f}%\")\n",
    "    print(f\"Digits validation cost: {digits_validation_cost:.3f}\")\n",
    "    print(f\"Wine training accuracy: {wine_training_accuracy*100:.1f}%\")\n",
    "    print(f\"Wine training cost: {wine_training_cost:.3f}\")\n",
    "    print(f\"Wine validation accuracy: {wine_validation_accuracy*100:.1f}%\")\n",
    "    print(f\"Wine validation cost: {wine_validation_cost:.3f}\")\n",
    "    \n",
    "    return (digits_validation_accuracy, digits_validation_cost, wine_validation_accuracy, wine_validation_cost)\n",
    "\n",
    "#runLogisticRegression(30, 0.04, 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean = lambda x1, x2: np.sqrt(np.sum((x1 - x2)**2, axis=-1))\n",
    "manhattan = lambda x1, x2: np.sum(np.abs(x1 - x2), axis=-1)\n",
    "\n",
    "class KNN:\n",
    "\n",
    "    def __init__(self, K=1, dist_fn= euclidean):\n",
    "        self.dist_fn = dist_fn\n",
    "        self.K = K\n",
    "        return\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.C = len(y[0])\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        num_test = x_test.shape[0]\n",
    "        distances = self.dist_fn(self.x[None,:,:], x_test[:,None,:])\n",
    "        knns = np.zeros((num_test, self.K), dtype=int)\n",
    "        y_prob = np.zeros((num_test),dtype=int)\n",
    "        counts = np.zeros((num_test, self.C))\n",
    "        \n",
    "        for i in range(num_test):\n",
    "            knns[i,:] = np.argsort(distances[i])[:self.K]\n",
    "            k_count=np.zeros(self.K, dtype=int)\n",
    "            \n",
    "            for s, arr in enumerate(self.y[knns[i,:]]):\n",
    "                k_count[s] = np.argmax(arr)\n",
    "            \n",
    "            y_prob_i, counts_i = np.unique(k_count, return_counts=True)\n",
    "            y_prob[i] = int(y_prob_i[np.argmax(counts_i)])\n",
    "        \n",
    "        return y_prob, knns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN digits validation accuracy: 95.7%\n",
      "KNN wine validation accuracy: 85.4%\n"
     ]
    }
   ],
   "source": [
    "KNNmodel = KNN(K=11)\n",
    "\n",
    "digits_knn_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    y_prob, knns = KNNmodel.fit(np.asarray(xDigitsTrainingSets[fold]), np.asarray(yDigitsTrainingSets[fold])).predict(np.asarray(xDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, prob in enumerate(y_prob):\n",
    "        if prob == np.argmax(yDigitsValidationSets[fold][i]):\n",
    "            digits_knn_accuracy += 1\n",
    "\n",
    "digits_knn_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"KNN digits validation accuracy: {digits_knn_accuracy*100:.1f}%\")\n",
    "\n",
    "KNNmodel = KNN(K=7)\n",
    "\n",
    "wine_knn_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    y_prob, knns = KNNmodel.fit(np.asarray(xWineTrainingSets[fold]), np.asarray(yWineTrainingSets[fold])).predict(np.asarray(xWineValidationSets[fold]))\n",
    "    \n",
    "    for i, prob in enumerate(y_prob):\n",
    "        if prob == np.argmax(yWineValidationSets[fold][i]):\n",
    "            wine_knn_accuracy += 1\n",
    "            \n",
    "wine_knn_accuracy /= len(wine.data)\n",
    "\n",
    "print(f\"KNN wine validation accuracy: {wine_knn_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive base digits validation accuracy: 81.1%\n",
      "Naive base wine validation accuracy: 93.8%\n"
     ]
    }
   ],
   "source": [
    "digits_naive_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    labels_training = np.zeros(len(yDigitsTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(np.asarray(xDigitsTrainingSets[fold]), labels_training).predict(np.asarray(xDigitsValidationSets[fold]))\n",
    "\n",
    "    for i, label in enumerate(y_pred):\n",
    "        if label == labels_validation[i]:\n",
    "            digits_naive_accuracy += 1\n",
    "\n",
    "digits_naive_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"Naive base digits validation accuracy: {digits_naive_accuracy*100:.1f}%\")\n",
    "\n",
    "wine_naive_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    labels_training = np.zeros(len(yWineTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yWineTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yWineValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yWineValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(np.asarray(xWineTrainingSets[fold]), labels_training).predict(np.asarray(xWineValidationSets[fold]))\n",
    "\n",
    "    for i, label in enumerate(y_pred):\n",
    "        if label == labels_validation[i]:\n",
    "            wine_naive_accuracy += 1\n",
    "\n",
    "wine_naive_accuracy /= len(wine.data)\n",
    "\n",
    "print(f\"Naive base wine validation accuracy: {wine_naive_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HoGFeatures(img, cellSize, blockSize, nbins):\n",
    "    cell_size = (cellSize, cellSize)\n",
    "    block_size = (blockSize, blockSize)\n",
    "    \n",
    "    hog = cv.HOGDescriptor(_winSize=(img.shape[1] // cell_size[1] * cell_size[1],\n",
    "                                     img.shape[0] // cell_size[0] * cell_size[0]),\n",
    "                           _blockSize=(block_size[1] * cell_size[1],\n",
    "                                       block_size[0] * cell_size[0]),\n",
    "                           _blockStride=(cell_size[1], cell_size[0]),\n",
    "                           _cellSize=(cell_size[1], cell_size[0]),\n",
    "                           _nbins=nbins\n",
    "    )\n",
    "    \n",
    "    return hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeHoGFeatures(imageArray):\n",
    "    HoG = HoGFeatures(imageArray[0], 2, 2, 2)\n",
    "    features = []\n",
    "    \n",
    "    for i, image in enumerate(imageArray):\n",
    "        features.append(HoG.compute((image*255).astype(np.uint8)))\n",
    "        \n",
    "    features = np.array(np.squeeze(features))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC digits validation accuracy: 89.6%\n"
     ]
    }
   ],
   "source": [
    "digits_svc_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    numbers_training = []\n",
    "    numbers_validation = []\n",
    "    \n",
    "    for i, number in enumerate(xDigitsTrainingSets[fold]):\n",
    "        numbers_training.append(np.asarray(number).reshape(8, 8))\n",
    "        \n",
    "    for i, number in enumerate(xDigitsValidationSets[fold]):\n",
    "        numbers_validation.append(np.asarray(number).reshape(8, 8))  \n",
    "        \n",
    "    HoGs_training = makeHoGFeatures(np.asarray(numbers_training))\n",
    "    HoGs_validation = makeHoGFeatures(np.asarray(numbers_validation))\n",
    "\n",
    "    clf = svm.SVC(gamma='auto', C=100) \n",
    "    \n",
    "    labels_training = np.zeros(len(yDigitsTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    clf.fit(HoGs_training, labels_training)\n",
    "\n",
    "    labels_predicted = clf.predict(HoGs_validation)\n",
    "    \n",
    "    for i, label in enumerate(labels_predicted):\n",
    "        if label == labels_validation[i]:\n",
    "            digits_svc_accuracy += 1\n",
    "\n",
    "digits_svc_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"SVC digits validation accuracy: {digits_svc_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of model hyper-parameters\n",
    "\n",
    "batch_size = 30\n",
    "learning_rate = 0.04\n",
    "momentum = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hyper-parameters:\n",
      "\tMini-batch size: 30\n",
      "\tLearning rate: 0.04\n",
      "\tMomentum: 0.2\n",
      "\n",
      "\n",
      "Digits gradient descent:\n",
      "\tCross-validation fold 1\n",
      "\t\tStep 1: new best accuracy of 0.098\n",
      "\t\tStep 2: new best accuracy of 0.447\n",
      "\t\tStep 3\n",
      "\t\tStep 4\n",
      "\t\tStep 5: new best accuracy of 0.586\n",
      "\t\tStep 6\n",
      "\t\tStep 7: new best accuracy of 0.646\n",
      "\t\tStep 8\n",
      "\t\tStep 9\n",
      "\t\tStep 10: new best accuracy of 0.772\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15: new best accuracy of 0.777\n",
      "\t\tStep 16\n",
      "\t\tStep 17: new best accuracy of 0.812\n",
      "\t\tStep 18: new best accuracy of 0.828\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22: new best accuracy of 0.908\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32: new best accuracy of 0.924\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44: new best accuracy of 0.927\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52: new best accuracy of 0.934\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57: new best accuracy of 0.935\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67: new best accuracy of 0.937\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74: new best accuracy of 0.937\n",
      "\t\tStep 75\n",
      "\t\tStep 76: new best accuracy of 0.938\n",
      "\t\tStep 77\n",
      "\t\tStep 78: new best accuracy of 0.939\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92: new best accuracy of 0.941\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\t\tStep 101\n",
      "\t\tStep 102: new best accuracy of 0.947\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\t\tStep 118\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "\t\tStep 126\n",
      "\t\tStep 127\n",
      "\tCross-validation fold 2\n",
      "\t\tStep 1: new best accuracy of 0.186\n",
      "\t\tStep 2: new best accuracy of 0.344\n",
      "\t\tStep 3\n",
      "\t\tStep 4\n",
      "\t\tStep 5\n",
      "\t\tStep 6: new best accuracy of 0.590\n",
      "\t\tStep 7\n",
      "\t\tStep 8: new best accuracy of 0.670\n",
      "\t\tStep 9\n",
      "\t\tStep 10: new best accuracy of 0.844\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14: new best accuracy of 0.875\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21: new best accuracy of 0.905\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28: new best accuracy of 0.909\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31: new best accuracy of 0.913\n",
      "\t\tStep 32: new best accuracy of 0.917\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41: new best accuracy of 0.919\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44: new best accuracy of 0.923\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58: new best accuracy of 0.927\n",
      "\t\tStep 59\n",
      "\t\tStep 60: new best accuracy of 0.935\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71: new best accuracy of 0.937\n",
      "\t\tStep 72: new best accuracy of 0.940\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85: new best accuracy of 0.942\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98: new best accuracy of 0.943\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106: new best accuracy of 0.946\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\t\tStep 118: new best accuracy of 0.949\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "\t\tStep 126\n",
      "\t\tStep 127\n",
      "\t\tStep 128\n",
      "\t\tStep 129\n",
      "\t\tStep 130\n",
      "\t\tStep 131\n",
      "\t\tStep 132\n",
      "\t\tStep 133: new best accuracy of 0.951\n",
      "\t\tStep 134\n",
      "\t\tStep 135\n",
      "\t\tStep 136\n",
      "\t\tStep 137\n",
      "\t\tStep 138\n",
      "\t\tStep 139\n",
      "\t\tStep 140\n",
      "\t\tStep 141: new best accuracy of 0.953\n",
      "\t\tStep 142\n",
      "\t\tStep 143\n",
      "\t\tStep 144\n",
      "\t\tStep 145\n",
      "\t\tStep 146\n",
      "\t\tStep 147\n",
      "\t\tStep 148\n",
      "\t\tStep 149: new best accuracy of 0.955\n",
      "\t\tStep 150\n",
      "\t\tStep 151\n",
      "\t\tStep 152\n",
      "\t\tStep 153\n",
      "\t\tStep 154\n",
      "\t\tStep 155\n",
      "\t\tStep 156\n",
      "\t\tStep 157\n",
      "\t\tStep 158\n",
      "\t\tStep 159\n",
      "\t\tStep 160\n",
      "\t\tStep 161\n",
      "\t\tStep 162\n",
      "\t\tStep 163\n",
      "\t\tStep 164\n",
      "\t\tStep 165\n",
      "\t\tStep 166\n",
      "\t\tStep 167\n",
      "\t\tStep 168\n",
      "\t\tStep 169\n",
      "\t\tStep 170\n",
      "\t\tStep 171\n",
      "\t\tStep 172\n",
      "\t\tStep 173\n",
      "\t\tStep 174\n",
      "\tCross-validation fold 3\n",
      "\t\tStep 1: new best accuracy of 0.241\n",
      "\t\tStep 2\n",
      "\t\tStep 3\n",
      "\t\tStep 4: new best accuracy of 0.279\n",
      "\t\tStep 5\n",
      "\t\tStep 6: new best accuracy of 0.438\n",
      "\t\tStep 7: new best accuracy of 0.535\n",
      "\t\tStep 8\n",
      "\t\tStep 9: new best accuracy of 0.784\n",
      "\t\tStep 10: new best accuracy of 0.831\n",
      "\t\tStep 11\n",
      "\t\tStep 12: new best accuracy of 0.864\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16: new best accuracy of 0.871\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20: new best accuracy of 0.882\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24: new best accuracy of 0.884\n",
      "\t\tStep 25: new best accuracy of 0.903\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38: new best accuracy of 0.910\n",
      "\t\tStep 39\n",
      "\t\tStep 40: new best accuracy of 0.914\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44: new best accuracy of 0.919\n",
      "\t\tStep 45: new best accuracy of 0.925\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54: new best accuracy of 0.935\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70: new best accuracy of 0.940\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\tCross-validation fold 4\n",
      "\t\tStep 1: new best accuracy of 0.357\n",
      "\t\tStep 2: new best accuracy of 0.406\n",
      "\t\tStep 3\n",
      "\t\tStep 4: new best accuracy of 0.643\n",
      "\t\tStep 5\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8\n",
      "\t\tStep 9: new best accuracy of 0.657\n",
      "\t\tStep 10: new best accuracy of 0.771\n",
      "\t\tStep 11: new best accuracy of 0.834\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17: new best accuracy of 0.836\n",
      "\t\tStep 18\n",
      "\t\tStep 19: new best accuracy of 0.842\n",
      "\t\tStep 20: new best accuracy of 0.848\n",
      "\t\tStep 21: new best accuracy of 0.874\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29: new best accuracy of 0.876\n",
      "\t\tStep 30: new best accuracy of 0.897\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35: new best accuracy of 0.907\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40: new best accuracy of 0.910\n",
      "\t\tStep 41\n",
      "\t\tStep 42: new best accuracy of 0.924\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60: new best accuracy of 0.935\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67: new best accuracy of 0.939\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75: new best accuracy of 0.940\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99\n",
      "\t\tStep 100: new best accuracy of 0.941\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103: new best accuracy of 0.944\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112: new best accuracy of 0.945\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115: new best accuracy of 0.946\n",
      "\t\tStep 116\n",
      "\t\tStep 117: new best accuracy of 0.946\n",
      "\t\tStep 118\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "\t\tStep 126\n",
      "\t\tStep 127\n",
      "\t\tStep 128\n",
      "\t\tStep 129\n",
      "\t\tStep 130\n",
      "\t\tStep 131\n",
      "\t\tStep 132\n",
      "\t\tStep 133\n",
      "\t\tStep 134\n",
      "\t\tStep 135\n",
      "\t\tStep 136\n",
      "\t\tStep 137\n",
      "\t\tStep 138\n",
      "\t\tStep 139\n",
      "\t\tStep 140\n",
      "\t\tStep 141\n",
      "\t\tStep 142\n",
      "\tCross-validation fold 5\n",
      "\t\tStep 1: new best accuracy of 0.275\n",
      "\t\tStep 2\n",
      "\t\tStep 3\n",
      "\t\tStep 4: new best accuracy of 0.563\n",
      "\t\tStep 5\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8: new best accuracy of 0.766\n",
      "\t\tStep 9\n",
      "\t\tStep 10: new best accuracy of 0.767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tStep 11: new best accuracy of 0.809\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15: new best accuracy of 0.862\n",
      "\t\tStep 16: new best accuracy of 0.880\n",
      "\t\tStep 17\n",
      "\t\tStep 18: new best accuracy of 0.898\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24: new best accuracy of 0.919\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38: new best accuracy of 0.930\n",
      "\t\tStep 39\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-17fd6948c88f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_size_tests\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mbatch_size_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-3de8b39225a8>\u001b[0m in \u001b[0;36mrunLogisticRegression\u001b[0;34m(batch_size, learning_rate, momentum)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mlogisticRegressionModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mlogisticRegressionModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myDigitsTrainingSets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradientDescentModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0myh_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogisticRegressionModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxDigitsTrainingSets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0myh_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogisticRegressionModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxDigitsValidationSets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-bce80aefdb73>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, optimizer)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mw0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-fe1f1eedcc9c>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, gradient_fn, x, y, w)\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                         \u001b[0mw_x\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mx_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                         \u001b[0mden\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0myh_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Analysis of changing batch size\n",
    "\n",
    "batch_size_tests = [30, 25, 20, 15, 10, 5]\n",
    "batch_size_results = []\n",
    "\n",
    "for test in batch_size_tests:\n",
    "    batch_size_results.append(runLogisticRegression(test, learning_rate, momentum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of changing learning rate\n",
    "\n",
    "learning_rate_tests = [0.02, 0.04, 0.06, 0.08]\n",
    "learning_rate_results = []\n",
    "\n",
    "for test in learning_rate_tests:\n",
    "    learning_rate_results.append(runLogisticRegression(batch_size, test, momentum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of changing momentum\n",
    "\n",
    "momentum_tests = [0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "momentum_results = []\n",
    "\n",
    "for test in momentum_tests:\n",
    "    momentum_results.append(runLogisticRegression(batch_size, learning_rate, test))"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
