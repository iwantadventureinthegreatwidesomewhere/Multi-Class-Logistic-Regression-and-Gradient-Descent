{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lia\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import cv2 as cv\n",
    "from sklearn import svm \n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "wine = fetch_openml(name='wine', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of digits and wine data\n",
    "\n",
    "digits_data_norm = []\n",
    "\n",
    "for col in digits.data:\n",
    "    col_norm = col/np.max(col)\n",
    "    digits_data_norm.append(col_norm)\n",
    "\n",
    "digits.data = np.asarray(digits_data_norm)\n",
    "\n",
    "wine_data_norm = []\n",
    "\n",
    "for col in wine.data.T:\n",
    "    col_norm = col/np.amax(col)\n",
    "    wine_data_norm.append(col_norm)\n",
    "    \n",
    "wine.data = np.asarray(wine_data_norm).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation for digits dataset\n",
    "\n",
    "digitsTrainingSetSize = int(np.ceil(0.8 * len(digits.data)))\n",
    "digitsValidationSetSize = int(len(digits.data) - digitsTrainingSetSize)\n",
    "\n",
    "xDigitsTrainingSets = []\n",
    "yDigitsTrainingSets = []\n",
    "xDigitsValidationSets = []\n",
    "yDigitsValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data[foldIndex*digitsValidationSetSize:((foldIndex*digitsValidationSetSize)+digitsValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(digits.target[index+(foldIndex*digitsValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(digits.target[index])\n",
    "            \n",
    "    xDigitsTrainingSets.append(xTrainingSet)\n",
    "    yDigitsTrainingSets.append(yTrainingSet)\n",
    "    xDigitsValidationSets.append(xValidationSet)\n",
    "    yDigitsValidationSets.append(yValidationSet)\n",
    "    \n",
    "# 5-fold cross validation for wine dataset\n",
    "\n",
    "wineTrainingSetSize = int(np.ceil(0.8 * len(wine.data)))\n",
    "wineValidationSetSize = int(len(wine.data) - wineTrainingSetSize)\n",
    "\n",
    "xWineTrainingSets = []\n",
    "yWineTrainingSets = []\n",
    "xWineValidationSets = []\n",
    "yWineValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "    for index, data in enumerate(wine.data[foldIndex*wineValidationSetSize:((foldIndex*wineValidationSetSize)+wineValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(wine.target[index+(foldIndex*wineValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "    \n",
    "    for index, data in enumerate(wine.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(wine.target[index])\n",
    "            \n",
    "    xWineTrainingSets.append(xTrainingSet)\n",
    "    yWineTrainingSets.append(yTrainingSet)\n",
    "    xWineValidationSets.append(xValidationSet)\n",
    "    yWineValidationSets.append(yValidationSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of y for digits dataset\n",
    "\n",
    "numberOfDigitsTargets = 10\n",
    "numberOfWineTargets = 3\n",
    "\n",
    "for index, fold in enumerate(yDigitsTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yDigitsValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsValidationSets[index] = encodedFold\n",
    "\n",
    "# one-hot encoding of y for wine dataset\n",
    "\n",
    "for index, fold in enumerate(yWineTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yWineValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineValidationSets[index] = encodedFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomIndices(arr, batch_size):\n",
    "    indices = []\n",
    "    \n",
    "    if batch_size > len(arr):\n",
    "        print(\"Error: batch size larger than size of dataset.\")\n",
    "        return\n",
    "    \n",
    "    while batch_size > 0:\n",
    "        x = np.floor(np.random.random() * len(arr))\n",
    "        if x not in indices:\n",
    "            indices.append(int(x))\n",
    "            batch_size -= 1\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent class\n",
    " \n",
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, batch_size, learning_rate=0.5, momentum=0.9, max_termination=10, max_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        self.max_termination = max_termination\n",
    "        self.max_iters = max_iters\n",
    "        self.deltas = []\n",
    "        \n",
    "    def run(self, gradient_fn, x, y, w):\n",
    "        t = 1\n",
    "        \n",
    "        min_cost = np.inf\n",
    "        termination_count = 0        \n",
    "        weight_history = []\n",
    "        error_history = []\n",
    "                \n",
    "        for number_of_targets in range(len(y[0])):\n",
    "            weight_history.append([])\n",
    "        \n",
    "        while termination_count < self.max_termination and t < self.max_iters:\n",
    "            gradients = gradient_fn(x, y, w, self.batch_size)   \n",
    "            \n",
    "            for c in range(len(y[0])):\n",
    "                if(t==1):\n",
    "                    w[c] = w[c] - self.learning_rate * gradients[c]\n",
    "                else:\n",
    "                    delta_w = (self.momentum)*(self.deltas[-(len(y[0]))]) + (1-self.momentum)*gradients[c]\n",
    "                    w[c] = w[c] - (self.learning_rate)*(delta_w)\n",
    "                self.deltas.append(w[c])\n",
    "            \n",
    "            a = np.asarray(x)\n",
    "            b = np.asarray(w)\n",
    "    \n",
    "            yh=[]\n",
    "            for i, x_c in enumerate(a):\n",
    "                yh_x=[]\n",
    "\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  b[c] @ x_c\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  b[i] @ x_c\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "                    yh_x.append(yh_c)\n",
    "                    \n",
    "                yh.append(yh_x)\n",
    "                \n",
    "            step_cost = 0\n",
    "                \n",
    "            def cost(yh, y):\n",
    "                return y * np.log1p(np.exp(-yh)) + (1-yh) * np.log1p(np.exp(yh))\n",
    "                \n",
    "            for sample_index, yh_x in enumerate(yh):\n",
    "                c = np.argmax(y[sample_index])\n",
    "                cst = cost(yh_x[c], y[sample_index][c])\n",
    "                step_cost += cst\n",
    "            \n",
    "            for c in range(len(b)):\n",
    "                weight_history[c].append(w[c])\n",
    "\n",
    "            error_history.append(step_cost)\n",
    "            \n",
    "            # We found an alternate termination condition that terminates faster as\n",
    "            # the suggested condition would run for a longtime for us (~1hr).\n",
    "            \n",
    "            # We track the lowest step_cost encountered, and if the \n",
    "            # next max_termination-number of steps do not have a better cost, then\n",
    "            # it terminates.\n",
    "\n",
    "            if step_cost < min_cost:\n",
    "                min_cost = step_cost\n",
    "                termination_count = 0\n",
    "                print(f\"\\t\\tStep {t}: new best cost of {min_cost:.3f}\")\n",
    "            else:\n",
    "                termination_count += 1\n",
    "                print(f\"\\t\\tStep {t}\")\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        # take the weight prior to the last max_termination-number of weights\n",
    "        # (as it is guaranteed to be the best prior to termination).\n",
    "        \n",
    "        index_best = len(error_history)-self.max_termination-1\n",
    "        \n",
    "        w_best = []\n",
    "        \n",
    "        for c in range(len(y[0])):\n",
    "            w_best.append(weight_history[c][index_best])\n",
    "            \n",
    "        #return w_best\n",
    "        \n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, add_bias=True):\n",
    "        self.add_bias = add_bias\n",
    "        pass\n",
    "            \n",
    "    def fit(self, x, y, optimizer):\n",
    "        def gradient(x, y, w, batch_size):\n",
    "            gradients = np.zeros(len(w)).tolist()\n",
    "\n",
    "            indices = getRandomIndices(x, batch_size)\n",
    "\n",
    "            for index in indices:\n",
    "                a = np.asarray(x[index])\n",
    "                b = np.asarray(y[index])\n",
    "\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  w[c] @ a\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  w[i] @ a\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "\n",
    "                    y_c = b[c]\n",
    "                    \n",
    "                    cost_c = np.dot(yh_c - y_c, a)\n",
    "                    \n",
    "                    gradients[c] += cost_c\n",
    "\n",
    "            return gradients\n",
    "        \n",
    "        if self.add_bias:\n",
    "            x = np.asarray(x)\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "\n",
    "        w0 = []\n",
    "        for c in range(len(y[0])):\n",
    "            w0.append(np.zeros(len(x[0])))\n",
    "            \n",
    "        self.w = optimizer.run(gradient, x, y, w0)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if self.add_bias:\n",
    "            x = np.asarray(x)\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "\n",
    "        a = np.asarray(x)\n",
    "        b = np.asarray(self.w)\n",
    "\n",
    "        yh=[]\n",
    "        \n",
    "        for i, x_c in enumerate(a):\n",
    "            yh_x=[]\n",
    "            \n",
    "            for c in range(len(b)):\n",
    "                w_x =  b[c] @ x_c\n",
    "                num = np.exp(w_x)\n",
    "\n",
    "                den = 0\n",
    "                for i in range(len(b)):\n",
    "                    w_x =  b[i] @ x_c\n",
    "                    den += np.exp(w_x)\n",
    "\n",
    "                yh_c = num/den\n",
    "                yh_x.append(yh_c)\n",
    "                \n",
    "            yh.append(yh_x)\n",
    "        \n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hyper-parameters:\n",
      "\tMini-batch size: 30\n",
      "\tLearning rate: 0.04\n",
      "\tMomentum: 0.2\n",
      "\n",
      "\n",
      "Digits gradient descent:\n",
      "\tCross-validation fold 1\n",
      "\t\tStep 1: new best cost of 1838.674\n",
      "\t\tStep 2\n",
      "\t\tStep 3: new best cost of 1803.062\n",
      "\t\tStep 4: new best cost of 1780.246\n",
      "\t\tStep 5: new best cost of 1755.715\n",
      "\t\tStep 6: new best cost of 1706.679\n",
      "\t\tStep 7: new best cost of 1700.851\n",
      "\t\tStep 8: new best cost of 1695.124\n",
      "\t\tStep 9: new best cost of 1634.909\n",
      "\t\tStep 10: new best cost of 1588.966\n",
      "\t\tStep 11\n",
      "\t\tStep 12: new best cost of 1552.115\n",
      "\t\tStep 13: new best cost of 1547.302\n",
      "\t\tStep 14: new best cost of 1504.241\n",
      "\t\tStep 15: new best cost of 1480.087\n",
      "\t\tStep 16: new best cost of 1434.071\n",
      "\t\tStep 17: new best cost of 1422.272\n",
      "\t\tStep 18: new best cost of 1416.952\n",
      "\t\tStep 19: new best cost of 1407.404\n",
      "\t\tStep 20\n",
      "\t\tStep 21: new best cost of 1365.877\n",
      "\t\tStep 22: new best cost of 1357.680\n",
      "\t\tStep 23\n",
      "\t\tStep 24: new best cost of 1332.265\n",
      "\t\tStep 25: new best cost of 1307.595\n",
      "\t\tStep 26: new best cost of 1297.617\n",
      "\t\tStep 27\n",
      "\t\tStep 28: new best cost of 1268.571\n",
      "\t\tStep 29: new best cost of 1253.909\n",
      "\t\tStep 30\n",
      "\t\tStep 31: new best cost of 1240.983\n",
      "\t\tStep 32\n",
      "\t\tStep 33: new best cost of 1238.953\n",
      "\t\tStep 34: new best cost of 1217.190\n",
      "\t\tStep 35: new best cost of 1210.594\n",
      "\t\tStep 36\n",
      "\t\tStep 37: new best cost of 1199.228\n",
      "\t\tStep 38: new best cost of 1186.470\n",
      "\t\tStep 39: new best cost of 1173.599\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43: new best cost of 1159.630\n",
      "\t\tStep 44: new best cost of 1151.927\n",
      "\t\tStep 45: new best cost of 1137.006\n",
      "\t\tStep 46: new best cost of 1129.804\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52: new best cost of 1123.625\n",
      "\t\tStep 53: new best cost of 1106.495\n",
      "\t\tStep 54\n",
      "\t\tStep 55: new best cost of 1103.096\n",
      "\t\tStep 56: new best cost of 1100.337\n",
      "\t\tStep 57: new best cost of 1090.114\n",
      "\t\tStep 58\n",
      "\t\tStep 59: new best cost of 1084.543\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63: new best cost of 1076.690\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69: new best cost of 1067.770\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75: new best cost of 1065.611\n",
      "\t\tStep 76\n",
      "\t\tStep 77: new best cost of 1058.818\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82: new best cost of 1056.480\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87: new best cost of 1041.128\n",
      "\t\tStep 88: new best cost of 1037.462\n",
      "\t\tStep 89: new best cost of 1036.762\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96: new best cost of 1030.721\n",
      "\t\tStep 97: new best cost of 1028.183\n",
      "\t\tStep 98\n",
      "\t\tStep 99: new best cost of 1027.944\n",
      "\t\tStep 100\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\tCross-validation fold 2\n",
      "\t\tStep 1: new best cost of 1839.855\n",
      "\t\tStep 2: new best cost of 1826.789\n",
      "\t\tStep 3: new best cost of 1801.027\n",
      "\t\tStep 4: new best cost of 1768.199\n",
      "\t\tStep 5: new best cost of 1757.648\n",
      "\t\tStep 6: new best cost of 1712.086\n",
      "\t\tStep 7: new best cost of 1671.703\n",
      "\t\tStep 8: new best cost of 1618.653\n",
      "\t\tStep 9: new best cost of 1598.842\n",
      "\t\tStep 10\n",
      "\t\tStep 11: new best cost of 1595.484\n",
      "\t\tStep 12: new best cost of 1514.988\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15: new best cost of 1482.296\n",
      "\t\tStep 16: new best cost of 1443.757\n",
      "\t\tStep 17\n",
      "\t\tStep 18: new best cost of 1384.033\n",
      "\t\tStep 19: new best cost of 1358.111\n",
      "\t\tStep 20: new best cost of 1350.196\n",
      "\t\tStep 21: new best cost of 1342.456\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24: new best cost of 1276.590\n",
      "\t\tStep 25\n",
      "\t\tStep 26: new best cost of 1264.211\n",
      "\t\tStep 27: new best cost of 1247.131\n",
      "\t\tStep 28: new best cost of 1237.516\n",
      "\t\tStep 29: new best cost of 1235.846\n",
      "\t\tStep 30: new best cost of 1220.251\n",
      "\t\tStep 31\n",
      "\t\tStep 32: new best cost of 1200.241\n",
      "\t\tStep 33\n",
      "\t\tStep 34: new best cost of 1189.583\n",
      "\t\tStep 35: new best cost of 1185.203\n",
      "\t\tStep 36: new best cost of 1160.791\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39: new best cost of 1144.876\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43: new best cost of 1125.928\n",
      "\t\tStep 44\n",
      "\t\tStep 45: new best cost of 1118.533\n",
      "\t\tStep 46: new best cost of 1115.862\n",
      "\t\tStep 47\n",
      "\t\tStep 48: new best cost of 1112.917\n",
      "\t\tStep 49\n",
      "\t\tStep 50: new best cost of 1091.931\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56: new best cost of 1091.329\n",
      "\t\tStep 57: new best cost of 1072.776\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62: new best cost of 1053.107\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68: new best cost of 1051.191\n",
      "\t\tStep 69\n",
      "\t\tStep 70: new best cost of 1046.651\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74: new best cost of 1045.887\n",
      "\t\tStep 75\n",
      "\t\tStep 76: new best cost of 1044.352\n",
      "\t\tStep 77: new best cost of 1042.126\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80: new best cost of 1039.051\n",
      "\t\tStep 81: new best cost of 1035.865\n",
      "\t\tStep 82\n",
      "\t\tStep 83: new best cost of 1032.163\n",
      "\t\tStep 84: new best cost of 1021.670\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\tCross-validation fold 3\n",
      "\t\tStep 1: new best cost of 1845.326\n",
      "\t\tStep 2: new best cost of 1830.286\n",
      "\t\tStep 3: new best cost of 1800.309\n",
      "\t\tStep 4: new best cost of 1773.938\n",
      "\t\tStep 5: new best cost of 1740.840\n",
      "\t\tStep 6: new best cost of 1704.843\n",
      "\t\tStep 7: new best cost of 1683.705\n",
      "\t\tStep 8: new best cost of 1667.500\n",
      "\t\tStep 9: new best cost of 1623.152\n",
      "\t\tStep 10: new best cost of 1605.883\n",
      "\t\tStep 11: new best cost of 1576.360\n",
      "\t\tStep 12: new best cost of 1519.726\n",
      "\t\tStep 13: new best cost of 1500.696\n",
      "\t\tStep 14: new best cost of 1476.056\n",
      "\t\tStep 15: new best cost of 1460.444\n",
      "\t\tStep 16: new best cost of 1435.473\n",
      "\t\tStep 17\n",
      "\t\tStep 18: new best cost of 1405.014\n",
      "\t\tStep 19: new best cost of 1376.188\n",
      "\t\tStep 20\n",
      "\t\tStep 21: new best cost of 1353.740\n",
      "\t\tStep 22: new best cost of 1345.108\n",
      "\t\tStep 23: new best cost of 1327.359\n",
      "\t\tStep 24: new best cost of 1309.259\n",
      "\t\tStep 25: new best cost of 1303.742\n",
      "\t\tStep 26: new best cost of 1286.620\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29: new best cost of 1261.470\n",
      "\t\tStep 30\n",
      "\t\tStep 31: new best cost of 1223.988\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34: new best cost of 1207.834\n",
      "\t\tStep 35\n",
      "\t\tStep 36: new best cost of 1180.487\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39: new best cost of 1175.947\n",
      "\t\tStep 40: new best cost of 1173.703\n",
      "\t\tStep 41\n",
      "\t\tStep 42: new best cost of 1163.038\n",
      "\t\tStep 43\n",
      "\t\tStep 44: new best cost of 1148.383\n",
      "\t\tStep 45\n",
      "\t\tStep 46: new best cost of 1139.524\n",
      "\t\tStep 47\n",
      "\t\tStep 48: new best cost of 1133.609\n",
      "\t\tStep 49\n",
      "\t\tStep 50: new best cost of 1119.120\n",
      "\t\tStep 51: new best cost of 1114.359\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55: new best cost of 1099.913\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58: new best cost of 1081.590\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63: new best cost of 1077.065\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66: new best cost of 1075.415\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69: new best cost of 1070.319\n",
      "\t\tStep 70: new best cost of 1062.839\n",
      "\t\tStep 71: new best cost of 1052.652\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76: new best cost of 1051.642\n",
      "\t\tStep 77: new best cost of 1044.328\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85: new best cost of 1044.190\n",
      "\t\tStep 86: new best cost of 1043.640\n",
      "\t\tStep 87\n",
      "\t\tStep 88: new best cost of 1035.109\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94: new best cost of 1024.479\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98: new best cost of 1021.110\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104: new best cost of 1020.396\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108: new best cost of 1012.047\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\t\tStep 118: new best cost of 1006.014\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "\t\tStep 126\n",
      "\t\tStep 127\n",
      "\t\tStep 128\n",
      "\tCross-validation fold 4\n",
      "\t\tStep 1: new best cost of 1851.208\n",
      "\t\tStep 2: new best cost of 1823.562\n",
      "\t\tStep 3: new best cost of 1818.409\n",
      "\t\tStep 4: new best cost of 1780.109\n",
      "\t\tStep 5: new best cost of 1735.302\n",
      "\t\tStep 6: new best cost of 1714.799\n",
      "\t\tStep 7: new best cost of 1697.207\n",
      "\t\tStep 8: new best cost of 1646.356\n",
      "\t\tStep 9: new best cost of 1628.714\n",
      "\t\tStep 10: new best cost of 1603.559\n",
      "\t\tStep 11: new best cost of 1565.799\n",
      "\t\tStep 12: new best cost of 1536.796\n",
      "\t\tStep 13: new best cost of 1530.187\n",
      "\t\tStep 14: new best cost of 1510.208\n",
      "\t\tStep 15: new best cost of 1481.004\n",
      "\t\tStep 16: new best cost of 1462.396\n",
      "\t\tStep 17: new best cost of 1438.682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tStep 18: new best cost of 1412.377\n",
      "\t\tStep 19\n",
      "\t\tStep 20: new best cost of 1379.277\n",
      "\t\tStep 21\n",
      "\t\tStep 22: new best cost of 1376.610\n",
      "\t\tStep 23: new best cost of 1342.424\n",
      "\t\tStep 24\n",
      "\t\tStep 25: new best cost of 1309.832\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28: new best cost of 1289.235\n",
      "\t\tStep 29: new best cost of 1266.592\n",
      "\t\tStep 30: new best cost of 1264.644\n",
      "\t\tStep 31: new best cost of 1249.703\n",
      "\t\tStep 32: new best cost of 1236.879\n",
      "\t\tStep 33\n",
      "\t\tStep 34: new best cost of 1233.030\n",
      "\t\tStep 35\n",
      "\t\tStep 36: new best cost of 1226.815\n",
      "\t\tStep 37\n",
      "\t\tStep 38: new best cost of 1204.525\n",
      "\t\tStep 39: new best cost of 1197.577\n",
      "\t\tStep 40: new best cost of 1174.815\n",
      "\t\tStep 41: new best cost of 1173.543\n",
      "\t\tStep 42\n",
      "\t\tStep 43: new best cost of 1161.227\n",
      "\t\tStep 44\n",
      "\t\tStep 45: new best cost of 1151.482\n",
      "\t\tStep 46\n",
      "\t\tStep 47: new best cost of 1149.104\n",
      "\t\tStep 48: new best cost of 1137.321\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54: new best cost of 1137.159\n",
      "\t\tStep 55\n",
      "\t\tStep 56: new best cost of 1131.053\n",
      "\t\tStep 57: new best cost of 1118.582\n",
      "\t\tStep 58\n",
      "\t\tStep 59: new best cost of 1110.417\n",
      "\t\tStep 60\n",
      "\t\tStep 61: new best cost of 1097.719\n",
      "\t\tStep 62\n",
      "\t\tStep 63: new best cost of 1088.738\n",
      "\t\tStep 64: new best cost of 1088.034\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67: new best cost of 1080.119\n",
      "\t\tStep 68\n",
      "\t\tStep 69: new best cost of 1071.241\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73: new best cost of 1066.282\n",
      "\t\tStep 74: new best cost of 1064.928\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77: new best cost of 1059.392\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81: new best cost of 1059.118\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87: new best cost of 1055.826\n",
      "\t\tStep 88: new best cost of 1049.136\n",
      "\t\tStep 89: new best cost of 1043.692\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92: new best cost of 1042.545\n",
      "\t\tStep 93: new best cost of 1039.648\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96: new best cost of 1038.059\n",
      "\t\tStep 97: new best cost of 1037.296\n",
      "\t\tStep 98: new best cost of 1036.255\n",
      "\t\tStep 99\n",
      "\t\tStep 100: new best cost of 1033.983\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104: new best cost of 1030.847\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107: new best cost of 1026.490\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\tCross-validation fold 5\n",
      "\t\tStep 1: new best cost of 1842.593\n",
      "\t\tStep 2: new best cost of 1813.387\n",
      "\t\tStep 3: new best cost of 1789.340\n",
      "\t\tStep 4: new best cost of 1760.203\n",
      "\t\tStep 5: new best cost of 1754.877\n",
      "\t\tStep 6: new best cost of 1712.110\n",
      "\t\tStep 7: new best cost of 1694.400\n",
      "\t\tStep 8: new best cost of 1647.184\n",
      "\t\tStep 9: new best cost of 1606.631\n",
      "\t\tStep 10: new best cost of 1591.512\n",
      "\t\tStep 11: new best cost of 1562.849\n",
      "\t\tStep 12: new best cost of 1520.802\n",
      "\t\tStep 13: new best cost of 1516.519\n",
      "\t\tStep 14: new best cost of 1510.635\n",
      "\t\tStep 15: new best cost of 1468.887\n",
      "\t\tStep 16: new best cost of 1450.128\n",
      "\t\tStep 17: new best cost of 1448.321\n",
      "\t\tStep 18: new best cost of 1415.061\n",
      "\t\tStep 19\n",
      "\t\tStep 20: new best cost of 1407.058\n",
      "\t\tStep 21: new best cost of 1346.584\n",
      "\t\tStep 22: new best cost of 1331.518\n",
      "\t\tStep 23: new best cost of 1315.818\n",
      "\t\tStep 24\n",
      "\t\tStep 25: new best cost of 1292.708\n",
      "\t\tStep 26: new best cost of 1272.066\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29: new best cost of 1242.333\n",
      "\t\tStep 30: new best cost of 1237.190\n",
      "\t\tStep 31: new best cost of 1229.090\n",
      "\t\tStep 32\n",
      "\t\tStep 33: new best cost of 1217.600\n",
      "\t\tStep 34: new best cost of 1215.399\n",
      "\t\tStep 35: new best cost of 1194.959\n",
      "\t\tStep 36: new best cost of 1193.571\n",
      "\t\tStep 37: new best cost of 1174.175\n",
      "\t\tStep 38\n",
      "\t\tStep 39: new best cost of 1165.349\n",
      "\t\tStep 40: new best cost of 1156.739\n",
      "\t\tStep 41: new best cost of 1152.865\n",
      "\t\tStep 42\n",
      "\t\tStep 43: new best cost of 1148.588\n",
      "\t\tStep 44\n",
      "\t\tStep 45: new best cost of 1134.274\n",
      "\t\tStep 46\n",
      "\t\tStep 47: new best cost of 1125.110\n",
      "\t\tStep 48: new best cost of 1122.535\n",
      "\t\tStep 49: new best cost of 1119.115\n",
      "\t\tStep 50\n",
      "\t\tStep 51: new best cost of 1093.467\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54: new best cost of 1088.069\n",
      "\t\tStep 55: new best cost of 1083.409\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58: new best cost of 1079.325\n",
      "\t\tStep 59: new best cost of 1070.969\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64: new best cost of 1065.267\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67: new best cost of 1058.535\n",
      "\t\tStep 68\n",
      "\t\tStep 69: new best cost of 1050.273\n",
      "\t\tStep 70: new best cost of 1048.974\n",
      "\t\tStep 71: new best cost of 1043.175\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78: new best cost of 1041.034\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81: new best cost of 1039.847\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85: new best cost of 1025.573\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88: new best cost of 1018.053\n",
      "\t\tStep 89: new best cost of 1018.034\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96: new best cost of 1017.777\n",
      "\t\tStep 97\n",
      "\t\tStep 98: new best cost of 1015.524\n",
      "\t\tStep 99\n",
      "\t\tStep 100: new best cost of 1010.346\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105: new best cost of 1007.903\n",
      "\t\tStep 106: new best cost of 1005.185\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111: new best cost of 1004.245\n",
      "\t\tStep 112: new best cost of 1003.627\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115: new best cost of 1000.164\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\t\tStep 118\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "Wine gradient descent:\n",
      "\tCross-validation fold 1\n",
      "\t\tStep 1: new best cost of 149.959\n",
      "\t\tStep 2: new best cost of 133.247\n",
      "\t\tStep 3\n",
      "\t\tStep 4\n",
      "\t\tStep 5\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8\n",
      "\t\tStep 9\n",
      "\t\tStep 10: new best cost of 127.052\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16: new best cost of 124.288\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20: new best cost of 121.902\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23: new best cost of 117.180\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29: new best cost of 113.948\n",
      "\t\tStep 30\n",
      "\t\tStep 31: new best cost of 113.297\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36: new best cost of 110.588\n",
      "\t\tStep 37\n",
      "\t\tStep 38: new best cost of 109.492\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44: new best cost of 108.191\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50: new best cost of 105.171\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\tCross-validation fold 2\n",
      "\t\tStep 1: new best cost of 154.137\n",
      "\t\tStep 2\n",
      "\t\tStep 3: new best cost of 146.611\n",
      "\t\tStep 4\n",
      "\t\tStep 5: new best cost of 146.134\n",
      "\t\tStep 6\n",
      "\t\tStep 7: new best cost of 145.262\n",
      "\t\tStep 8\n",
      "\t\tStep 9: new best cost of 144.185\n",
      "\t\tStep 10: new best cost of 140.717\n",
      "\t\tStep 11: new best cost of 139.593\n",
      "\t\tStep 12: new best cost of 138.058\n",
      "\t\tStep 13: new best cost of 137.985\n",
      "\t\tStep 14: new best cost of 136.822\n",
      "\t\tStep 15: new best cost of 134.047\n",
      "\t\tStep 16\n",
      "\t\tStep 17: new best cost of 130.343\n",
      "\t\tStep 18: new best cost of 129.531\n",
      "\t\tStep 19\n",
      "\t\tStep 20: new best cost of 127.792\n",
      "\t\tStep 21\n",
      "\t\tStep 22: new best cost of 126.857\n",
      "\t\tStep 23: new best cost of 125.532\n",
      "\t\tStep 24\n",
      "\t\tStep 25: new best cost of 122.650\n",
      "\t\tStep 26\n",
      "\t\tStep 27: new best cost of 122.290\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30: new best cost of 119.685\n",
      "\t\tStep 31\n",
      "\t\tStep 32: new best cost of 118.712\n",
      "\t\tStep 33\n",
      "\t\tStep 34: new best cost of 117.801\n",
      "\t\tStep 35\n",
      "\t\tStep 36: new best cost of 114.722\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39: new best cost of 114.429\n",
      "\t\tStep 40\n",
      "\t\tStep 41: new best cost of 113.677\n",
      "\t\tStep 42: new best cost of 112.951\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45: new best cost of 112.256\n",
      "\t\tStep 46: new best cost of 111.784\n",
      "\t\tStep 47: new best cost of 110.313\n",
      "\t\tStep 48\n",
      "\t\tStep 49: new best cost of 109.433\n",
      "\t\tStep 50: new best cost of 108.892\n",
      "\t\tStep 51\n",
      "\t\tStep 52: new best cost of 108.091\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56: new best cost of 106.977\n",
      "\t\tStep 57\n",
      "\t\tStep 58: new best cost of 106.864\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61: new best cost of 105.563\n",
      "\t\tStep 62: new best cost of 105.129\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67: new best cost of 104.322\n",
      "\t\tStep 68: new best cost of 103.975\n",
      "\t\tStep 69\n",
      "\t\tStep 70: new best cost of 103.815\n",
      "\t\tStep 71\n",
      "\t\tStep 72: new best cost of 103.446\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75: new best cost of 103.238\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78: new best cost of 102.709\n",
      "\t\tStep 79: new best cost of 102.634\n",
      "\t\tStep 80: new best cost of 101.802\n",
      "\t\tStep 81\n",
      "\t\tStep 82: new best cost of 101.698\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87: new best cost of 101.138\n",
      "\t\tStep 88: new best cost of 100.477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94: new best cost of 100.345\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98: new best cost of 100.030\n",
      "\t\tStep 99\n",
      "\t\tStep 100: new best cost of 99.502\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105: new best cost of 99.014\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108: new best cost of 99.009\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115: new best cost of 98.358\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\t\tStep 118\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121: new best cost of 97.876\n",
      "\t\tStep 122\n",
      "\t\tStep 123\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "\t\tStep 126: new best cost of 97.688\n",
      "\t\tStep 127\n",
      "\t\tStep 128\n",
      "\t\tStep 129\n",
      "\t\tStep 130\n",
      "\t\tStep 131\n",
      "\t\tStep 132\n",
      "\t\tStep 133: new best cost of 97.212\n",
      "\t\tStep 134\n",
      "\t\tStep 135\n",
      "\t\tStep 136\n",
      "\t\tStep 137\n",
      "\t\tStep 138\n",
      "\t\tStep 139\n",
      "\t\tStep 140: new best cost of 97.204\n",
      "\t\tStep 141: new best cost of 96.999\n",
      "\t\tStep 142\n",
      "\t\tStep 143\n",
      "\t\tStep 144\n",
      "\t\tStep 145: new best cost of 96.713\n",
      "\t\tStep 146: new best cost of 96.660\n",
      "\t\tStep 147\n",
      "\t\tStep 148\n",
      "\t\tStep 149\n",
      "\t\tStep 150\n",
      "\t\tStep 151\n",
      "\t\tStep 152: new best cost of 96.480\n",
      "\t\tStep 153\n",
      "\t\tStep 154\n",
      "\t\tStep 155\n",
      "\t\tStep 156\n",
      "\t\tStep 157\n",
      "\t\tStep 158\n",
      "\t\tStep 159\n",
      "\t\tStep 160\n",
      "\t\tStep 161\n",
      "\t\tStep 162\n",
      "\tCross-validation fold 3\n",
      "\t\tStep 1: new best cost of 150.959\n",
      "\t\tStep 2\n",
      "\t\tStep 3: new best cost of 144.885\n",
      "\t\tStep 4\n",
      "\t\tStep 5: new best cost of 141.174\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8\n",
      "\t\tStep 9: new best cost of 129.149\n",
      "\t\tStep 10\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17: new best cost of 128.954\n",
      "\t\tStep 18\n",
      "\t\tStep 19: new best cost of 121.896\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22: new best cost of 119.341\n",
      "\t\tStep 23\n",
      "\t\tStep 24: new best cost of 116.746\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28: new best cost of 114.202\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33: new best cost of 111.813\n",
      "\t\tStep 34\n",
      "\t\tStep 35: new best cost of 111.020\n",
      "\t\tStep 36\n",
      "\t\tStep 37: new best cost of 110.634\n",
      "\t\tStep 38\n",
      "\t\tStep 39: new best cost of 108.681\n",
      "\t\tStep 40\n",
      "\t\tStep 41: new best cost of 105.791\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46: new best cost of 105.705\n",
      "\t\tStep 47\n",
      "\t\tStep 48: new best cost of 104.121\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52: new best cost of 103.492\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55: new best cost of 101.632\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59: new best cost of 100.401\n",
      "\t\tStep 60\n",
      "\t\tStep 61: new best cost of 99.841\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\tCross-validation fold 4\n",
      "\t\tStep 1: new best cost of 149.944\n",
      "\t\tStep 2\n",
      "\t\tStep 3: new best cost of 141.319\n",
      "\t\tStep 4\n",
      "\t\tStep 5: new best cost of 139.062\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8\n",
      "\t\tStep 9: new best cost of 134.106\n",
      "\t\tStep 10\n",
      "\t\tStep 11: new best cost of 133.073\n",
      "\t\tStep 12\n",
      "\t\tStep 13: new best cost of 127.971\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21: new best cost of 127.094\n",
      "\t\tStep 22: new best cost of 126.063\n",
      "\t\tStep 23: new best cost of 125.691\n",
      "\t\tStep 24\n",
      "\t\tStep 25: new best cost of 119.014\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30: new best cost of 116.893\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33: new best cost of 115.848\n",
      "\t\tStep 34\n",
      "\t\tStep 35: new best cost of 115.766\n",
      "\t\tStep 36\n",
      "\t\tStep 37: new best cost of 114.458\n",
      "\t\tStep 38\n",
      "\t\tStep 39: new best cost of 113.020\n",
      "\t\tStep 40\n",
      "\t\tStep 41: new best cost of 110.601\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49: new best cost of 110.511\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52: new best cost of 107.786\n",
      "\t\tStep 53\n",
      "\t\tStep 54: new best cost of 107.387\n",
      "\t\tStep 55\n"
     ]
    }
   ],
   "source": [
    "def accurate(a, b):\n",
    "    return np.argmax(a) == np.argmax(b)\n",
    "\n",
    "def cost(yh, y):\n",
    "    return y * np.log1p(np.exp(-yh)) + (1-yh) * np.log1p(np.exp(yh))\n",
    "\n",
    "# TODO: grid-search to find lowest cost combination of model hyper-parameters\n",
    "\n",
    "batch_size = 30\n",
    "learning_rate = 0.04\n",
    "momentum = 0.2\n",
    "\n",
    "print(\"Model hyper-parameters:\")\n",
    "print(\"\\tMini-batch size:\", batch_size)\n",
    "print(\"\\tLearning rate:\", learning_rate)\n",
    "print(\"\\tMomentum:\", momentum)\n",
    "print(\"\\n\")\n",
    "\n",
    "digits_training_accuracy = 0\n",
    "digits_training_cost = 0\n",
    "digits_validation_accuracy = 0\n",
    "digits_validation_cost = 0\n",
    "\n",
    "print(\"Digits gradient descent:\")\n",
    "\n",
    "for fold_index, fold in enumerate(xDigitsTrainingSets):\n",
    "    print(f\"\\tCross-validation fold {fold_index+1}\")\n",
    "    \n",
    "    gradientDescentModel = GradientDescent(batch_size, learning_rate, momentum)\n",
    "    logisticRegressionModel = LogisticRegression(add_bias=True)\n",
    "    \n",
    "    logisticRegressionModel.fit(fold, yDigitsTrainingSets[fold_index], gradientDescentModel)\n",
    "    yh_training = logisticRegressionModel.predict(xDigitsTrainingSets[fold_index])\n",
    "    yh_validation = logisticRegressionModel.predict(xDigitsValidationSets[fold_index])\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh_training):\n",
    "        if accurate(yh_x, yDigitsTrainingSets[fold_index][sample_index]):\n",
    "            digits_training_accuracy += 1\n",
    "        c = np.argmax(yDigitsTrainingSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yDigitsTrainingSets[fold_index][sample_index][c])\n",
    "        digits_training_cost += cst\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh_validation):\n",
    "        if accurate(yh_x, yDigitsValidationSets[fold_index][sample_index]):\n",
    "            digits_validation_accuracy += 1\n",
    "        c = np.argmax(yDigitsValidationSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yDigitsValidationSets[fold_index][sample_index][c])\n",
    "        digits_validation_cost += cst\n",
    "        \n",
    "digits_training_accuracy /= 4*len(digits.data)\n",
    "digits_training_cost /= 4\n",
    "digits_validation_accuracy /= len(digits.data)\n",
    "\n",
    "wine_training_accuracy = 0\n",
    "wine_training_cost = 0\n",
    "wine_validation_accuracy = 0\n",
    "wine_validation_cost = 0\n",
    "\n",
    "print(\"Wine gradient descent:\")\n",
    "\n",
    "for fold_index, fold in enumerate(xWineTrainingSets):\n",
    "    print(f\"\\tCross-validation fold {fold_index+1}\")\n",
    "    \n",
    "    gradientDescentModel = GradientDescent(batch_size, learning_rate, momentum)\n",
    "    logisticRegressionModel = LogisticRegression(add_bias=True)\n",
    "    \n",
    "    logisticRegressionModel.fit(fold, yWineTrainingSets[fold_index], gradientDescentModel)\n",
    "    yh_training = logisticRegressionModel.predict(xWineTrainingSets[fold_index])\n",
    "    yh_validation = logisticRegressionModel.predict(xWineValidationSets[fold_index])\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh_training):\n",
    "        if accurate(yh_x, yWineTrainingSets[fold_index][sample_index]):\n",
    "            wine_training_accuracy += 1\n",
    "        c = np.argmax(yWineTrainingSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yWineTrainingSets[fold_index][sample_index][c])\n",
    "        wine_training_cost += cst\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh_validation):\n",
    "        if accurate(yh_x, yWineValidationSets[fold_index][sample_index]):\n",
    "            wine_validation_accuracy += 1\n",
    "        c = np.argmax(yWineValidationSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yWineValidationSets[fold_index][sample_index][c])\n",
    "        wine_validation_cost += cst\n",
    "\n",
    "wine_training_accuracy /= 4*len(wine.data)\n",
    "wine_training_cost /= 4\n",
    "wine_validation_accuracy /= len(wine.data)\n",
    "    \n",
    "print(\"\\n\")\n",
    "print(f\"Digits training accuracy: {digits_training_accuracy*100:.1f}%\")\n",
    "print(f\"Digits training cost: {digits_training_cost:.3f}\")\n",
    "print(f\"Digits validation accuracy: {digits_validation_accuracy*100:.1f}%\")\n",
    "print(f\"Digits validation cost: {digits_validation_cost:.3f}\")\n",
    "print(f\"Wine training accuracy: {wine_training_accuracy*100:.1f}%\")\n",
    "print(f\"Wine training cost: {wine_training_cost:.3f}\")\n",
    "print(f\"Wine validation accuracy: {wine_validation_accuracy*100:.1f}%\")\n",
    "print(f\"Wine validation cost: {wine_validation_cost:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean = lambda x1, x2: np.sqrt(np.sum((x1 - x2)**2, axis=-1))\n",
    "manhattan = lambda x1, x2: np.sum(np.abs(x1 - x2), axis=-1)\n",
    "\n",
    "class KNN:\n",
    "\n",
    "    def __init__(self, K=1, dist_fn= euclidean):\n",
    "        self.dist_fn = dist_fn\n",
    "        self.K = K\n",
    "        return\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.C = len(y[0])\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        num_test = x_test.shape[0]\n",
    "        distances = self.dist_fn(self.x[None,:,:], x_test[:,None,:])\n",
    "        knns = np.zeros((num_test, self.K), dtype=int)\n",
    "        y_prob = np.zeros((num_test),dtype=int)\n",
    "        counts = np.zeros((num_test, self.C))\n",
    "        \n",
    "        for i in range(num_test):\n",
    "            knns[i,:] = np.argsort(distances[i])[:self.K]\n",
    "            k_count=np.zeros(self.K, dtype=int)\n",
    "            \n",
    "            for s, arr in enumerate(self.y[knns[i,:]]):\n",
    "                k_count[s] = np.argmax(arr)\n",
    "            \n",
    "            y_prob_i, counts_i = np.unique(k_count, return_counts=True)\n",
    "            y_prob[i] = int(y_prob_i[np.argmax(counts_i)])\n",
    "        \n",
    "        return y_prob, knns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNmodel = KNN(K=11)\n",
    "\n",
    "digits_knn_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    y_prob, knns = KNNmodel.fit(np.asarray(xDigitsTrainingSets[fold]), np.asarray(yDigitsTrainingSets[fold])).predict(np.asarray(xDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, prob in enumerate(y_prob):\n",
    "        if prob == np.argmax(yDigitsValidationSets[fold][i]):\n",
    "            digits_knn_accuracy += 1\n",
    "\n",
    "digits_knn_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"KNN digits validation accuracy: {digits_knn_accuracy*100:.1f}%\")\n",
    "\n",
    "KNNmodel = KNN(K=7)\n",
    "\n",
    "wine_knn_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    y_prob, knns = KNNmodel.fit(np.asarray(xWineTrainingSets[fold]), np.asarray(yWineTrainingSets[fold])).predict(np.asarray(xWineValidationSets[fold]))\n",
    "    \n",
    "    for i, prob in enumerate(y_prob):\n",
    "        if prob == np.argmax(yWineValidationSets[fold][i]):\n",
    "            wine_knn_accuracy += 1\n",
    "            \n",
    "wine_knn_accuracy /= len(wine.data)\n",
    "\n",
    "print(f\"KNN wine validation accuracy: {wine_knn_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HoGFeatures(img, cellSize, blockSize, nbins):\n",
    "    cell_size = (cellSize, cellSize)\n",
    "    block_size = (blockSize, blockSize)\n",
    "    \n",
    "    hog = cv.HOGDescriptor(_winSize=(img.shape[1] // cell_size[1] * cell_size[1],\n",
    "                                     img.shape[0] // cell_size[0] * cell_size[0]),\n",
    "                           _blockSize=(block_size[1] * cell_size[1],\n",
    "                                       block_size[0] * cell_size[0]),\n",
    "                           _blockStride=(cell_size[1], cell_size[0]),\n",
    "                           _cellSize=(cell_size[1], cell_size[0]),\n",
    "                           _nbins=nbins\n",
    "    )\n",
    "    \n",
    "    return hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeHoGFeatures(imageArray):\n",
    "    HoG = HoGFeatures(imageArray[0], 2, 2, 2)\n",
    "    features = []\n",
    "    \n",
    "    for i, image in enumerate(imageArray):\n",
    "        features.append(HoG.compute((image*255).astype(np.uint8)))\n",
    "        \n",
    "    features = np.array(np.squeeze(features))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_svc_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    numbers_training = []\n",
    "    numbers_validation = []\n",
    "    \n",
    "    for i, number in enumerate(xDigitsTrainingSets[fold]):\n",
    "        numbers_training.append(np.asarray(number).reshape(8, 8))\n",
    "        \n",
    "    for i, number in enumerate(xDigitsValidationSets[fold]):\n",
    "        numbers_validation.append(np.asarray(number).reshape(8, 8))  \n",
    "        \n",
    "    HoGs_training = makeHoGFeatures(np.asarray(numbers_training))\n",
    "    HoGs_validation = makeHoGFeatures(np.asarray(numbers_validation))\n",
    "\n",
    "    clf = svm.SVC(gamma='auto', C=100) \n",
    "    \n",
    "    labels_training = np.zeros(len(yDigitsTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    clf.fit(HoGs_training, labels_training)\n",
    "\n",
    "    labels_predicted = clf.predict(HoGs_validation)\n",
    "    \n",
    "    for i, label in enumerate(labels_predicted):\n",
    "        if label == labels_validation[i]:\n",
    "            digits_svc_accuracy += 1\n",
    "\n",
    "digits_svc_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"SVC digits validation accuracy: {digits_svc_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_naive_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    labels_training = np.zeros(len(yDigitsTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(np.asarray(xDigitsTrainingSets[fold]), labels_training).predict(np.asarray(xDigitsValidationSets[fold]))\n",
    "\n",
    "    for i, label in enumerate(y_pred):\n",
    "        if label == labels_validation[i]:\n",
    "            digits_naive_accuracy += 1\n",
    "\n",
    "digits_naive_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"Naive base digits validation accuracy: {digits_naive_accuracy*100:.1f}%\\n\")\n",
    "\n",
    "wine_naive_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    labels_training = np.zeros(len(yWineTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yWineTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yWineValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yWineValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(np.asarray(xWineTrainingSets[fold]), labels_training).predict(np.asarray(xWineValidationSets[fold]))\n",
    "\n",
    "    for i, label in enumerate(y_pred):\n",
    "        if label == labels_validation[i]:\n",
    "            wine_naive_accuracy += 1\n",
    "\n",
    "wine_naive_accuracy /= len(wine.data)\n",
    "\n",
    "print(f\"Naive base wine validation accuracy: {wine_naive_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
