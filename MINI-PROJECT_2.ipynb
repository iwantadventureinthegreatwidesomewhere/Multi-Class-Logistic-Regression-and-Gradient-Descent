{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lia\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797\n",
      "1797\n"
     ]
    }
   ],
   "source": [
    "print(len(digits.data))\n",
    "print(digits.target.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "wine = fetch_openml(name='wine', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n",
      "178\n"
     ]
    }
   ],
   "source": [
    "print(len(wine.data))\n",
    "print(wine.target.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalising wine data\n",
    "wine_data_norm = []\n",
    "for col in wine.data.T:\n",
    "    col_norm = col/np.amax(col)\n",
    "    wine_data_norm.append(col_norm)\n",
    "    \n",
    "wine.data = np.asarray(wine_data_norm).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation for digits dataset\n",
    "\n",
    "digitsTrainingSetSize = int(np.ceil(0.8 * len(digits.data)))\n",
    "digitsValidationSetSize = int(len(digits.data) - digitsTrainingSetSize)\n",
    "\n",
    "xDigitsTrainingSets = []\n",
    "yDigitsTrainingSets = []\n",
    "xDigitsValidationSets = []\n",
    "yDigitsValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data[foldIndex*digitsValidationSetSize:((foldIndex*digitsValidationSetSize)+digitsValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(digits.target[index])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(digits.target[index])\n",
    "            \n",
    "    xDigitsTrainingSets.append(xTrainingSet)\n",
    "    yDigitsTrainingSets.append(yTrainingSet)\n",
    "    xDigitsValidationSets.append(xValidationSet)\n",
    "    yDigitsValidationSets.append(yValidationSet)\n",
    "    \n",
    "# 5-fold cross validation for wine dataset\n",
    "\n",
    "wineTrainingSetSize = int(np.ceil(0.8 * len(wine.data)))\n",
    "wineValidationSetSize = int(len(wine.data) - wineTrainingSetSize)\n",
    "\n",
    "xWineTrainingSets = []\n",
    "yWineTrainingSets = []\n",
    "xWineValidationSets = []\n",
    "yWineValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "\n",
    "    for index, data in enumerate(wine.data[foldIndex*wineValidationSetSize:((foldIndex*wineValidationSetSize)+wineValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(wine.target[index])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "\n",
    "    for index, data in enumerate(wine.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(wine.target[index])\n",
    "            \n",
    "    xWineTrainingSets.append(xTrainingSet)\n",
    "    yWineTrainingSets.append(yTrainingSet)\n",
    "    xWineValidationSets.append(xValidationSet)\n",
    "    yWineValidationSets.append(yValidationSet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of y for digits dataset\n",
    "\n",
    "numberOfDigitsTargets = 10\n",
    "numberOfWineTargets = 3\n",
    "\n",
    "for index, fold in enumerate(yDigitsTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yDigitsValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsValidationSets[index] = encodedFold\n",
    "\n",
    "# one-hot encoding of y for wine dataset\n",
    "\n",
    "for index, fold in enumerate(yWineTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yWineValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineValidationSets[index] = encodedFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomIndices(arr, batch_size):\n",
    "    indices = []\n",
    "    \n",
    "    if batch_size > len(arr):\n",
    "        print(\"Error: batch size larger than size of dataset.\")\n",
    "        return\n",
    "    \n",
    "    while batch_size > 0:\n",
    "        x = np.floor(np.random.random() * len(arr))\n",
    "        if x not in indices:\n",
    "            indices.append(int(x))\n",
    "            batch_size -= 1\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent class\n",
    " \n",
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, batch_size, learning_rate=0.5, momentum=0.9, max_iters=20, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iters = max_iters\n",
    "        self.epsilon = epsilon\n",
    "            \n",
    "    def run(self, gradient_fn, x, y, w):\n",
    "        # TODO: use epsilon\n",
    "        \n",
    "        grad = np.inf\n",
    "        t = 1\n",
    "        \n",
    "        while np.linalg.norm(grad) > self.epsilon and t < self.max_iters:\n",
    "            # TODO: implement momentum here\n",
    "            \n",
    "            print(\"gradient descent step:\", t)\n",
    "            \n",
    "            gradients = gradient_fn(x, y, w, self.batch_size)\n",
    "            \n",
    "            for c in range(len(y[0])):\n",
    "                w[c] = w[c] - self.learning_rate * gradients[c]\n",
    "                print(\"w for class: \", w[c])\n",
    "            \n",
    "            print(\"###################################################\")\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, add_bias=True):\n",
    "        self.add_bias = add_bias\n",
    "        pass\n",
    "            \n",
    "    def fit(self, x, y, optimizer):\n",
    "        # TODO: add bias\n",
    "        \n",
    "        def gradient(x, y, w, batch_size):\n",
    "            gradients = np.zeros(len(w)).tolist()\n",
    "\n",
    "            indices = getRandomIndices(x, batch_size)\n",
    "            \n",
    "            print(\"indices for batch:\", indices)\n",
    "\n",
    "            for index in indices:\n",
    "                a = np.asarray(x[index])\n",
    "                b = np.asarray(y[index])\n",
    "                \n",
    "                print(\"x:\", a.astype(int))\n",
    "                print(\"y:\", b)\n",
    "                \n",
    "                # do max normalization on input for\n",
    "                # numerical stability during softmax\n",
    "                \n",
    "#                 max_x = np.amax(a)\n",
    "#                 a = a - max_x\n",
    "\n",
    "                print(\"x:\", a)\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  w[c] @ a\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  w[i] @ a\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "\n",
    "                    y_c = b[c]\n",
    "                    cost_c = np.dot(yh_c - y_c, a)\n",
    "                    \n",
    "                    gradients[c] += cost_c\n",
    "                    \n",
    "                    print(\"class:\", c)\n",
    "                    print(\"softmax numerator:\", num)\n",
    "                    print(\"softmax denominator:\", den)\n",
    "                    print(\"y hat for class:\", yh_c)\n",
    "                    print(\"y actual for class:\", y_c)\n",
    "                    print(\"x gradient:\", cost_c)\n",
    "                    print(\"new gradient for class:\", gradients[c])\n",
    "\n",
    "            return gradients\n",
    "        \n",
    "#        w0 = np.random.rand(len(y[0]),len(x[0])).tolist()\n",
    "        \n",
    "        w0 = []\n",
    "        for c in range(len(y[0])):\n",
    "            w0.append(np.zeros(len(x[0])))\n",
    "            \n",
    "        self.w = optimizer.run(gradient, x, y, w0)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # TODO: not tested yet, so not sure if it works\n",
    "        \n",
    "        a = np.asarray(x)\n",
    "        b = np.asarray(self.w)\n",
    "#         if self.add_bias:\n",
    "#             x = np.column_stack([x,np.ones(N)])\n",
    "        yh=[]\n",
    "        for i,x_c in enumerate(a):\n",
    "            yh_x=[]\n",
    "            for c in range(len(b)):\n",
    "                w_x =  b[c] @ x_c\n",
    "                num = np.exp(w_x)\n",
    "\n",
    "                den = 0\n",
    "                for i in range(len(b)):\n",
    "                    w_x =  b[i] @ x_c\n",
    "                    den += np.exp(w_x)\n",
    "\n",
    "                yh_c = num/den\n",
    "                yh_x.append(yh_c)\n",
    "            yh.append(yh_x)\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientDescentModel = GradientDescent(2)\n",
    "logisticRegressionModel = LogisticRegression(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient descent step: 1\n",
      "indices for batch: [91, 38]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.83816588 0.2637931  0.70897833 0.71666667 0.5308642  0.70618557\n",
      " 0.62007874 0.59090909 0.49441341 0.30307692 0.40350877 0.71\n",
      " 0.20952381]\n",
      "class: 0\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.27938863 0.08793103 0.23632611 0.23888889 0.17695473 0.23539519\n",
      " 0.20669291 0.1969697  0.16480447 0.10102564 0.13450292 0.23666667\n",
      " 0.06984127]\n",
      "new gradient for class: [0.27938863 0.08793103 0.23632611 0.23888889 0.17695473 0.23539519\n",
      " 0.20669291 0.1969697  0.16480447 0.10102564 0.13450292 0.23666667\n",
      " 0.06984127]\n",
      "class: 1\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.55877725 -0.17586207 -0.47265222 -0.47777778 -0.35390947 -0.47079038\n",
      " -0.41338583 -0.39393939 -0.32960894 -0.20205128 -0.26900585 -0.47333333\n",
      " -0.13968254]\n",
      "new gradient for class: [-0.55877725 -0.17586207 -0.47265222 -0.47777778 -0.35390947 -0.47079038\n",
      " -0.41338583 -0.39393939 -0.32960894 -0.20205128 -0.26900585 -0.47333333\n",
      " -0.13968254]\n",
      "class: 2\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.27938863 0.08793103 0.23632611 0.23888889 0.17695473 0.23539519\n",
      " 0.20669291 0.1969697  0.16480447 0.10102564 0.13450292 0.23666667\n",
      " 0.06984127]\n",
      "new gradient for class: [0.27938863 0.08793103 0.23632611 0.23888889 0.17695473 0.23539519\n",
      " 0.20669291 0.1969697  0.16480447 0.10102564 0.13450292 0.23666667\n",
      " 0.06984127]\n",
      "x: [0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.87592717 0.28793103 0.80495356 1.         0.85802469 0.85051546\n",
      " 0.56889764 0.31818182 0.54748603 0.25769231 0.76608187 0.875\n",
      " 0.58630952]\n",
      "class: 0\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.29197572 0.09597701 0.26831785 0.33333333 0.28600823 0.28350515\n",
      " 0.18963255 0.10606061 0.18249534 0.08589744 0.25536062 0.29166667\n",
      " 0.19543651]\n",
      "new gradient for class: [0.57136435 0.18390805 0.50464396 0.57222222 0.46296296 0.51890034\n",
      " 0.39632546 0.3030303  0.34729981 0.18692308 0.38986355 0.52833333\n",
      " 0.26527778]\n",
      "class: 1\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.58395145 -0.19195402 -0.53663571 -0.66666667 -0.57201646 -0.56701031\n",
      " -0.37926509 -0.21212121 -0.36499069 -0.17179487 -0.51072125 -0.58333333\n",
      " -0.39087302]\n",
      "new gradient for class: [-1.1427287  -0.36781609 -1.00928793 -1.14444444 -0.92592593 -1.03780069\n",
      " -0.79265092 -0.60606061 -0.69459963 -0.37384615 -0.7797271  -1.05666667\n",
      " -0.53055556]\n",
      "class: 2\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.29197572 0.09597701 0.26831785 0.33333333 0.28600823 0.28350515\n",
      " 0.18963255 0.10606061 0.18249534 0.08589744 0.25536062 0.29166667\n",
      " 0.19543651]\n",
      "new gradient for class: [0.57136435 0.18390805 0.50464396 0.57222222 0.46296296 0.51890034\n",
      " 0.39632546 0.3030303  0.34729981 0.18692308 0.38986355 0.52833333\n",
      " 0.26527778]\n",
      "w for class:  [-0.28568218 -0.09195402 -0.25232198 -0.28611111 -0.23148148 -0.25945017\n",
      " -0.19816273 -0.15151515 -0.17364991 -0.09346154 -0.19493177 -0.26416667\n",
      " -0.13263889]\n",
      "w for class:  [0.57136435 0.18390805 0.50464396 0.57222222 0.46296296 0.51890034\n",
      " 0.39632546 0.3030303  0.34729981 0.18692308 0.38986355 0.52833333\n",
      " 0.26527778]\n",
      "w for class:  [-0.28568218 -0.09195402 -0.25232198 -0.28611111 -0.23148148 -0.25945017\n",
      " -0.19816273 -0.15151515 -0.17364991 -0.09346154 -0.19493177 -0.26416667\n",
      " -0.13263889]\n",
      "###################################################\n",
      "gradient descent step: 2\n",
      "indices for batch: [17, 92]\n",
      "x: [0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      "y: [1. 0. 0.]\n",
      "x: [0.93189481 0.30172414 0.74922601 0.46666667 0.68518519 1.\n",
      " 0.73622047 0.48484848 0.52234637 0.54230769 0.59064327 0.815\n",
      " 0.70833333]\n",
      "class: 0\n",
      "softmax numerator: 0.16207108759749053\n",
      "softmax denominator: 38.394670778708466\n",
      "y hat for class: 0.004221187063475644\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.92796111 -0.3004505  -0.74606338 -0.46469678 -0.68229289 -0.99577881\n",
      " -0.73311275 -0.48280185 -0.52014145 -0.54001851 -0.58815006 -0.81155973\n",
      " -0.70534333]\n",
      "new gradient for class: [-0.92796111 -0.3004505  -0.74606338 -0.46469678 -0.68229289 -0.99577881\n",
      " -0.73311275 -0.48280185 -0.52014145 -0.54001851 -0.58815006 -0.81155973\n",
      " -0.70534333]\n",
      "class: 1\n",
      "softmax numerator: 38.07052860351349\n",
      "softmax denominator: 38.394670778708466\n",
      "y hat for class: 0.9915576258730487\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.9240274  0.29917687 0.74290076 0.46272689 0.6794006  0.99155763\n",
      " 0.73000502 0.48075521 0.51793653 0.53772933 0.58565684 0.80811947\n",
      " 0.70235332]\n",
      "new gradient for class: [0.9240274  0.29917687 0.74290076 0.46272689 0.6794006  0.99155763\n",
      " 0.73000502 0.48075521 0.51793653 0.53772933 0.58565684 0.80811947\n",
      " 0.70235332]\n",
      "class: 2\n",
      "softmax numerator: 0.16207108759749053\n",
      "softmax denominator: 38.394670778708466\n",
      "y hat for class: 0.004221187063475644\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.0039337  0.00127363 0.00316262 0.00196989 0.00289229 0.00422119\n",
      " 0.00310772 0.00204664 0.00220492 0.00228918 0.00249322 0.00344027\n",
      " 0.00299001]\n",
      "new gradient for class: [0.0039337  0.00127363 0.00316262 0.00196989 0.00289229 0.00422119\n",
      " 0.00310772 0.00204664 0.00220492 0.00228918 0.00249322 0.00344027\n",
      " 0.00299001]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.79501011 0.36724138 0.86068111 0.95       0.56790123 0.54896907\n",
      " 0.44094488 0.87878788 0.49162011 0.23076923 0.56725146 0.61\n",
      " 0.27738095]\n",
      "class: 0\n",
      "softmax numerator: 0.19019080836209232\n",
      "softmax denominator: 28.025658970205615\n",
      "y hat for class: 0.006786309951330181\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.00539519 0.00249221 0.00584085 0.00644699 0.00385395 0.00372547\n",
      " 0.00299239 0.00596373 0.00333629 0.00156607 0.00384954 0.00413965\n",
      " 0.00188239]\n",
      "new gradient for class: [-0.92256592 -0.29795829 -0.74022253 -0.45824978 -0.67843894 -0.99205334\n",
      " -0.73012036 -0.47683812 -0.51680516 -0.53845244 -0.58430051 -0.80742008\n",
      " -0.70346093]\n",
      "class: 1\n",
      "softmax numerator: 27.64527735348143\n",
      "softmax denominator: 28.025658970205615\n",
      "y hat for class: 0.9864273800973395\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.01079037 -0.00498443 -0.0116817  -0.01289399 -0.00770791 -0.00745095\n",
      " -0.00598478 -0.01192745 -0.00667257 -0.00313214 -0.00769909 -0.0082793\n",
      " -0.00376479]\n",
      "new gradient for class: [0.91323703 0.29419244 0.73121906 0.4498329  0.67169269 0.98410668\n",
      " 0.72402025 0.46882776 0.51126395 0.53459718 0.57795775 0.79984017\n",
      " 0.69858853]\n",
      "class: 2\n",
      "softmax numerator: 0.19019080836209232\n",
      "softmax denominator: 28.025658970205615\n",
      "y hat for class: 0.006786309951330181\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.00539519 0.00249221 0.00584085 0.00644699 0.00385395 0.00372547\n",
      " 0.00299239 0.00596373 0.00333629 0.00156607 0.00384954 0.00413965\n",
      " 0.00188239]\n",
      "new gradient for class: [0.00932889 0.00376585 0.00900347 0.00841688 0.00674625 0.00794666\n",
      " 0.00610011 0.00801036 0.00554121 0.00385525 0.00634276 0.00757992\n",
      " 0.0048724 ]\n",
      "w for class:  [ 0.17560078  0.05702512  0.11778929 -0.05698622  0.10773799  0.2365765\n",
      "  0.16689745  0.08690391  0.08475267  0.17576468  0.09721848  0.13954338\n",
      "  0.21909158]\n",
      "w for class:  [ 0.11474583  0.03681182  0.13903443  0.34730577  0.12711662  0.026847\n",
      "  0.03431534  0.06861642  0.09166784 -0.08037552  0.10088467  0.12841325\n",
      " -0.08401649]\n",
      "w for class:  [-0.29034662 -0.09383695 -0.25682372 -0.29031955 -0.23485461 -0.2634235\n",
      " -0.20121279 -0.15552033 -0.17642051 -0.09538917 -0.19810315 -0.26795662\n",
      " -0.13507509]\n",
      "###################################################\n",
      "gradient descent step: 3\n",
      "indices for batch: [104, 57]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 0. 1.]\n",
      "x: [0.86581254 0.51034483 0.80804954 0.8        0.62345679 0.59793814\n",
      " 0.11811024 0.8030303  0.22625698 0.37846154 0.52046784 0.5375\n",
      " 0.35119048]\n",
      "class: 0\n",
      "softmax numerator: 2.2639246786379075\n",
      "softmax denominator: 4.580308103080568\n",
      "y hat for class: 0.4942734479183321\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.42794815 0.2522499  0.39939743 0.39541876 0.30815814 0.29554495\n",
      " 0.05837875 0.39691656 0.11183282 0.18706349 0.25725343 0.26567198\n",
      " 0.17358413]\n",
      "new gradient for class: [0.42794815 0.2522499  0.39939743 0.39541876 0.30815814 0.29554495\n",
      " 0.05837875 0.39691656 0.11183282 0.18706349 0.25725343 0.26567198\n",
      " 0.17358413]\n",
      "class: 1\n",
      "softmax numerator: 2.1067154250717586\n",
      "softmax denominator: 4.580308103080568\n",
      "y hat for class: 0.45995059233129965\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.39823099 0.23473341 0.37166286 0.36796047 0.28675932 0.275022\n",
      " 0.05432487 0.36935426 0.10406703 0.17407361 0.23938949 0.24722344\n",
      " 0.16153027]\n",
      "new gradient for class: [0.39823099 0.23473341 0.37166286 0.36796047 0.28675932 0.275022\n",
      " 0.05432487 0.36935426 0.10406703 0.17407361 0.23938949 0.24722344\n",
      " 0.16153027]\n",
      "class: 2\n",
      "softmax numerator: 0.2096679993709015\n",
      "softmax denominator: 4.580308103080568\n",
      "y hat for class: 0.045775959750368224\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.82617914 -0.4869833  -0.77106029 -0.76337923 -0.59491746 -0.57056695\n",
      " -0.11270363 -0.76627082 -0.21589985 -0.3611371  -0.49664292 -0.51289542\n",
      " -0.3351144 ]\n",
      "new gradient for class: [-0.82617914 -0.4869833  -0.77106029 -0.76337923 -0.59491746 -0.57056695\n",
      " -0.11270363 -0.76627082 -0.21589985 -0.3611371  -0.49664292 -0.51289542\n",
      " -0.3351144 ]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.85569791 0.2637931  0.6996904  0.69       0.49382716 0.3556701\n",
      " 0.28740157 0.87878788 0.45251397 0.23461538 0.56140351 0.515\n",
      " 0.29464286]\n",
      "class: 0\n",
      "softmax numerator: 2.097339620814871\n",
      "softmax denominator: 4.362047217082774\n",
      "y hat for class: 0.4808154328548323\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.41143276 0.1268358  0.33642194 0.33176265 0.23743972 0.17101167\n",
      " 0.13818711 0.42253477 0.2175757  0.1128067  0.26993147 0.24761995\n",
      " 0.14166883]\n",
      "new gradient for class: [0.83938091 0.37908569 0.73581937 0.72718141 0.54559786 0.46655662\n",
      " 0.19656587 0.81945133 0.32940852 0.29987019 0.5271849  0.51329193\n",
      " 0.31525296]\n",
      "class: 1\n",
      "softmax numerator: 2.029811664645415\n",
      "softmax denominator: 4.362047217082774\n",
      "y hat for class: 0.4653346384459592\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.45751203 -0.14104104 -0.37410022 -0.3689191  -0.26403228 -0.19016448\n",
      " -0.15366367 -0.46985744 -0.24194354 -0.12544072 -0.30016301 -0.27535266\n",
      " -0.15753533]\n",
      "new gradient for class: [-0.05928104  0.09369237 -0.00243736 -0.00095863  0.02272704  0.08485752\n",
      " -0.09933879 -0.10050318 -0.13787651  0.04863289 -0.06077352 -0.02812922\n",
      "  0.00399494]\n",
      "class: 2\n",
      "softmax numerator: 0.23489593162248842\n",
      "softmax denominator: 4.362047217082774\n",
      "y hat for class: 0.05384992869920854\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.04607927 0.01420524 0.03767828 0.03715645 0.02659256 0.01915281\n",
      " 0.01547655 0.04732266 0.02436784 0.01263402 0.03023154 0.02773271\n",
      " 0.0158665 ]\n",
      "new gradient for class: [-0.78009987 -0.47277806 -0.73338201 -0.72622278 -0.5683249  -0.55141414\n",
      " -0.09722707 -0.71894816 -0.19153201 -0.34850308 -0.46641138 -0.48516271\n",
      " -0.3192479 ]\n",
      "w for class:  [-0.24408967 -0.13251772 -0.2501204  -0.42057692 -0.16506094  0.00329819\n",
      "  0.06861452 -0.32282176 -0.07995159  0.02582959 -0.16637397 -0.11710259\n",
      "  0.0614651 ]\n",
      "w for class:  [ 0.14438636 -0.01003436  0.14025311  0.34778508  0.1157531  -0.01558175\n",
      "  0.08398473  0.11886801  0.16060609 -0.10469196  0.13127143  0.14247786\n",
      " -0.08601396]\n",
      "w for class:  [ 0.09970332  0.14255208  0.10986729  0.07279184  0.04930784  0.01228357\n",
      " -0.15259925  0.20395374 -0.08065451  0.07886237  0.03510254 -0.02537527\n",
      "  0.02454886]\n",
      "###################################################\n",
      "gradient descent step: 4\n",
      "indices for batch: [76, 62]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.84423466 0.41896552 0.67182663 0.7        0.54320988 0.65721649\n",
      " 0.44685039 0.39393939 0.34078212 0.15384615 0.52631579 0.695\n",
      " 0.19345238]\n",
      "class: 0\n",
      "softmax numerator: 0.33676468168407564\n",
      "softmax denominator: 3.881469797781873\n",
      "y hat for class: 0.0867621543458937\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.07324762 0.03635035 0.05828913 0.06073351 0.04713006 0.05702152\n",
      " 0.0387697  0.03417903 0.02956699 0.01334802 0.04566429 0.0602997\n",
      " 0.01678435]\n",
      "new gradient for class: [0.07324762 0.03635035 0.05828913 0.06073351 0.04713006 0.05702152\n",
      " 0.0387697  0.03417903 0.02956699 0.01334802 0.04566429 0.0602997\n",
      " 0.01678435]\n",
      "class: 1\n",
      "softmax numerator: 2.1868414749610183\n",
      "softmax denominator: 3.881469797781873\n",
      "y hat for class: 0.5634055110285087\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.3685882  -0.18291804 -0.2933158  -0.30561614 -0.23716244 -0.2869371\n",
      " -0.19509242 -0.17199177 -0.1487836  -0.06716838 -0.22978657 -0.30343317\n",
      " -0.08446024]\n",
      "new gradient for class: [-0.3685882  -0.18291804 -0.2933158  -0.30561614 -0.23716244 -0.2869371\n",
      " -0.19509242 -0.17199177 -0.1487836  -0.06716838 -0.22978657 -0.30343317\n",
      " -0.08446024]\n",
      "class: 2\n",
      "softmax numerator: 1.3578636411367793\n",
      "softmax denominator: 3.881469797781873\n",
      "y hat for class: 0.3498323346255977\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.29534058 0.14656769 0.23502668 0.24488263 0.19003238 0.22991558\n",
      " 0.15632272 0.13781274 0.11921661 0.05382036 0.18412228 0.24313347\n",
      " 0.0676759 ]\n",
      "new gradient for class: [0.29534058 0.14656769 0.23502668 0.24488263 0.19003238 0.22991558\n",
      " 0.15632272 0.13781274 0.11921661 0.05382036 0.18412228 0.24313347\n",
      " 0.0676759 ]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.82872556 0.24310345 0.6130031  0.53333333 0.52469136 0.65721649\n",
      " 0.49212598 0.43939394 0.49441341 0.22307692 0.71929825 0.685\n",
      " 0.2547619 ]\n",
      "class: 0\n",
      "softmax numerator: 0.3596426202543493\n",
      "softmax denominator: 3.7973570780528467\n",
      "y hat for class: 0.09470866522743802\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.07848749 0.023024   0.05805671 0.05051129 0.04969282 0.0622441\n",
      " 0.0466086  0.04161441 0.04682523 0.02112732 0.06812378 0.06487544\n",
      " 0.02412816]\n",
      "new gradient for class: [0.15173511 0.05937435 0.11634583 0.1112448  0.09682288 0.11926562\n",
      " 0.0853783  0.07579344 0.07639223 0.03447534 0.11378807 0.12517513\n",
      " 0.04091251]\n",
      "class: 1\n",
      "softmax numerator: 2.135908899063061\n",
      "softmax denominator: 3.7973570780528467\n",
      "y hat for class: 0.5624724920939701\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.36259023 -0.10636445 -0.26820572 -0.233348   -0.2295669  -0.2875503\n",
      " -0.21531866 -0.19224694 -0.21631947 -0.09760229 -0.31471277 -0.29970634\n",
      " -0.11146534]\n",
      "new gradient for class: [-0.73117843 -0.28928248 -0.56152152 -0.53896415 -0.46672934 -0.57448739\n",
      " -0.41041107 -0.3642387  -0.36510306 -0.16477067 -0.54449934 -0.60313951\n",
      " -0.19592558]\n",
      "class: 2\n",
      "softmax numerator: 1.3018055587354362\n",
      "softmax denominator: 3.7973570780528467\n",
      "y hat for class: 0.34281884267859186\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.28410274 0.08334044 0.21014901 0.18283672 0.17987408 0.2253062\n",
      " 0.16871006 0.15063252 0.16949423 0.07647497 0.24658899 0.23483091\n",
      " 0.08733718]\n",
      "new gradient for class: [0.57944332 0.22990813 0.44517569 0.42771935 0.36990646 0.45522178\n",
      " 0.32503278 0.28844526 0.28871084 0.13029533 0.43071127 0.47796438\n",
      " 0.15501308]\n",
      "w for class:  [-0.31995723 -0.1622049  -0.30829332 -0.47619932 -0.21347238 -0.05633462\n",
      "  0.02592537 -0.36071848 -0.1181477   0.00859192 -0.223268   -0.17969015\n",
      "  0.04100884]\n",
      "w for class:  [ 0.50997557  0.13460688  0.42101387  0.61726716  0.34911777  0.27166194\n",
      "  0.28919027  0.30098736  0.34315762 -0.02230662  0.4035211   0.44404762\n",
      "  0.01194884]\n",
      "w for class:  [-0.19001834  0.02759802 -0.11272055 -0.14106784 -0.13564539 -0.21532732\n",
      " -0.31511564  0.05973111 -0.22500993  0.01371471 -0.1802531  -0.26435746\n",
      " -0.05295768]\n",
      "###################################################\n",
      "gradient descent step: 5\n",
      "indices for batch: [126, 32]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 0. 1.]\n",
      "x: [0.92312879 0.56206897 0.78637771 0.66666667 0.66049383 0.47164948\n",
      " 0.11023622 0.75757576 0.22346369 0.45230769 0.56140351 0.455\n",
      " 0.4047619 ]\n",
      "class: 0\n",
      "softmax numerator: 0.2024153527180267\n",
      "softmax denominator: 11.754985312638095\n",
      "y hat for class: 0.017219532592729368\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.01589585 0.00967856 0.01354106 0.01147969 0.01137339 0.00812158\n",
      " 0.00189822 0.0130451  0.00384794 0.00778853 0.00966711 0.00783489\n",
      " 0.00696981]\n",
      "new gradient for class: [0.01589585 0.00967856 0.01354106 0.01147969 0.01137339 0.00812158\n",
      " 0.00189822 0.0130451  0.00384794 0.00778853 0.00966711 0.00783489\n",
      " 0.00696981]\n",
      "class: 1\n",
      "softmax numerator: 11.107807412469477\n",
      "softmax denominator: 11.754985312638095\n",
      "y hat for class: 0.944944388873645\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.87230537 0.53112392 0.7430832  0.62996293 0.62412994 0.44568253\n",
      " 0.1041671  0.71586696 0.21116076 0.42740562 0.5304951  0.4299497\n",
      " 0.38247749]\n",
      "new gradient for class: [0.87230537 0.53112392 0.7430832  0.62996293 0.62412994 0.44568253\n",
      " 0.1041671  0.71586696 0.21116076 0.42740562 0.5304951  0.4299497\n",
      " 0.38247749]\n",
      "class: 2\n",
      "softmax numerator: 0.44476254745059074\n",
      "softmax denominator: 11.754985312638095\n",
      "y hat for class: 0.037836078533625625\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.88820122 -0.54080248 -0.75662426 -0.64144261 -0.63550333 -0.45380412\n",
      " -0.10606531 -0.72891206 -0.2150087  -0.43519414 -0.5401622  -0.43778458\n",
      " -0.3894473 ]\n",
      "new gradient for class: [-0.88820122 -0.54080248 -0.75662426 -0.64144261 -0.63550333 -0.45380412\n",
      " -0.10606531 -0.72891206 -0.2150087  -0.43519414 -0.5401622  -0.43778458\n",
      " -0.3894473 ]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.83412003 0.20172414 0.59442724 0.65333333 0.48148148 0.54381443\n",
      " 0.39370079 0.40909091 0.29050279 0.36       0.65497076 0.87\n",
      " 0.30357143]\n",
      "class: 0\n",
      "softmax numerator: 0.25004832478772165\n",
      "softmax denominator: 11.687239792460435\n",
      "y hat for class: 0.021394985405281966\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.01784599 0.00431588 0.01271776 0.01397806 0.01030129 0.0116349\n",
      " 0.00842322 0.00875249 0.0062153  0.00770219 0.01401309 0.01861364\n",
      " 0.00649491]\n",
      "new gradient for class: [0.03374183 0.01399445 0.02625882 0.02545775 0.02167468 0.01975649\n",
      " 0.01032144 0.02179759 0.01006324 0.01549072 0.0236802  0.02644852\n",
      " 0.01346472]\n",
      "class: 1\n",
      "softmax numerator: 11.07612410186443\n",
      "softmax denominator: 11.687239792460435\n",
      "y hat for class: 0.9477108623209526\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.04361542 -0.01054798 -0.03108209 -0.03416224 -0.02517625 -0.02843559\n",
      " -0.02058627 -0.02139101 -0.01519014 -0.01882409 -0.03424786 -0.04549155\n",
      " -0.01587349]\n",
      "new gradient for class: [0.82868996 0.52057593 0.71200112 0.59580069 0.59895368 0.41724695\n",
      " 0.08358082 0.69447595 0.19597062 0.40858153 0.49624724 0.38445815\n",
      " 0.366604  ]\n",
      "class: 2\n",
      "softmax numerator: 0.36106736580828264\n",
      "softmax denominator: 11.687239792460435\n",
      "y hat for class: 0.03089415227376537\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.02576943 0.0062321  0.01836433 0.02018418 0.01487496 0.01680069\n",
      " 0.01216305 0.01263852 0.00897484 0.01112189 0.02023477 0.02687791\n",
      " 0.00937858]\n",
      "new gradient for class: [-0.86243179 -0.53457038 -0.73825993 -0.62125843 -0.62062837 -0.43700343\n",
      " -0.09390226 -0.71627354 -0.20603386 -0.42407225 -0.51992744 -0.41090667\n",
      " -0.38006872]\n",
      "w for class:  [-0.33682814 -0.16920213 -0.32142273 -0.48892819 -0.22430972 -0.06621286\n",
      "  0.02076465 -0.37161728 -0.12317932  0.00084656 -0.2351081  -0.19291442\n",
      "  0.03427649]\n",
      "w for class:  [ 0.09563059 -0.12568109  0.06501331  0.31936681  0.04964093  0.06303847\n",
      "  0.24739986 -0.04625061  0.24517232 -0.22659739  0.15539748  0.25181854\n",
      " -0.17135317]\n",
      "w for class:  [ 0.24119755  0.29488321  0.25640941  0.16956138  0.1746688   0.0031744\n",
      " -0.26816451  0.41786789 -0.121993    0.22575083  0.07971062 -0.05890412\n",
      "  0.13707668]\n",
      "###################################################\n",
      "gradient descent step: 6\n",
      "indices for batch: [15, 70]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [1. 0. 0.]\n",
      "x: [0.87997303 0.29827586 0.63157895 0.41333333 0.56790123 0.70103093\n",
      " 0.64370079 0.25757576 0.81284916 0.55384615 0.65497076 0.7275\n",
      " 0.68452381]\n",
      "class: 0\n",
      "softmax numerator: 0.2519190193075733\n",
      "softmax denominator: 4.23679527218196\n",
      "y hat for class: 0.05945980466925757\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.82765    -0.28054044 -0.59402539 -0.38875661 -0.53413394 -0.65934777\n",
      " -0.60542646 -0.24226035 -0.76451731 -0.52091457 -0.61602633 -0.68424299\n",
      " -0.64382216]\n",
      "new gradient for class: [-0.82765    -0.28054044 -0.59402539 -0.38875661 -0.53413394 -0.65934777\n",
      " -0.60542646 -0.24226035 -0.76451731 -0.52091457 -0.61602633 -0.68424299\n",
      " -0.64382216]\n",
      "class: 1\n",
      "softmax numerator: 1.9757018383811067\n",
      "softmax denominator: 4.23679527218196\n",
      "y hat for class: 0.46631987421087145\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.41034891 0.13909196 0.29451782 0.19274555 0.26482363 0.32690465\n",
      " 0.30017047 0.12011269 0.37904772 0.25826947 0.30542588 0.33924771\n",
      " 0.31920706]\n",
      "new gradient for class: [0.41034891 0.13909196 0.29451782 0.19274555 0.26482363 0.32690465\n",
      " 0.30017047 0.12011269 0.37904772 0.25826947 0.30542588 0.33924771\n",
      " 0.31920706]\n",
      "class: 2\n",
      "softmax numerator: 2.0091744144932804\n",
      "softmax denominator: 4.23679527218196\n",
      "y hat for class: 0.47422032111987095\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.41730109 0.14144848 0.29950757 0.19601107 0.26931031 0.33244311\n",
      " 0.30525599 0.12214766 0.38546959 0.2626451  0.31060044 0.34499528\n",
      " 0.3246151 ]\n",
      "new gradient for class: [0.41730109 0.14144848 0.29950757 0.19601107 0.26931031 0.33244311\n",
      " 0.30525599 0.12214766 0.38546959 0.2626451  0.31060044 0.34499528\n",
      " 0.3246151 ]\n",
      "x: [0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.83749157 0.43965517 0.70278638 0.73333333 0.55555556 0.43298969\n",
      " 0.36220472 1.         0.39664804 0.20769231 0.50292398 0.825\n",
      " 0.1875    ]\n",
      "class: 0\n",
      "softmax numerator: 0.1689834559082635\n",
      "softmax denominator: 5.115118671591952\n",
      "y hat for class: 0.03303607731463866\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.02766744 0.01452448 0.02321731 0.02422646 0.01835338 0.01430428\n",
      " 0.01196582 0.03303608 0.0131037  0.00686134 0.01661464 0.02725476\n",
      " 0.00619426]\n",
      "new gradient for class: [-0.79998257 -0.26601596 -0.57080808 -0.36453016 -0.51578056 -0.64504348\n",
      " -0.59346064 -0.20922428 -0.75141361 -0.51405323 -0.59941169 -0.65698823\n",
      " -0.63762789]\n",
      "class: 1\n",
      "softmax numerator: 2.0277314221426366\n",
      "softmax denominator: 5.115118671591952\n",
      "y hat for class: 0.3964192333218256\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.5054938  -0.26536741 -0.42418834 -0.4426259  -0.33532265 -0.26134425\n",
      " -0.21861981 -0.60358077 -0.23940913 -0.12535908 -0.30355524 -0.49795413\n",
      " -0.11317139]\n",
      "new gradient for class: [-0.09514489 -0.12627544 -0.12967053 -0.24988035 -0.07049902  0.0655604\n",
      "  0.08155066 -0.48346807  0.13963859  0.13291039  0.00187064 -0.15870642\n",
      "  0.20603566]\n",
      "class: 2\n",
      "softmax numerator: 2.9184037935410516\n",
      "softmax denominator: 5.115118671591952\n",
      "y hat for class: 0.5705446893635358\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.47782637 0.25084292 0.40097104 0.41839944 0.31696927 0.24703997\n",
      " 0.20665398 0.57054469 0.22630544 0.11849774 0.2869406  0.47069937\n",
      " 0.10697713]\n",
      "new gradient for class: [0.89512746 0.3922914  0.70047861 0.6144105  0.58627958 0.57948308\n",
      " 0.51190998 0.69269235 0.61177503 0.38114284 0.59754105 0.81569465\n",
      " 0.43159223]\n",
      "w for class:  [ 0.06316314 -0.03619415 -0.03601869 -0.30666311  0.03358056  0.25630888\n",
      "  0.31749497 -0.26700514  0.25252749  0.25787317  0.06459775  0.1355797\n",
      "  0.35309043]\n",
      "w for class:  [ 0.14320304 -0.06254336  0.12984858  0.44430699  0.08489043  0.03025827\n",
      "  0.20662453  0.19548342  0.17535302 -0.29305258  0.15446216  0.33117175\n",
      " -0.274371  ]\n",
      "w for class:  [-0.20636618  0.09873751 -0.09382989 -0.13764387 -0.11847099 -0.28656715\n",
      " -0.5241195   0.07152171 -0.42788051  0.03517941 -0.21905991 -0.46675145\n",
      " -0.07871943]\n",
      "###################################################\n",
      "gradient descent step: 7\n",
      "indices for batch: [8, 2]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [1. 0. 0.]\n",
      "x: [0.8927849  0.6862069  0.70897833 0.58333333 0.63580247 0.68041237\n",
      " 0.51771654 0.48484848 0.46368715 0.33538462 0.47953216 0.75\n",
      " 0.4047619 ]\n",
      "class: 0\n",
      "softmax numerator: 1.710784956492687\n",
      "softmax denominator: 4.423439790431724\n",
      "y hat for class: 0.38675443490680267\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.54749638 -0.42081334 -0.43477782 -0.35772658 -0.38990304 -0.41725987\n",
      " -0.31748737 -0.29733118 -0.28435409 -0.20567313 -0.29407097 -0.45993417\n",
      " -0.24821844]\n",
      "new gradient for class: [-0.54749638 -0.42081334 -0.43477782 -0.35772658 -0.38990304 -0.41725987\n",
      " -0.31748737 -0.29733118 -0.28435409 -0.20567313 -0.29407097 -0.45993417\n",
      " -0.24821844]\n",
      "class: 1\n",
      "softmax numerator: 2.4766385617004594\n",
      "softmax denominator: 4.423439790431724\n",
      "y hat for class: 0.5598897417022922\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.4998611  0.3842002  0.39694969 0.32660235 0.35597928 0.38095591\n",
      " 0.28986418 0.27146169 0.25961368 0.18777841 0.26848514 0.41991731\n",
      " 0.22662204]\n",
      "new gradient for class: [0.4998611  0.3842002  0.39694969 0.32660235 0.35597928 0.38095591\n",
      " 0.28986418 0.27146169 0.25961368 0.18777841 0.26848514 0.41991731\n",
      " 0.22662204]\n",
      "class: 2\n",
      "softmax numerator: 0.2360162722385781\n",
      "softmax denominator: 4.423439790431724\n",
      "y hat for class: 0.05335582339090528\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.04763527 0.03661313 0.03782812 0.03112423 0.03392376 0.03630396\n",
      " 0.02762319 0.02586949 0.02474041 0.01789472 0.02558583 0.04001687\n",
      " 0.0215964 ]\n",
      "new gradient for class: [0.04763527 0.03661313 0.03782812 0.03112423 0.03392376 0.03630396\n",
      " 0.02762319 0.02586949 0.02474041 0.01789472 0.02558583 0.04001687\n",
      " 0.0215964 ]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [1. 0. 0.]\n",
      "x: [0.87997303 0.28448276 0.78947368 0.6        0.60493827 0.6314433\n",
      " 0.47834646 0.43939394 0.40223464 0.32692308 0.65497076 0.6275\n",
      " 0.6577381 ]\n",
      "class: 0\n",
      "softmax numerator: 1.813156656960093\n",
      "softmax denominator: 4.358817740659044\n",
      "y hat for class: 0.4159744143571251\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.51392676 -0.16614521 -0.46107283 -0.35041535 -0.35329943 -0.36877904\n",
      " -0.27936657 -0.2566173  -0.23491532 -0.19093144 -0.38251968 -0.36647605\n",
      " -0.38413588]\n",
      "new gradient for class: [-1.06142314 -0.58695855 -0.89585065 -0.70814193 -0.74320247 -0.78603891\n",
      " -0.59685394 -0.55394849 -0.51926941 -0.39660457 -0.67659065 -0.82641023\n",
      " -0.63235432]\n",
      "class: 1\n",
      "softmax numerator: 2.306548716212744\n",
      "softmax denominator: 4.358817740659044\n",
      "y hat for class: 0.5291684244324469\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.46565394 0.15053929 0.41776455 0.31750105 0.32011423 0.33413986\n",
      " 0.25312584 0.2325134  0.21284987 0.17299737 0.34658985 0.33205319\n",
      " 0.34805423]\n",
      "new gradient for class: [0.96551505 0.5347395  0.81471424 0.6441034  0.67609351 0.71509576\n",
      " 0.54299002 0.50397509 0.47246355 0.36077578 0.61507498 0.75197049\n",
      " 0.57467627]\n",
      "class: 2\n",
      "softmax numerator: 0.2391123674862066\n",
      "softmax denominator: 4.358817740659044\n",
      "y hat for class: 0.054857161210427975\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.04827282 0.01560592 0.04330829 0.0329143  0.0331852  0.03463919\n",
      " 0.02624073 0.0241039  0.02206545 0.01793407 0.03592984 0.03442287\n",
      " 0.03608164]\n",
      "new gradient for class: [0.0959081  0.05221905 0.08113641 0.06403853 0.06710896 0.07094315\n",
      " 0.05386392 0.04997339 0.04680586 0.03582879 0.06151567 0.07443974\n",
      " 0.05767805]\n",
      "w for class:  [0.59387471 0.25728512 0.41190664 0.04740785 0.40518179 0.64932833\n",
      " 0.61592194 0.00996911 0.51216219 0.45617546 0.40289307 0.54878481\n",
      " 0.66926759]\n",
      "w for class:  [-0.33955449 -0.32991311 -0.27750854  0.12225528 -0.25315632 -0.32728961\n",
      " -0.06487048 -0.05650412 -0.06087875 -0.47344047 -0.15307533 -0.04481349\n",
      " -0.56170913]\n",
      "w for class:  [-0.25432023  0.07262799 -0.13439809 -0.16966314 -0.15202547 -0.32203872\n",
      " -0.55105146  0.04653502 -0.45128344  0.01726501 -0.24981774 -0.50397132\n",
      " -0.10755846]\n",
      "###################################################\n",
      "gradient descent step: 8\n",
      "indices for batch: [105, 5]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 0. 1.]\n",
      "x: [0.87188132 0.48448276 0.83591331 0.7        0.59259259 0.39690722\n",
      " 0.0984252  0.8030303  0.20949721 0.35384615 0.4502924  0.5775\n",
      " 0.35714286]\n",
      "class: 0\n",
      "softmax numerator: 13.362874643240104\n",
      "softmax denominator: 13.913514427754796\n",
      "y hat for class: 0.9604241051120578\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.83737584 0.46530892 0.8028313  0.67229687 0.56914021 0.38119926\n",
      " 0.09452993 0.77124966 0.20120617 0.33984238 0.43247167 0.55464492\n",
      " 0.34300861]\n",
      "new gradient for class: [0.83737584 0.46530892 0.8028313  0.67229687 0.56914021 0.38119926\n",
      " 0.09452993 0.77124966 0.20120617 0.33984238 0.43247167 0.55464492\n",
      " 0.34300861]\n",
      "class: 1\n",
      "softmax numerator: 0.2442257515061023\n",
      "softmax denominator: 13.913514427754796\n",
      "y hat for class: 0.017553131724858725\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.01530425 0.00850419 0.0146729  0.01228719 0.01040186 0.00696696\n",
      " 0.00172767 0.0140957  0.00367733 0.00621111 0.00790404 0.01013693\n",
      " 0.00626898]\n",
      "new gradient for class: [0.01530425 0.00850419 0.0146729  0.01228719 0.01040186 0.00696696\n",
      " 0.00172767 0.0140957  0.00367733 0.00621111 0.00790404 0.01013693\n",
      " 0.00626898]\n",
      "class: 2\n",
      "softmax numerator: 0.30641403300859044\n",
      "softmax denominator: 13.913514427754796\n",
      "y hat for class: 0.022022763163083593\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.85268009 -0.47381311 -0.81750419 -0.68458407 -0.57954207 -0.38816622\n",
      " -0.0962576  -0.78534536 -0.2048835  -0.34605348 -0.44037571 -0.56478185\n",
      " -0.34927758]\n",
      "new gradient for class: [-0.85268009 -0.47381311 -0.81750419 -0.68458407 -0.57954207 -0.38816622\n",
      " -0.0962576  -0.78534536 -0.2048835  -0.34605348 -0.44037571 -0.56478185\n",
      " -0.34927758]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [1. 0. 0.]\n",
      "x: [0.91436278 0.29482759 0.71517028 0.54       0.72222222 0.81185567\n",
      " 0.6476378  0.51515152 0.65363128 0.47153846 0.55555556 0.845\n",
      " 0.47321429]\n",
      "class: 0\n",
      "softmax numerator: 41.14105633163393\n",
      "softmax denominator: 41.45623784129921\n",
      "y hat for class: 0.9923972476501162\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.00695167 -0.0022415  -0.00543726 -0.00410549 -0.00549088 -0.00617234\n",
      " -0.00492383 -0.00391657 -0.0049694  -0.00358499 -0.00422375 -0.00642433\n",
      " -0.00359773]\n",
      "new gradient for class: [0.83042416 0.46306742 0.79739403 0.66819139 0.56364933 0.37502692\n",
      " 0.0896061  0.76733309 0.19623677 0.33625739 0.42824792 0.54822059\n",
      " 0.33941088]\n",
      "class: 1\n",
      "softmax numerator: 0.18057393475416916\n",
      "softmax denominator: 41.45623784129921\n",
      "y hat for class: 0.004355772355548366\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.00398276 0.0012842  0.00311512 0.00235212 0.00314584 0.00353626\n",
      " 0.00282096 0.00224388 0.00284707 0.00205391 0.00241987 0.00368063\n",
      " 0.00206121]\n",
      "new gradient for class: [0.019287   0.00978839 0.01778802 0.01463931 0.01354769 0.01050322\n",
      " 0.00454863 0.01633958 0.0065244  0.00826502 0.01032392 0.01381756\n",
      " 0.00833019]\n",
      "class: 2\n",
      "softmax numerator: 0.13460757491110995\n",
      "softmax denominator: 41.45623784129921\n",
      "y hat for class: 0.0032469799943354296\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.00296892 0.0009573  0.00232214 0.00175337 0.00234504 0.00263608\n",
      " 0.00210287 0.00167269 0.00212233 0.00153108 0.00180388 0.0027437\n",
      " 0.00153652]\n",
      "new gradient for class: [-0.84971117 -0.47285581 -0.81518205 -0.6828307  -0.57719703 -0.38553014\n",
      " -0.09415474 -0.78367267 -0.20276117 -0.34452241 -0.43857184 -0.56203816\n",
      " -0.34774107]\n",
      "w for class:  [ 0.17866263  0.02575141  0.01320962 -0.28668784  0.12335713  0.46181487\n",
      "  0.57111889 -0.37369744  0.41404381  0.28804676  0.18876911  0.27467451\n",
      "  0.49956215]\n",
      "w for class:  [-0.34919799 -0.33480731 -0.28640255  0.11493563 -0.25993017 -0.33254123\n",
      " -0.0671448  -0.06467391 -0.06414095 -0.47757298 -0.15823729 -0.05172227\n",
      " -0.56587423]\n",
      "w for class:  [ 0.17053536  0.30905589  0.27319293  0.17175221  0.13657304 -0.12927365\n",
      " -0.50397409  0.43837135 -0.34990285  0.18952622 -0.03053182 -0.22295224\n",
      "  0.06631207]\n",
      "###################################################\n",
      "gradient descent step: 9\n",
      "indices for batch: [128, 46]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 0. 1.]\n",
      "x: [0.87390425 0.59482759 0.72755418 0.61666667 0.65432099 0.35824742\n",
      " 0.13779528 0.60606061 0.26256983 0.40615385 0.39766082 0.4375\n",
      " 0.40178571]\n",
      "class: 0\n",
      "softmax numerator: 2.0632960439851096\n",
      "softmax denominator: 4.464769176778352\n",
      "y hat for class: 0.46212826739543206\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.40385586 0.27488664 0.33622335 0.2849791  0.30238022 0.16555626\n",
      " 0.06367909 0.28007774 0.12134094 0.18769517 0.18377031 0.20218112\n",
      " 0.18567654]\n",
      "new gradient for class: [0.40385586 0.27488664 0.33622335 0.2849791  0.30238022 0.16555626\n",
      " 0.06367909 0.28007774 0.12134094 0.18769517 0.18377031 0.20218112\n",
      " 0.18567654]\n",
      "class: 1\n",
      "softmax numerator: 0.22241817379333692\n",
      "softmax denominator: 4.464769176778352\n",
      "y hat for class: 0.04981627604628542\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.04353466 0.0296321  0.03624404 0.03072004 0.03259583 0.01784655\n",
      " 0.00686445 0.03019168 0.01308025 0.02023307 0.01980998 0.02179462\n",
      " 0.02001547]\n",
      "new gradient for class: [0.04353466 0.0296321  0.03624404 0.03072004 0.03259583 0.01784655\n",
      " 0.00686445 0.03019168 0.01308025 0.02023307 0.01980998 0.02179462\n",
      " 0.02001547]\n",
      "class: 2\n",
      "softmax numerator: 2.1790549589999055\n",
      "softmax denominator: 4.464769176778352\n",
      "y hat for class: 0.4880554565582824\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.44739051 -0.30451874 -0.37246739 -0.31569914 -0.33497606 -0.18340281\n",
      " -0.07054354 -0.31026942 -0.13442119 -0.20792825 -0.20358029 -0.22397574\n",
      " -0.205692  ]\n",
      "new gradient for class: [-0.44739051 -0.30451874 -0.37246739 -0.31569914 -0.33497606 -0.18340281\n",
      " -0.07054354 -0.31026942 -0.13442119 -0.20792825 -0.20358029 -0.22397574\n",
      " -0.205692  ]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.85772084 0.31206897 0.68111455 0.62666667 0.5308642  0.56701031\n",
      " 0.4980315  0.39393939 0.49441341 0.3        0.67836257 0.785\n",
      " 0.425     ]\n",
      "class: 0\n",
      "softmax numerator: 3.6777064251119187\n",
      "softmax denominator: 5.095899147920866\n",
      "y hat for class: 0.7216992170287413\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.61901646 0.22521993 0.49155984 0.45226484 0.38312428 0.4092109\n",
      " 0.35942894 0.28430575 0.35681777 0.21650977 0.48957374 0.56653389\n",
      " 0.30672217]\n",
      "new gradient for class: [1.02287231 0.50010657 0.82778319 0.73724394 0.6855045  0.57476716\n",
      " 0.42310803 0.56438349 0.47815871 0.40420494 0.67334404 0.768715\n",
      " 0.4923987 ]\n",
      "class: 1\n",
      "softmax numerator: 0.2285663923201619\n",
      "softmax denominator: 5.095899147920866\n",
      "y hat for class: 0.04485300546291567\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.81924948 -0.29807173 -0.65056452 -0.59855878 -0.50705334 -0.54157819\n",
      " -0.47569329 -0.37627003 -0.47223748 -0.2865441  -0.64793597 -0.74979039\n",
      " -0.40593747]\n",
      "new gradient for class: [-0.77571482 -0.26843964 -0.61432048 -0.56783875 -0.47445751 -0.52373164\n",
      " -0.46882884 -0.34607835 -0.45915723 -0.26631103 -0.62812599 -0.72799577\n",
      " -0.385922  ]\n",
      "class: 2\n",
      "softmax numerator: 1.1896263304887857\n",
      "softmax denominator: 5.095899147920866\n",
      "y hat for class: 0.23344777750834314\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.20023302 0.07285181 0.15900468 0.14629394 0.12392907 0.1323673\n",
      " 0.11626435 0.09196428 0.11541971 0.07003433 0.15836224 0.18325651\n",
      " 0.09921531]\n",
      "new gradient for class: [-0.24715749 -0.23166693 -0.21346271 -0.16940519 -0.21104699 -0.05103552\n",
      "  0.04572081 -0.21830514 -0.01900148 -0.13789391 -0.04521805 -0.04071923\n",
      " -0.1064767 ]\n",
      "w for class:  [-0.33277353 -0.22430187 -0.40068197 -0.65530981 -0.21939512  0.17443129\n",
      "  0.35956487 -0.65588918  0.17496445  0.08594429 -0.14790291 -0.10968299\n",
      "  0.2533628 ]\n",
      "w for class:  [ 0.03865942 -0.20058749  0.02075769  0.398855   -0.02270141 -0.07067541\n",
      "  0.16726962  0.10836526  0.16543766 -0.34441747  0.15582571  0.31227561\n",
      " -0.37291322]\n",
      "w for class:  [ 0.2941141   0.42488936  0.37992429  0.25645481  0.24209654 -0.10375589\n",
      " -0.52683449  0.54752392 -0.34040211  0.25847317 -0.0079228  -0.20259262\n",
      "  0.11955042]\n",
      "###################################################\n",
      "gradient descent step: 10\n",
      "indices for batch: [131, 77]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 0. 1.]\n",
      "x: [0.90694538 0.63793103 0.80495356 0.76666667 0.68518519 0.43814433\n",
      " 0.18110236 0.65151515 0.40782123 0.82153846 0.49707602 0.39\n",
      " 0.41369048]\n",
      "class: 0\n",
      "softmax numerator: 0.20691321534208243\n",
      "softmax denominator: 5.6416111573066665\n",
      "y hat for class: 0.03667626314055714\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.03326337 0.02339693 0.02952269 0.02811847 0.02513003 0.0160695\n",
      " 0.00664216 0.02389514 0.01495736 0.03013096 0.01823089 0.01430374\n",
      " 0.01517262]\n",
      "new gradient for class: [0.03326337 0.02339693 0.02952269 0.02811847 0.02513003 0.0160695\n",
      " 0.00664216 0.02389514 0.01495736 0.03013096 0.01823089 0.01430374\n",
      " 0.01517262]\n",
      "class: 1\n",
      "softmax numerator: 1.120151003902117\n",
      "softmax denominator: 5.6416111573066665\n",
      "y hat for class: 0.19855161454212714\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.18007547 0.12666224 0.15982483 0.1522229  0.13604462 0.08699426\n",
      " 0.03595817 0.12935939 0.08097356 0.16311779 0.09869525 0.07743513\n",
      " 0.08213891]\n",
      "new gradient for class: [0.18007547 0.12666224 0.15982483 0.1522229  0.13604462 0.08699426\n",
      " 0.03595817 0.12935939 0.08097356 0.16311779 0.09869525 0.07743513\n",
      " 0.08213891]\n",
      "class: 2\n",
      "softmax numerator: 4.314546938062467\n",
      "softmax denominator: 5.6416111573066665\n",
      "y hat for class: 0.7647721223173157\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.21333884 -0.15005916 -0.18934752 -0.18034137 -0.16117466 -0.10306376\n",
      " -0.04260032 -0.15325453 -0.09593092 -0.19324875 -0.11692614 -0.09173887\n",
      " -0.09731153]\n",
      "new gradient for class: [-0.21333884 -0.15005916 -0.18934752 -0.18034137 -0.16117466 -0.10306376\n",
      " -0.04260032 -0.15325453 -0.09593092 -0.19324875 -0.11692614 -0.09173887\n",
      " -0.09731153]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.79298719 0.46206897 0.90402477 0.66666667 0.63580247 0.45103093\n",
      " 0.3996063  0.90909091 0.29329609 0.29230769 0.71929825 0.625\n",
      " 0.36130952]\n",
      "class: 0\n",
      "softmax numerator: 0.18507734101182588\n",
      "softmax denominator: 5.176331052011352\n",
      "y hat for class: 0.03575454103537503\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.02835289 0.01652106 0.03232299 0.02383636 0.02273283 0.0161264\n",
      " 0.01428774 0.03250413 0.01048667 0.01045133 0.02571818 0.02234659\n",
      " 0.01291846]\n",
      "new gradient for class: [0.06161626 0.03991799 0.06184568 0.05195483 0.04786286 0.0321959\n",
      " 0.0209299  0.05639927 0.02544403 0.04058229 0.04394907 0.03665033\n",
      " 0.02809108]\n",
      "class: 1\n",
      "softmax numerator: 1.5873279876344362\n",
      "softmax denominator: 5.176331052011352\n",
      "y hat for class: 0.3066511727486311\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.54981674 -0.32037498 -0.62680451 -0.46223255 -0.4408329  -0.31272176\n",
      " -0.27706656 -0.63031712 -0.2033565  -0.2026712  -0.4987246  -0.43334302\n",
      " -0.25051353]\n",
      "new gradient for class: [-0.36974127 -0.19371274 -0.46697968 -0.31000965 -0.30478827 -0.2257275\n",
      " -0.24110839 -0.50095773 -0.12238294 -0.03955341 -0.40002935 -0.35590789\n",
      " -0.16837462]\n",
      "class: 2\n",
      "softmax numerator: 3.4039257233650897\n",
      "softmax denominator: 5.176331052011352\n",
      "y hat for class: 0.6575942862159939\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.52146384 0.30385391 0.59448152 0.43839619 0.41810007 0.29659536\n",
      " 0.26277882 0.59781299 0.19286983 0.19221987 0.47300642 0.41099643\n",
      " 0.23759508]\n",
      "new gradient for class: [ 0.30812501  0.15379475  0.405134    0.25805482  0.25692541  0.1935316\n",
      "  0.22017849  0.44455846  0.09693891 -0.00102888  0.35608028  0.31925756\n",
      "  0.14028355]\n",
      "w for class:  [-0.36358166 -0.24426087 -0.43160481 -0.68128723 -0.24332655  0.15833334\n",
      "  0.34909992 -0.68408882  0.16224244  0.06565315 -0.16987744 -0.12800815\n",
      "  0.23931726]\n",
      "w for class:  [ 0.22353006 -0.10373112  0.25424753  0.55385983  0.12969272  0.04218835\n",
      "  0.28782382  0.35884413  0.22662913 -0.32464076  0.35584038  0.49022956\n",
      " -0.28872591]\n",
      "w for class:  [ 0.1400516   0.34799198  0.17735728  0.1274274   0.11363383 -0.20052169\n",
      " -0.63692374  0.32524469 -0.38887157  0.25898761 -0.18596294 -0.3622214\n",
      "  0.04940865]\n",
      "###################################################\n",
      "gradient descent step: 11\n",
      "indices for batch: [39, 10]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.80647336 0.18793103 0.7120743  0.7        0.62345679 0.87113402\n",
      " 0.42125984 0.1969697  0.46089385 0.24692308 0.57894737 0.7825\n",
      " 0.52738095]\n",
      "class: 0\n",
      "softmax numerator: 0.3308357720813762\n",
      "softmax denominator: 5.489638504485789\n",
      "y hat for class: 0.06026549322164618\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.04860252 0.01132576 0.04291351 0.04218585 0.03757293 0.05249932\n",
      " 0.02538743 0.01187048 0.027776   0.01488094 0.03489055 0.04715775\n",
      " 0.03178287]\n",
      "new gradient for class: [0.04860252 0.01132576 0.04291351 0.04218585 0.03757293 0.05249932\n",
      " 0.02538743 0.01187048 0.027776   0.01488094 0.03489055 0.04715775\n",
      " 0.03178287]\n",
      "class: 1\n",
      "softmax numerator: 4.484831413282722\n",
      "softmax denominator: 5.489638504485789\n",
      "y hat for class: 0.8169629766364394\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.14761448 -0.03439834 -0.13033596 -0.12812592 -0.11411568 -0.15944978\n",
      " -0.07710615 -0.03605275 -0.08436064 -0.04519606 -0.1059688  -0.14322647\n",
      " -0.09653024]\n",
      "new gradient for class: [-0.14761448 -0.03439834 -0.13033596 -0.12812592 -0.11411568 -0.15944978\n",
      " -0.07710615 -0.03605275 -0.08436064 -0.04519606 -0.1059688  -0.14322647\n",
      " -0.09653024]\n",
      "class: 2\n",
      "softmax numerator: 0.6739713191216917\n",
      "softmax denominator: 5.489638504485789\n",
      "y hat for class: 0.12277153014191454\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.09901197 0.02307258 0.08742245 0.08594007 0.07654274 0.10695046\n",
      " 0.05171872 0.02418227 0.05658464 0.03031512 0.07107825 0.09606872\n",
      " 0.06474737]\n",
      "new gradient for class: [0.09901197 0.02307258 0.08742245 0.08594007 0.07654274 0.10695046\n",
      " 0.05171872 0.02418227 0.05658464 0.03031512 0.07107825 0.09606872\n",
      " 0.06474737]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [1. 0. 0.]\n",
      "x: [0.95819285 0.69655172 0.75541796 0.63       0.68518519 0.73453608\n",
      " 0.52165354 0.45454545 0.34916201 0.40307692 0.50877193 0.8325\n",
      " 0.64285714]\n",
      "class: 0\n",
      "softmax numerator: 0.24436951820534963\n",
      "softmax denominator: 5.527204007240552\n",
      "y hat for class: 0.04421214014992559\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.9158291  -0.66575568 -0.72201931 -0.60214635 -0.65489168 -0.70206067\n",
      " -0.49859012 -0.43444903 -0.33372481 -0.38525603 -0.48627803 -0.79569339\n",
      " -0.61443505]\n",
      "new gradient for class: [-0.86722658 -0.65442993 -0.6791058  -0.55996051 -0.61731875 -0.64956135\n",
      " -0.47320269 -0.42257855 -0.30594882 -0.37037509 -0.45138749 -0.74853564\n",
      " -0.58265218]\n",
      "class: 1\n",
      "softmax numerator: 4.339922956429316\n",
      "softmax denominator: 5.527204007240552\n",
      "y hat for class: 0.785193191846019\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.7523665  0.54692767 0.59314904 0.49467171 0.53800274 0.57675273\n",
      " 0.40959881 0.356906   0.27415963 0.31649326 0.39948426 0.65367333\n",
      " 0.50476705]\n",
      "new gradient for class: [0.60475202 0.51252933 0.46281308 0.36654579 0.42388707 0.41730295\n",
      " 0.33249266 0.32085325 0.18979899 0.27129719 0.29351545 0.51044686\n",
      " 0.40823681]\n",
      "class: 2\n",
      "softmax numerator: 0.9429115326058867\n",
      "softmax denominator: 5.527204007240552\n",
      "y hat for class: 0.1705946680040554\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.16346259 0.11882801 0.12887028 0.10747464 0.11688894 0.12530794\n",
      " 0.08899131 0.07754303 0.05956518 0.06876277 0.08679378 0.14202006\n",
      " 0.109668  ]\n",
      "new gradient for class: [0.26247456 0.14190059 0.21629273 0.19341471 0.19343168 0.2322584\n",
      " 0.14071003 0.1017253  0.11614982 0.0990779  0.15787203 0.23808878\n",
      " 0.17441537]\n",
      "w for class:  [ 0.07003163  0.0829541  -0.09205191 -0.40130697  0.06533282  0.48311402\n",
      "  0.58570127 -0.47279954  0.31521685  0.25084069  0.0558163   0.24625967\n",
      "  0.53064335]\n",
      "w for class:  [-0.07884595 -0.35999579  0.02284099  0.37058693 -0.08225081 -0.16646313\n",
      "  0.12157748  0.1984175   0.13172963 -0.46028936  0.20908265  0.23500612\n",
      " -0.49284432]\n",
      "w for class:  [ 0.00881432  0.27704169  0.06921092  0.03072005  0.01691799 -0.31665089\n",
      " -0.70727875  0.27438204 -0.44694648  0.20944866 -0.26489895 -0.48126579\n",
      " -0.03779903]\n",
      "###################################################\n",
      "gradient descent step: 12\n",
      "indices for batch: [46, 85]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.85772084 0.31206897 0.68111455 0.62666667 0.5308642  0.56701031\n",
      " 0.4980315  0.39393939 0.49441341 0.3        0.67836257 0.785\n",
      " 0.425     ]\n",
      "class: 0\n",
      "softmax numerator: 2.395588179415174\n",
      "softmax denominator: 3.887589561149534\n",
      "y hat for class: 0.6162142740981161\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.52853982 0.19230135 0.41971251 0.38616095 0.3271261  0.34939985\n",
      " 0.30689412 0.24275108 0.3046646  0.18486428 0.4180167  0.48372821\n",
      " 0.26189107]\n",
      "new gradient for class: [0.52853982 0.19230135 0.41971251 0.38616095 0.3271261  0.34939985\n",
      " 0.30689412 0.24275108 0.3046646  0.18486428 0.4180167  0.48372821\n",
      " 0.26189107]\n",
      "class: 1\n",
      "softmax numerator: 1.118939040253657\n",
      "softmax denominator: 3.887589561149534\n",
      "y hat for class: 0.2878233472575727\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.61084875 -0.22224823 -0.48507388 -0.44629737 -0.37806909 -0.4038115\n",
      " -0.3546864  -0.28055444 -0.35210969 -0.213653   -0.48311399 -0.55905867\n",
      " -0.30267508]\n",
      "new gradient for class: [-0.61084875 -0.22224823 -0.48507388 -0.44629737 -0.37806909 -0.4038115\n",
      " -0.3546864  -0.28055444 -0.35210969 -0.213653   -0.48311399 -0.55905867\n",
      " -0.30267508]\n",
      "class: 2\n",
      "softmax numerator: 0.3730623414807033\n",
      "softmax denominator: 3.887589561149534\n",
      "y hat for class: 0.09596237864431122\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.08230893 0.02994688 0.06536137 0.06013642 0.05094299 0.05441166\n",
      " 0.04779229 0.03780336 0.04744509 0.02878871 0.06509729 0.07533047\n",
      " 0.04078401]\n",
      "new gradient for class: [0.08230893 0.02994688 0.06536137 0.06013642 0.05094299 0.05441166\n",
      " 0.04779229 0.03780336 0.04744509 0.02878871 0.06509729 0.07533047\n",
      " 0.04078401]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.77208361 0.4137931  0.74922601 0.66666667 0.59259259 0.74742268\n",
      " 0.5492126  0.48484848 0.51117318 0.25       0.46783626 0.8475\n",
      " 0.37202381]\n",
      "class: 0\n",
      "softmax numerator: 2.4601084380835654\n",
      "softmax denominator: 3.9405645050473286\n",
      "y hat for class: 0.6243035572523937\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.48201455 0.25833251 0.46774446 0.41620237 0.36995766 0.46661864\n",
      " 0.34287538 0.30269263 0.31912724 0.15607589 0.29207184 0.52909726\n",
      " 0.23225579]\n",
      "new gradient for class: [1.01055437 0.45063386 0.88745697 0.80236332 0.69708376 0.81601848\n",
      " 0.6497695  0.54544371 0.62379184 0.34094017 0.71008854 1.01282547\n",
      " 0.49414685]\n",
      "class: 1\n",
      "softmax numerator: 1.1163282789394637\n",
      "softmax denominator: 3.9405645050473286\n",
      "y hat for class: 0.2832914617968057\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.55335892 -0.29656905 -0.53697668 -0.47780569 -0.42471617 -0.53568422\n",
      " -0.39362536 -0.34749505 -0.36636219 -0.17917713 -0.33530224 -0.60741049\n",
      " -0.26663264]\n",
      "new gradient for class: [-1.16420767 -0.51881728 -1.02205056 -0.92410306 -0.80278526 -0.93949572\n",
      " -0.74831176 -0.62804949 -0.71847187 -0.39283013 -0.81841623 -1.16646916\n",
      " -0.56930772]\n",
      "class: 2\n",
      "softmax numerator: 0.3641277880242995\n",
      "softmax denominator: 3.9405645050473286\n",
      "y hat for class: 0.09240498095080063\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.07134437 0.03823654 0.06923221 0.06160332 0.05475851 0.06906558\n",
      " 0.05074998 0.04480242 0.04723495 0.02310125 0.0432304  0.07831322\n",
      " 0.03437685]\n",
      "new gradient for class: [0.1536533  0.06818342 0.13459359 0.12173974 0.1057015  0.12347724\n",
      " 0.09854227 0.08260578 0.09468004 0.05188996 0.10832769 0.15364369\n",
      " 0.07516086]\n",
      "w for class:  [-0.43524555 -0.14236283 -0.5357804  -0.80248863 -0.28320906  0.07510478\n",
      "  0.26081652 -0.7455214   0.00332093  0.08037061 -0.29922797 -0.26015306\n",
      "  0.28356993]\n",
      "w for class:  [ 0.50325788 -0.10058715  0.53386627  0.83263846  0.31914182  0.30328473\n",
      "  0.49573337  0.51244225  0.49096557 -0.26387429  0.61829077  0.8182407\n",
      " -0.20819046]\n",
      "w for class:  [-0.06801233  0.24294998  0.00191413 -0.03014983 -0.03593276 -0.37838951\n",
      " -0.75654989  0.23307915 -0.4942865   0.18350368 -0.3190628  -0.55808764\n",
      " -0.07537947]\n",
      "###################################################\n",
      "gradient descent step: 13\n",
      "indices for batch: [26, 82]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.85232637 0.23448276 0.625387   0.56       0.61728395 0.52061856\n",
      " 0.27755906 0.8030303  0.17318436 0.44230769 0.57309942 0.3975\n",
      " 0.26785714]\n",
      "class: 0\n",
      "softmax numerator: 0.13350201792468186\n",
      "softmax denominator: 15.54030335620094\n",
      "y hat for class: 0.008590695745421949\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.00732208 0.00201437 0.00537251 0.00481079 0.0053029  0.00447248\n",
      " 0.00238443 0.00689859 0.00148777 0.00379973 0.00492332 0.0034148\n",
      " 0.00230108]\n",
      "new gradient for class: [0.00732208 0.00201437 0.00537251 0.00481079 0.0053029  0.00447248\n",
      " 0.00238443 0.00689859 0.00148777 0.00379973 0.00492332 0.0034148\n",
      " 0.00230108]\n",
      "class: 1\n",
      "softmax numerator: 14.904224121249031\n",
      "softmax denominator: 15.54030335620094\n",
      "y hat for class: 0.9590690593116319\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.03488652 -0.0095976  -0.02559768 -0.02292133 -0.02526601 -0.02130941\n",
      " -0.01136075 -0.03286879 -0.0070886  -0.01810407 -0.0234575  -0.01627005\n",
      " -0.01096364]\n",
      "new gradient for class: [-0.03488652 -0.0095976  -0.02559768 -0.02292133 -0.02526601 -0.02130941\n",
      " -0.01136075 -0.03286879 -0.0070886  -0.01810407 -0.0234575  -0.01627005\n",
      " -0.01096364]\n",
      "class: 2\n",
      "softmax numerator: 0.5025772170272267\n",
      "softmax denominator: 15.54030335620094\n",
      "y hat for class: 0.032340244942946156\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.02756444 0.00758323 0.02022517 0.01811054 0.01996311 0.01683693\n",
      " 0.00897633 0.0259702  0.00560082 0.01430434 0.01853418 0.01285525\n",
      " 0.00866257]\n",
      "new gradient for class: [0.02756444 0.00758323 0.02022517 0.01811054 0.01996311 0.01683693\n",
      " 0.00897633 0.0259702  0.00560082 0.01430434 0.01853418 0.01285525\n",
      " 0.00866257]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.83749157 0.27758621 0.67801858 0.75       0.66666667 0.51546392\n",
      " 0.41141732 0.51515152 0.44972067 0.15846154 0.61988304 0.74\n",
      " 0.20535714]\n",
      "class: 0\n",
      "softmax numerator: 0.12226681473101968\n",
      "softmax denominator: 28.6880121540992\n",
      "y hat for class: 0.004261947954924758\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.00356935 0.00118306 0.00288968 0.00319646 0.0028413  0.00219688\n",
      " 0.00175344 0.00219555 0.00191669 0.00067535 0.00264191 0.00315384\n",
      " 0.00087522]\n",
      "new gradient for class: [0.01089142 0.00319743 0.00826219 0.00800725 0.0081442  0.00666936\n",
      " 0.00413786 0.00909414 0.00340446 0.00447509 0.00756523 0.00656864\n",
      " 0.0031763 ]\n",
      "class: 1\n",
      "softmax numerator: 28.276500417563888\n",
      "softmax denominator: 28.6880121540992\n",
      "y hat for class: 0.9856556203920698\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.0120133  -0.0039818  -0.00972576 -0.01075828 -0.00956292 -0.00739401\n",
      " -0.00590153 -0.00738953 -0.00645096 -0.00227303 -0.00889184 -0.01061484\n",
      " -0.00294572]\n",
      "new gradient for class: [-0.04689982 -0.0135794  -0.03532343 -0.03367961 -0.03482893 -0.02870342\n",
      " -0.01726228 -0.04025831 -0.01353956 -0.0203771  -0.03234934 -0.02688489\n",
      " -0.01390937]\n",
      "class: 2\n",
      "softmax numerator: 0.2892449218042927\n",
      "softmax denominator: 28.6880121540992\n",
      "y hat for class: 0.010082431653005375\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.00844395 0.00279874 0.00683608 0.00756182 0.00672162 0.00519713\n",
      " 0.00414809 0.00519398 0.00453428 0.00159768 0.00624993 0.007461\n",
      " 0.0020705 ]\n",
      "new gradient for class: [0.03600839 0.01038197 0.02706124 0.02567236 0.02668474 0.02203406\n",
      " 0.01312441 0.03116418 0.0101351  0.01590202 0.0247841  0.02031625\n",
      " 0.01073306]\n",
      "w for class:  [-0.44069126 -0.14396155 -0.53991149 -0.80649226 -0.28728115  0.0717701\n",
      "  0.25874759 -0.75006847  0.0016187   0.07813307 -0.30301059 -0.26343739\n",
      "  0.28198178]\n",
      "w for class:  [ 0.52670779 -0.09379744  0.55152799  0.84947826  0.33655628  0.31763644\n",
      "  0.50436451  0.5325714   0.49773535 -0.25368574  0.63446544  0.83168315\n",
      " -0.20123578]\n",
      "w for class:  [-0.08601653  0.23775899 -0.0116165  -0.04298601 -0.04927513 -0.38940654\n",
      " -0.76311209  0.21749707 -0.49935405  0.17555268 -0.33145485 -0.56824576\n",
      " -0.080746  ]\n",
      "###################################################\n",
      "gradient descent step: 14\n",
      "indices for batch: [28, 34]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.83412003 0.19482759 0.66873065 0.63333333 0.53703704 0.90206186\n",
      " 0.61023622 0.28787879 0.52234637 0.34230769 0.71345029 0.7175\n",
      " 0.25      ]\n",
      "class: 0\n",
      "softmax numerator: 0.17868069621406749\n",
      "softmax denominator: 31.190359744160244\n",
      "y hat for class: 0.005728715464640378\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.00477844 0.00111611 0.00383097 0.00362819 0.00307653 0.00516766\n",
      " 0.00349587 0.00164918 0.00299237 0.00196098 0.00408715 0.00411035\n",
      " 0.00143218]\n",
      "new gradient for class: [0.00477844 0.00111611 0.00383097 0.00362819 0.00307653 0.00516766\n",
      " 0.00349587 0.00164918 0.00299237 0.00196098 0.00408715 0.00411035\n",
      " 0.00143218]\n",
      "class: 1\n",
      "softmax numerator: 30.83014974926573\n",
      "softmax denominator: 31.190359744160244\n",
      "y hat for class: 0.9884512394903698\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.00963305 -0.00225002 -0.00772301 -0.00731421 -0.00620211 -0.0104177\n",
      " -0.00704747 -0.00332464 -0.00603245 -0.00395323 -0.00823947 -0.00828624\n",
      " -0.00288719]\n",
      "new gradient for class: [-0.00963305 -0.00225002 -0.00772301 -0.00731421 -0.00620211 -0.0104177\n",
      " -0.00704747 -0.00332464 -0.00603245 -0.00395323 -0.00823947 -0.00828624\n",
      " -0.00288719]\n",
      "class: 2\n",
      "softmax numerator: 0.18152929868044845\n",
      "softmax denominator: 31.190359744160244\n",
      "y hat for class: 0.005820045044989778\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.00485462 0.00113391 0.00389204 0.00368603 0.00312558 0.00525004\n",
      " 0.0035516  0.00167547 0.00304008 0.00199225 0.00415231 0.00417588\n",
      " 0.00145501]\n",
      "new gradient for class: [0.00485462 0.00113391 0.00389204 0.00368603 0.00312558 0.00525004\n",
      " 0.0035516  0.00167547 0.00304008 0.00199225 0.00415231 0.00417588\n",
      " 0.00145501]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.82333109 0.20517241 0.54179567 0.56       0.93209877 0.47680412\n",
      " 0.2519685  0.21212121 0.69832402 0.21923077 0.74853801 0.7675\n",
      " 0.42738095]\n",
      "class: 0\n",
      "softmax numerator: 0.1730417484860436\n",
      "softmax denominator: 25.267471691339487\n",
      "y hat for class: 0.006848399816169745\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.0056385  0.0014051  0.00371043 0.0038351  0.00638339 0.00326535\n",
      " 0.00172558 0.00145269 0.0047824  0.00150138 0.00512629 0.00525615\n",
      " 0.00292688]\n",
      "new gradient for class: [0.01041694 0.00252121 0.0075414  0.00746329 0.00945992 0.008433\n",
      " 0.00522145 0.00310187 0.00777478 0.00346236 0.00921344 0.0093665\n",
      " 0.00435905]\n",
      "class: 1\n",
      "softmax numerator: 24.86198867212259\n",
      "softmax denominator: 25.267471691339487\n",
      "y hat for class: 0.9839523706934288\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.01321251 -0.00329253 -0.00869454 -0.00898667 -0.01495798 -0.00765158\n",
      " -0.0040435  -0.00340404 -0.01120645 -0.00351813 -0.01201226 -0.01231656\n",
      " -0.00685845]\n",
      "new gradient for class: [-0.02284556 -0.00554255 -0.01641755 -0.01630089 -0.02116009 -0.01806927\n",
      " -0.01109097 -0.00672869 -0.0172389  -0.00747136 -0.02025173 -0.02060279\n",
      " -0.00974564]\n",
      "class: 2\n",
      "softmax numerator: 0.23244127073085624\n",
      "softmax denominator: 25.267471691339487\n",
      "y hat for class: 0.009199229490401539\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.00757401 0.00188743 0.0049841  0.00515157 0.00857459 0.00438623\n",
      " 0.00231792 0.00195135 0.00642404 0.00201675 0.00688597 0.00706041\n",
      " 0.00393158]\n",
      "new gradient for class: [0.01242863 0.00302133 0.00887615 0.0088376  0.01170017 0.00963627\n",
      " 0.00586952 0.00362682 0.00946412 0.004009   0.01103829 0.01123629\n",
      " 0.00538659]\n",
      "w for class:  [-0.44589973 -0.14522215 -0.54368219 -0.8102239  -0.29201111  0.0675536\n",
      "  0.25613686 -0.7516194  -0.00226869  0.07640188 -0.30761731 -0.26812064\n",
      "  0.27980225]\n",
      "w for class:  [ 0.53813058 -0.09102617  0.55973676  0.85762871  0.34713633  0.32667107\n",
      "  0.50990999  0.53593575  0.5063548  -0.24995006  0.6445913   0.84198454\n",
      " -0.19636296]\n",
      "w for class:  [-0.09223085  0.23624832 -0.01605457 -0.04740481 -0.05512521 -0.39422467\n",
      " -0.76604685  0.21568366 -0.50408611  0.17354818 -0.33697399 -0.57386391\n",
      " -0.08343929]\n",
      "###################################################\n",
      "gradient descent step: 15\n",
      "indices for batch: [20, 96]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [1. 0. 0.]\n",
      "x: [0.91436278 0.29827586 0.76160991 0.68333333 0.71604938 0.7628866\n",
      " 0.54724409 0.3030303  0.68435754 0.48076923 0.57309942 0.7575\n",
      " 0.66666667]\n",
      "class: 0\n",
      "softmax numerator: 0.16176791331915283\n",
      "softmax denominator: 34.100344177662926\n",
      "y hat for class: 0.004743879195950087\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.91002515 -0.29686088 -0.75799692 -0.68009168 -0.71265253 -0.75926756\n",
      " -0.54464803 -0.30159276 -0.68111103 -0.47848852 -0.5703807  -0.75390651\n",
      " -0.66350408]\n",
      "new gradient for class: [-0.91002515 -0.29686088 -0.75799692 -0.68009168 -0.71265253 -0.75926756\n",
      " -0.54464803 -0.30159276 -0.68111103 -0.47848852 -0.5703807  -0.75390651\n",
      " -0.66350408]\n",
      "class: 1\n",
      "softmax numerator: 33.75544445517272\n",
      "softmax denominator: 34.100344177662926\n",
      "y hat for class: 0.9898857407217571\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.90511468 0.29525902 0.75390679 0.67642192 0.70880707 0.75517057\n",
      " 0.54170913 0.29996538 0.67743577 0.47590661 0.56730294 0.74983845\n",
      " 0.65992383]\n",
      "new gradient for class: [0.90511468 0.29525902 0.75390679 0.67642192 0.70880707 0.75517057\n",
      " 0.54170913 0.29996538 0.67743577 0.47590661 0.56730294 0.74983845\n",
      " 0.65992383]\n",
      "class: 2\n",
      "softmax numerator: 0.18313180917105304\n",
      "softmax denominator: 34.100344177662926\n",
      "y hat for class: 0.005370380082292883\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.00491048 0.00160185 0.00409013 0.00366976 0.00384546 0.00409699\n",
      " 0.00293891 0.00162739 0.00367526 0.00258191 0.00307776 0.00406806\n",
      " 0.00358025]\n",
      "new gradient for class: [0.00491048 0.00160185 0.00409013 0.00366976 0.00384546 0.00409699\n",
      " 0.00293891 0.00162739 0.00367526 0.00258191 0.00307776 0.00406806\n",
      " 0.00358025]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 0. 1.]\n",
      "x: [0.86850978 0.51551724 0.74303406 0.66666667 0.64197531 0.33505155\n",
      " 0.24015748 0.36363636 0.23184358 0.41538462 0.43274854 0.355\n",
      " 0.31547619]\n",
      "class: 0\n",
      "softmax numerator: 0.15081339002707128\n",
      "softmax denominator: 13.94244408141294\n",
      "y hat for class: 0.010816854573447764\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.00939454 0.00557628 0.00803729 0.00721124 0.00694415 0.0036242\n",
      " 0.00259775 0.0039334  0.00250782 0.00449315 0.00468098 0.00383998\n",
      " 0.00341246]\n",
      "new gradient for class: [-0.90063061 -0.2912846  -0.74995963 -0.67288045 -0.70570838 -0.75564335\n",
      " -0.54205029 -0.29765936 -0.67860321 -0.47399536 -0.56569972 -0.75006653\n",
      " -0.66009162]\n",
      "class: 1\n",
      "softmax numerator: 13.29281138365808\n",
      "softmax denominator: 13.94244408141294\n",
      "y hat for class: 0.9534061105813647\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.82804253 0.49149729 0.70841321 0.63560407 0.61206318 0.31944019\n",
      " 0.22896761 0.34669313 0.22104108 0.39603023 0.4125851  0.33845917\n",
      " 0.30077693]\n",
      "new gradient for class: [1.7331572  0.78675631 1.46232    1.312026   1.32087026 1.07461076\n",
      " 0.77067674 0.64665851 0.89847685 0.87193684 0.97988804 1.08829762\n",
      " 0.96070075]\n",
      "class: 2\n",
      "softmax numerator: 0.4988193077277888\n",
      "softmax denominator: 13.94244408141294\n",
      "y hat for class: 0.03577703484518749\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.83743707 -0.49707356 -0.7164505  -0.64281531 -0.61900734 -0.3230644\n",
      " -0.23156536 -0.35062653 -0.2235489  -0.40052339 -0.41726608 -0.34229915\n",
      " -0.30418939]\n",
      "new gradient for class: [-0.8325266  -0.49547171 -0.71236037 -0.63914555 -0.61516188 -0.3189674\n",
      " -0.22862645 -0.34899914 -0.21987364 -0.39794147 -0.41418832 -0.33823109\n",
      " -0.30060913]\n",
      "w for class:  [ 4.41557371e-03  4.20148589e-04 -1.68702376e-01 -4.73783680e-01\n",
      "  6.08430759e-02  4.45375274e-01  5.27162006e-01 -6.02789721e-01\n",
      "  3.37032917e-01  3.13399566e-01 -2.47674471e-02  1.06912628e-01\n",
      "  6.09848058e-01]\n",
      "w for class:  [-0.32844803 -0.48440433 -0.17142324  0.20161571 -0.3132988  -0.2106343\n",
      "  0.12457162  0.21260649  0.05711637 -0.68591848  0.15464728  0.29783574\n",
      " -0.67671333]\n",
      "w for class:  [ 0.32403245  0.48398418  0.34012562  0.27216797  0.25245572 -0.23474097\n",
      " -0.65173363  0.39018323 -0.39414929  0.37251891 -0.12987983 -0.40474836\n",
      "  0.06686528]\n",
      "###################################################\n",
      "gradient descent step: 16\n",
      "indices for batch: [75, 71]\n",
      "x: [0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.77275792 0.64482759 0.56346749 0.65       0.66049383 0.81958763\n",
      " 0.50787402 0.36363636 1.         0.22307692 0.43859649 0.7025\n",
      " 0.33452381]\n",
      "class: 0\n",
      "softmax numerator: 2.0742685562181227\n",
      "softmax denominator: 3.6066413713092347\n",
      "y hat for class: 0.57512470541676\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.44443217 0.37085628 0.32406408 0.37383106 0.37986632 0.47136509\n",
      " 0.29209089 0.20913626 0.57512471 0.12829705 0.25224768 0.40402511\n",
      " 0.19239291]\n",
      "new gradient for class: [0.44443217 0.37085628 0.32406408 0.37383106 0.37986632 0.47136509\n",
      " 0.29209089 0.20913626 0.57512471 0.12829705 0.25224768 0.40402511\n",
      " 0.19239291]\n",
      "class: 1\n",
      "softmax numerator: 0.4422358567164972\n",
      "softmax denominator: 3.6066413713092347\n",
      "y hat for class: 0.12261708642131021\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.6780046  -0.56576071 -0.49437675 -0.57029889 -0.579506   -0.71909218\n",
      " -0.44559998 -0.31904833 -0.87738291 -0.19572388 -0.38481707 -0.6163615\n",
      " -0.29350547]\n",
      "new gradient for class: [-0.6780046  -0.56576071 -0.49437675 -0.57029889 -0.579506   -0.71909218\n",
      " -0.44559998 -0.31904833 -0.87738291 -0.19572388 -0.38481707 -0.6163615\n",
      " -0.29350547]\n",
      "class: 2\n",
      "softmax numerator: 1.0901369583746152\n",
      "softmax denominator: 3.6066413713092347\n",
      "y hat for class: 0.30225820816193\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.23357243 0.19490443 0.17031267 0.19646784 0.19963968 0.24772709\n",
      " 0.15350909 0.10991208 0.30225821 0.06742683 0.13256939 0.21233639\n",
      " 0.10111257]\n",
      "new gradient for class: [0.23357243 0.19490443 0.17031267 0.19646784 0.19963968 0.24772709\n",
      " 0.15350909 0.10991208 0.30225821 0.06742683 0.13256939 0.21233639\n",
      " 0.10111257]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.82602832 0.29827586 0.65634675 0.63333333 0.49382716 0.42525773\n",
      " 0.3996063  0.56060606 0.45530726 0.26153846 0.58479532 0.7925\n",
      " 0.30357143]\n",
      "class: 0\n",
      "softmax numerator: 1.1923829416868397\n",
      "softmax denominator: 3.1881241622050553\n",
      "y hat for class: 0.3740076863449798\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.30894094 0.11155747 0.24547873 0.23687153 0.18469515 0.15904966\n",
      " 0.14945583 0.20967098 0.17028842 0.09781739 0.21871795 0.29640109\n",
      " 0.11353805]\n",
      "new gradient for class: [0.75337311 0.48241374 0.5695428  0.61070259 0.56456147 0.63041475\n",
      " 0.44154672 0.41880723 0.74541312 0.22611444 0.47096562 0.7004262\n",
      " 0.30593096]\n",
      "class: 1\n",
      "softmax numerator: 0.6015260543169441\n",
      "softmax denominator: 3.1881241622050553\n",
      "y hat for class: 0.18867711033591006\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.67017568 -0.24199803 -0.53250914 -0.51383783 -0.40065328 -0.34502133\n",
      " -0.32420974 -0.45483253 -0.3694012  -0.21219214 -0.47445783 -0.64297339\n",
      " -0.24629445]\n",
      "new gradient for class: [-1.34818028 -0.80775874 -1.02688589 -1.08413672 -0.98015928 -1.06411351\n",
      " -0.76980972 -0.77388086 -1.24678412 -0.40791602 -0.8592749  -1.25933489\n",
      " -0.53979992]\n",
      "class: 2\n",
      "softmax numerator: 1.3942151662012716\n",
      "softmax denominator: 3.1881241622050553\n",
      "y hat for class: 0.43731520331911017\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.36123474 0.13044057 0.28703041 0.2769663  0.21595813 0.18597167\n",
      " 0.17475391 0.24516155 0.19911279 0.11437475 0.25573988 0.3465723\n",
      " 0.1327564 ]\n",
      "new gradient for class: [0.59480717 0.325345   0.45734309 0.47343413 0.41559781 0.43369876\n",
      " 0.328263   0.35507363 0.501371   0.18180158 0.38830927 0.55890869\n",
      " 0.23386897]\n",
      "w for class:  [-0.37227098 -0.24078672 -0.45347378 -0.77913498 -0.22143766  0.1301679\n",
      "  0.30638865 -0.81219334 -0.03567364  0.20034234 -0.26025026 -0.24330047\n",
      "  0.45688258]\n",
      "w for class:  [ 0.34564211 -0.08052496  0.34201971  0.74368407  0.17678084  0.32142245\n",
      "  0.50947648  0.59954692  0.68050843 -0.48196047  0.58428473  0.92750318\n",
      " -0.40681337]\n",
      "w for class:  [ 0.02662887  0.32131168  0.11145407  0.0354509   0.04465682 -0.45159035\n",
      " -0.81586513  0.21264641 -0.64483479  0.28161812 -0.32403447 -0.68420271\n",
      " -0.05006921]\n",
      "###################################################\n",
      "gradient descent step: 17\n",
      "indices for batch: [1, 88]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [1. 0. 0.]\n",
      "x: [0.89548213 0.28275862 0.87925697 0.51666667 0.67901235 0.67010309\n",
      " 0.52755906 0.51515152 0.37988827 0.35384615 0.6374269  0.695\n",
      " 0.52380952]\n",
      "class: 0\n",
      "softmax numerator: 0.20993787405176514\n",
      "softmax denominator: 16.57240698133461\n",
      "y hat for class: 0.012667916874610717\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.88413824 -0.27917666 -0.86811861 -0.51012158 -0.67041067 -0.66161428\n",
      " -0.52087598 -0.50862562 -0.37507588 -0.34936366 -0.62935203 -0.6861958\n",
      " -0.51717395]\n",
      "new gradient for class: [-0.88413824 -0.27917666 -0.86811861 -0.51012158 -0.67041067 -0.66161428\n",
      " -0.52087598 -0.50862562 -0.37507588 -0.34936366 -0.62935203 -0.6861958\n",
      " -0.51717395]\n",
      "class: 1\n",
      "softmax numerator: 16.065984701315447\n",
      "softmax denominator: 16.57240698133461\n",
      "y hat for class: 0.969441839040669\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.86811784 0.27411804 0.85238849 0.50087828 0.65826298 0.64962597\n",
      " 0.51143782 0.49940943 0.36827958 0.34303327 0.61794831 0.67376208\n",
      " 0.50780287]\n",
      "new gradient for class: [0.86811784 0.27411804 0.85238849 0.50087828 0.65826298 0.64962597\n",
      " 0.51143782 0.49940943 0.36827958 0.34303327 0.61794831 0.67376208\n",
      " 0.50780287]\n",
      "class: 2\n",
      "softmax numerator: 0.2964844059673976\n",
      "softmax denominator: 16.57240698133461\n",
      "y hat for class: 0.017890244084720223\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.01602039 0.00505862 0.01573012 0.00924329 0.0121477  0.01198831\n",
      " 0.00943816 0.00921619 0.00679629 0.00633039 0.01140372 0.01243372\n",
      " 0.00937108]\n",
      "new gradient for class: [0.01602039 0.00505862 0.01573012 0.00924329 0.0121477  0.01198831\n",
      " 0.00943816 0.00921619 0.00679629 0.00633039 0.01140372 0.01243372\n",
      " 0.00937108]\n",
      "x: [0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.87997303 1.         0.65944272 0.71666667 0.5308642  0.67525773\n",
      " 0.52165354 0.45454545 0.56145251 0.2        0.42690058 0.775\n",
      " 0.22619048]\n",
      "class: 0\n",
      "softmax numerator: 0.15864620803992968\n",
      "softmax denominator: 20.424779630786347\n",
      "y hat for class: 0.007767340010895474\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.00683505 0.00776734 0.00512212 0.00556659 0.0041234  0.00524496\n",
      " 0.00405186 0.00353061 0.00436099 0.00155347 0.00331588 0.00601969\n",
      " 0.0017569 ]\n",
      "new gradient for class: [-0.87730319 -0.27140932 -0.8629965  -0.50455498 -0.66628727 -0.65636933\n",
      " -0.51682412 -0.50509501 -0.37071488 -0.34781019 -0.62603615 -0.68017611\n",
      " -0.51541705]\n",
      "class: 1\n",
      "softmax numerator: 19.950179688774014\n",
      "softmax denominator: 20.424779630786347\n",
      "y hat for class: 0.9767635220261096\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.02044747 -0.02323648 -0.01532313 -0.01665281 -0.01233541 -0.01569061\n",
      " -0.01212139 -0.01056204 -0.01304618 -0.0046473  -0.00991967 -0.01800827\n",
      " -0.00525587]\n",
      "new gradient for class: [0.84767037 0.25088156 0.83706536 0.48422547 0.64592756 0.63393536\n",
      " 0.49931643 0.4888474  0.3552334  0.33838597 0.60802864 0.65575381\n",
      " 0.502547  ]\n",
      "class: 2\n",
      "softmax numerator: 0.31595373397240417\n",
      "softmax denominator: 20.424779630786347\n",
      "y hat for class: 0.015469137962995004\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.01361242 0.01546914 0.01020101 0.01108622 0.00821201 0.01044566\n",
      " 0.00806953 0.00703143 0.00868519 0.00309383 0.00660378 0.01198858\n",
      " 0.00349897]\n",
      "new gradient for class: [0.02963282 0.02052776 0.02593113 0.02032951 0.02035971 0.02243396\n",
      " 0.01750769 0.01624761 0.01548148 0.00942422 0.01800751 0.0244223\n",
      " 0.01287005]\n",
      "w for class:  [ 0.06638061 -0.10508206 -0.02197553 -0.52685749  0.11170598  0.45835256\n",
      "  0.56480071 -0.55964583  0.1496838   0.37424744  0.05276782  0.09678758\n",
      "  0.71459111]\n",
      "w for class:  [-0.07819307 -0.20596574 -0.07651297  0.50157134 -0.14618294  0.00445477\n",
      "  0.25981827  0.35512322  0.50289173 -0.65115345  0.28027041  0.59962628\n",
      " -0.65808687]\n",
      "w for class:  [ 0.01181246  0.3110478   0.09848851  0.02528615  0.03447697 -0.46280733\n",
      " -0.82461897  0.20452261 -0.65257553  0.27690601 -0.33303822 -0.69641386\n",
      " -0.05650423]\n",
      "###################################################\n",
      "gradient descent step: 18\n",
      "indices for batch: [38, 43]\n",
      "x: [0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.87592717 0.28793103 0.80495356 1.         0.85802469 0.85051546\n",
      " 0.56889764 0.31818182 0.54748603 0.25769231 0.76608187 0.875\n",
      " 0.58630952]\n",
      "class: 0\n",
      "softmax numerator: 2.3042244588661616\n",
      "softmax denominator: 4.968833864710469\n",
      "y hat for class: 0.463735460191408\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.40619849 0.13352383 0.37328551 0.46373546 0.39789648 0.39441418\n",
      " 0.26381801 0.14755219 0.25388869 0.11950106 0.35525933 0.40576853\n",
      " 0.27189252]\n",
      "new gradient for class: [0.40619849 0.13352383 0.37328551 0.46373546 0.39789648 0.39441418\n",
      " 0.26381801 0.14755219 0.25388869 0.11950106 0.35525933 0.40576853\n",
      " 0.27189252]\n",
      "class: 1\n",
      "softmax numerator: 2.490341974825829\n",
      "softmax denominator: 4.968833864710469\n",
      "y hat for class: 0.5011924412511908\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.4369191  -0.14362218 -0.40151692 -0.49880756 -0.4279892  -0.42424354\n",
      " -0.28377044 -0.1587115  -0.27309017 -0.12853887 -0.38212743 -0.43645661\n",
      " -0.29245562]\n",
      "new gradient for class: [-0.4369191  -0.14362218 -0.40151692 -0.49880756 -0.4279892  -0.42424354\n",
      " -0.28377044 -0.1587115  -0.27309017 -0.12853887 -0.38212743 -0.43645661\n",
      " -0.29245562]\n",
      "class: 2\n",
      "softmax numerator: 0.17426743101847836\n",
      "softmax denominator: 4.968833864710469\n",
      "y hat for class: 0.03507209855740122\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.0307206  0.01009835 0.02823141 0.0350721  0.03009273 0.02982936\n",
      " 0.01995243 0.0111593  0.01920148 0.00903781 0.0268681  0.03068809\n",
      " 0.02056311]\n",
      "new gradient for class: [0.0307206  0.01009835 0.02823141 0.0350721  0.03009273 0.02982936\n",
      " 0.01995243 0.0111593  0.01920148 0.00903781 0.0268681  0.03068809\n",
      " 0.02056311]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.83142279 0.17068966 0.60371517 0.49333333 0.83950617 0.48969072\n",
      " 0.36417323 0.53030303 0.77094972 0.26153846 0.61988304 0.5775\n",
      " 0.44642857]\n",
      "class: 0\n",
      "softmax numerator: 1.844300826436165\n",
      "softmax denominator: 4.13978045346469\n",
      "y hat for class: 0.44550691689280814\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.3704046  0.07604342 0.26895928 0.21978341 0.37400581 0.2181606\n",
      " 0.16224169 0.23625367 0.34346343 0.11651719 0.27616218 0.25728024\n",
      " 0.19888702]\n",
      "new gradient for class: [0.7766031  0.20956725 0.64224479 0.68351887 0.77190228 0.61257478\n",
      " 0.4260597  0.38380586 0.59735212 0.23601825 0.63142151 0.66304877\n",
      " 0.47077953]\n",
      "class: 1\n",
      "softmax numerator: 2.0281350574105876\n",
      "softmax denominator: 4.13978045346469\n",
      "y hat for class: 0.48991367542527253\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.4240974  -0.08706646 -0.30794685 -0.25164259 -0.42822062 -0.24978454\n",
      " -0.18575978 -0.27050032 -0.39325091 -0.13340719 -0.31619386 -0.29457485\n",
      " -0.22771711]\n",
      "new gradient for class: [-0.86101649 -0.23068864 -0.70946377 -0.75045015 -0.85620982 -0.67402808\n",
      " -0.46953023 -0.42921182 -0.66634108 -0.26194606 -0.69832129 -0.73103147\n",
      " -0.52017273]\n",
      "class: 2\n",
      "softmax numerator: 0.2673445696179373\n",
      "softmax denominator: 4.13978045346469\n",
      "y hat for class: 0.06457940768191939\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.05369279 0.01102304 0.03898757 0.03185917 0.05421481 0.03162394\n",
      " 0.02351809 0.03424666 0.04978748 0.01689    0.04003168 0.03729461\n",
      " 0.02883009]\n",
      "new gradient for class: [0.0844134  0.02112138 0.06721898 0.06693127 0.08430754 0.0614533\n",
      " 0.04347053 0.04540596 0.06898896 0.02592781 0.06689978 0.06798269\n",
      " 0.0493932 ]\n",
      "w for class:  [-0.32192094 -0.20986569 -0.34309793 -0.86861692 -0.27424517  0.15206517\n",
      "  0.35177086 -0.75154876 -0.14899226  0.25623831 -0.26294294 -0.2347368\n",
      "  0.47920134]\n",
      "w for class:  [ 0.35231517 -0.09062142  0.27821891  0.87679641  0.28192197  0.34146881\n",
      "  0.49458338  0.56972913  0.83606227 -0.52018042  0.62943105  0.96514201\n",
      " -0.39800051]\n",
      "w for class:  [-0.03039424  0.30048711  0.06487902 -0.00817949 -0.0076768  -0.49353398\n",
      " -0.84635424  0.18181963 -0.68707001  0.26394211 -0.36648811 -0.73040521\n",
      " -0.08120083]\n",
      "###################################################\n",
      "gradient descent step: 19\n",
      "indices for batch: [88, 23]\n",
      "x: [0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [0. 1. 0.]\n",
      "x: [0.87997303 1.         0.65944272 0.71666667 0.5308642  0.67525773\n",
      " 0.52165354 0.45454545 0.56145251 0.2        0.42690058 0.775\n",
      " 0.22619048]\n",
      "class: 0\n",
      "softmax numerator: 0.17185414907624869\n",
      "softmax denominator: 25.444773065734253\n",
      "y hat for class: 0.006754005965479792\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.00594334 0.00675401 0.00445388 0.00484037 0.00358546 0.00456069\n",
      " 0.00352325 0.00307    0.00379205 0.0013508  0.00288329 0.00523435\n",
      " 0.00152769]\n",
      "new gradient for class: [0.00594334 0.00675401 0.00445388 0.00484037 0.00358546 0.00456069\n",
      " 0.00352325 0.00307    0.00379205 0.0013508  0.00288329 0.00523435\n",
      " 0.00152769]\n",
      "class: 1\n",
      "softmax numerator: 25.040540234773328\n",
      "softmax denominator: 25.444773065734253\n",
      "y hat for class: 0.9841133253609051\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.01397985 -0.01588667 -0.01047635 -0.01138545 -0.00843367 -0.0107276\n",
      " -0.00828734 -0.00722122 -0.00891961 -0.00317733 -0.00678203 -0.01231217\n",
      " -0.00359341]\n",
      "new gradient for class: [-0.01397985 -0.01588667 -0.01047635 -0.01138545 -0.00843367 -0.0107276\n",
      " -0.00828734 -0.00722122 -0.00891961 -0.00317733 -0.00678203 -0.01231217\n",
      " -0.00359341]\n",
      "class: 2\n",
      "softmax numerator: 0.23237868188467453\n",
      "softmax denominator: 25.444773065734253\n",
      "y hat for class: 0.009132668673615025\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.0080365  0.00913267 0.00602247 0.00654508 0.00484821 0.00616691\n",
      " 0.00476409 0.00415121 0.00512756 0.00182653 0.00389874 0.00707782\n",
      " 0.00206572]\n",
      "new gradient for class: [0.0080365  0.00913267 0.00602247 0.00654508 0.00484821 0.00616691\n",
      " 0.00476409 0.00415121 0.00512756 0.00182653 0.00389874 0.00707782\n",
      " 0.00206572]\n",
      "x: [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y: [1. 0. 0.]\n",
      "x: [0.92515172 0.24655172 0.77399381 0.55666667 0.66666667 0.87628866\n",
      " 0.72244094 0.28787879 0.5698324  0.52307692 0.52046784 0.7175\n",
      " 0.76488095]\n",
      "class: 0\n",
      "softmax numerator: 0.36785519408163747\n",
      "softmax denominator: 19.343219266441565\n",
      "y hat for class: 0.01901726848125158\n",
      "y actual for class: 1.0\n",
      "x gradient: [-0.90755786 -0.24186298 -0.75927456 -0.54608039 -0.65398849 -0.85962404\n",
      " -0.70870209 -0.28240412 -0.55899575 -0.51312943 -0.51056996 -0.70385511\n",
      " -0.75033501]\n",
      "new gradient for class: [-0.90161452 -0.23510898 -0.75482068 -0.54124002 -0.65040303 -0.85506335\n",
      " -0.70517884 -0.27933412 -0.55520369 -0.51177863 -0.50768667 -0.69862076\n",
      " -0.74880731]\n",
      "class: 1\n",
      "softmax numerator: 18.831003148946913\n",
      "softmax denominator: 19.343219266441565\n",
      "y hat for class: 0.9735196034104161\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.90065334 0.24002294 0.75349815 0.54192591 0.64901307 0.85308419\n",
      " 0.70331042 0.28025564 0.55474301 0.50922564 0.50668564 0.69850032\n",
      " 0.7446266 ]\n",
      "new gradient for class: [0.88667349 0.22413626 0.74302179 0.53054046 0.6405794  0.84235659\n",
      " 0.69502308 0.27303443 0.5458234  0.5060483  0.49990361 0.68618814\n",
      " 0.74103319]\n",
      "class: 2\n",
      "softmax numerator: 0.14436092341301363\n",
      "softmax denominator: 19.343219266441565\n",
      "y hat for class: 0.007463128108332232\n",
      "y actual for class: 0.0\n",
      "x gradient: [0.00690453 0.00184005 0.00577641 0.00415447 0.00497542 0.00653985\n",
      " 0.00539167 0.00214848 0.00425273 0.00390379 0.00388432 0.00535479\n",
      " 0.0057084 ]\n",
      "new gradient for class: [0.01494103 0.01097272 0.01179889 0.01069955 0.00982363 0.01270676\n",
      " 0.01015576 0.00629969 0.00938029 0.00573032 0.00778306 0.01243261\n",
      " 0.00777413]\n",
      "w for class:  [ 0.12888632 -0.0923112   0.03431241 -0.59799691  0.05095635  0.57959684\n",
      "  0.70436028 -0.6118817   0.12860958  0.51212763 -0.00909961  0.11457358\n",
      "  0.853605  ]\n",
      "w for class:  [-0.09102157 -0.20268955 -0.09329199  0.61152618 -0.03836773 -0.07970948\n",
      "  0.14707184  0.43321192  0.56315057 -0.77320457  0.37947925  0.62204794\n",
      " -0.7685171 ]\n",
      "w for class:  [-0.03786475  0.29500075  0.05897957 -0.01352926 -0.01258861 -0.49988736\n",
      " -0.85143212  0.17866978 -0.69176015  0.26107695 -0.37037964 -0.73662151\n",
      " -0.0850879 ]\n",
      "###################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6590052126034447, 0.3197912666546218, 0.0212035207419334] [1.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "xtest = np.asarray([[1, 2, 3, 4], [5, 6, 7, 8], [1, 1, 2, 2]])\n",
    "ytest = np.asarray([[0, 1, 0], [1, 0, 1], [0, 0, 1]])\n",
    "\n",
    "model = logisticRegressionModel.fit(xWineTrainingSets[0], yWineTrainingSets[0], gradientDescentModel)\n",
    "yh = model.predict(xWineValidationSets[0])\n",
    "\n",
    "print(yh[0],yWineTrainingSets[0][0])\n",
    "#logisticRegressionModel.fit(xtest, ytest, gradientDescentModel)\n",
    "\n",
    "\n",
    "\n",
    "#xWineTrainingSets = []\n",
    "#yWineTrainingSets = []\n",
    "#xWineValidationSets = []\n",
    "#yWineValidationSets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
