{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797\n",
      "1797\n"
     ]
    }
   ],
   "source": [
    "print(len(digits.data))\n",
    "print(digits.target.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "wine = fetch_openml(name='wine', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n",
      "178\n"
     ]
    }
   ],
   "source": [
    "print(len(wine.data))\n",
    "print(wine.target.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation for digits dataset\n",
    "\n",
    "digitsTrainingSetSize = int(np.ceil(0.8 * len(digits.data)))\n",
    "digitsValidationSetSize = int(len(digits.data) - digitsTrainingSetSize)\n",
    "\n",
    "xDigitsTrainingSets = []\n",
    "yDigitsTrainingSets = []\n",
    "xDigitsValidationSets = []\n",
    "yDigitsValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data[foldIndex*digitsValidationSetSize:((foldIndex*digitsValidationSetSize)+digitsValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(digits.target[index])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(digits.target[index])\n",
    "            \n",
    "    xDigitsTrainingSets.append(xTrainingSet)\n",
    "    yDigitsTrainingSets.append(yTrainingSet)\n",
    "    xDigitsValidationSets.append(xValidationSet)\n",
    "    yDigitsValidationSets.append(yValidationSet)\n",
    "    \n",
    "# 5-fold cross validation for wine dataset\n",
    "\n",
    "wineTrainingSetSize = int(np.ceil(0.8 * len(wine.data)))\n",
    "wineValidationSetSize = int(len(wine.data) - wineTrainingSetSize)\n",
    "\n",
    "xWineTrainingSets = []\n",
    "yWineTrainingSets = []\n",
    "xWineValidationSets = []\n",
    "yWineValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "\n",
    "    for index, data in enumerate(wine.data[foldIndex*wineValidationSetSize:((foldIndex*wineValidationSetSize)+wineValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(wine.target[index])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "\n",
    "    for index, data in enumerate(wine.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(wine.target[index])\n",
    "            \n",
    "    xWineTrainingSets.append(xTrainingSet)\n",
    "    yWineTrainingSets.append(yTrainingSet)\n",
    "    xWineValidationSets.append(xValidationSet)\n",
    "    yWineValidationSets.append(yValidationSet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of y for digits dataset\n",
    "\n",
    "numberOfDigitsTargets = 10\n",
    "numberOfWineTargets = 3\n",
    "\n",
    "for index, fold in enumerate(yDigitsTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yDigitsValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsValidationSets[index] = encodedFold\n",
    "\n",
    "# one-hot encoding of y for wine dataset\n",
    "\n",
    "for index, fold in enumerate(yWineTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yWineValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineValidationSets[index] = encodedFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomIndices(arr, batch_size):\n",
    "    indices = []\n",
    "    \n",
    "    if batch_size > len(arr):\n",
    "        print(\"Error: batch size larger than size of dataset.\")\n",
    "        return\n",
    "    \n",
    "    while batch_size > 0:\n",
    "        x = np.floor(np.random.random() * len(arr))\n",
    "        if x not in indices:\n",
    "            indices.append(int(x))\n",
    "            batch_size -= 1\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent class\n",
    " \n",
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, batch_size, learning_rate=.001, momentum=0.9, max_iters=10, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iters = max_iters\n",
    "        self.epsilon = epsilon\n",
    "            \n",
    "    def run(self, gradient_fn, x, y, w):\n",
    "        # TODO: grad variable and usage in loop?\n",
    "        \n",
    "        grad = np.inf\n",
    "        t = 1\n",
    "        \n",
    "        while np.linalg.norm(grad) > self.epsilon and t < self.max_iters:\n",
    "            print(\"step:\", t)\n",
    "            \n",
    "            gradients = gradient_fn(x, y, w, self.batch_size)\n",
    "            \n",
    "            # TODO: implement momentum here\n",
    "            \n",
    "            for c in range(len(y[0])):\n",
    "                w[c] = w[c] - 1 * gradients[c]\n",
    "                \n",
    "                print(\"w for class: \", w[c])\n",
    "            \n",
    "            print(\"###################################################\")\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, add_bias=True):\n",
    "        self.add_bias = add_bias\n",
    "        pass\n",
    "            \n",
    "    def fit(self, x, y, optimizer):\n",
    "        # TODO: add bias\n",
    "        \n",
    "        def gradient(x, y, w, batch_size):\n",
    "            gradients = np.zeros(len(w)).tolist()\n",
    "\n",
    "            indices = getRandomIndices(x, batch_size)\n",
    "            print(\"indices for batch:\", indices)\n",
    "\n",
    "            for index in indices:\n",
    "                a = np.asarray(x[index])\n",
    "                b = np.asarray(y[index])\n",
    "\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  w[c] @ a\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  w[i] @ a\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "\n",
    "                    y_c = b[c]\n",
    "                    cost_c = np.dot(yh_c - y_c, a)\n",
    "                    \n",
    "                    gradients[c] += cost_c\n",
    "                    \n",
    "                    print(\"x:\", a)\n",
    "                    print(\"y:\", b)\n",
    "                    print(\"class:\", c)\n",
    "                    print(\"softmax numerator:\", num)\n",
    "                    print(\"softmax denominator:\", den)\n",
    "                    print(\"y hat for class:\", yh_c)\n",
    "                    print(\"y actual for class:\", y_c)\n",
    "                    print(\"x gradient:\", cost_c)\n",
    "                    print(\"new gradient for class:\", gradients[c])\n",
    "\n",
    "            return gradients\n",
    "        \n",
    "        w0 = []\n",
    "        for c in range(len(y[0])):\n",
    "            w0.append(np.zeros(len(x[0])))\n",
    "            \n",
    "        self.w = optimizer.run(gradient, x, y, w0)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if self.add_bias:\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "        yh = x@self.w\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientDescentModel = GradientDescent(2)\n",
    "logisticRegressionModel = LogisticRegression(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1\n",
      "indices for batch: [2, 0]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 0\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 0\n",
      "x gradient: [0.33333333 0.33333333 0.66666667 0.66666667]\n",
      "new gradient for class: [0.33333333 0.33333333 0.66666667 0.66666667]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 1\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 0\n",
      "x gradient: [0.33333333 0.33333333 0.66666667 0.66666667]\n",
      "new gradient for class: [0.33333333 0.33333333 0.66666667 0.66666667]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 2\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 1\n",
      "x gradient: [-0.66666667 -0.66666667 -1.33333333 -1.33333333]\n",
      "new gradient for class: [-0.66666667 -0.66666667 -1.33333333 -1.33333333]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 0\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 0\n",
      "x gradient: [0.33333333 0.66666667 1.         1.33333333]\n",
      "new gradient for class: [0.66666667 1.         1.66666667 2.        ]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 1\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 1\n",
      "x gradient: [-0.66666667 -1.33333333 -2.         -2.66666667]\n",
      "new gradient for class: [-0.33333333 -1.         -1.33333333 -2.        ]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 2\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 0\n",
      "x gradient: [0.33333333 0.66666667 1.         1.33333333]\n",
      "new gradient for class: [-3.33333333e-01 -1.11022302e-16 -3.33333333e-01 -2.22044605e-16]\n",
      "w for class:  [-0.66666667 -1.         -1.66666667 -2.        ]\n",
      "w for class:  [0.33333333 1.         1.33333333 2.        ]\n",
      "w for class:  [3.33333333e-01 1.11022302e-16 3.33333333e-01 2.22044605e-16]\n",
      "###################################################\n",
      "step: 2\n",
      "indices for batch: [1, 0]\n",
      "x: [5 6 7 8]\n",
      "y: [1 0 1]\n",
      "class: 0\n",
      "softmax numerator: 8.533047625744066e-17\n",
      "softmax denominator: 214643579785972.2\n",
      "y hat for class: 3.975449735907607e-31\n",
      "y actual for class: 1\n",
      "x gradient: [-5. -6. -7. -8.]\n",
      "new gradient for class: [-5. -6. -7. -8.]\n",
      "x: [5 6 7 8]\n",
      "y: [1 0 1]\n",
      "class: 1\n",
      "softmax numerator: 214643579785917.6\n",
      "softmax denominator: 214643579785972.2\n",
      "y hat for class: 0.9999999999997456\n",
      "y actual for class: 0\n",
      "x gradient: [5. 6. 7. 8.]\n",
      "new gradient for class: [5. 6. 7. 8.]\n",
      "x: [5 6 7 8]\n",
      "y: [1 0 1]\n",
      "class: 2\n",
      "softmax numerator: 54.598150033144435\n",
      "softmax denominator: 214643579785972.2\n",
      "y hat for class: 2.543665647376267e-13\n",
      "y actual for class: 1\n",
      "x gradient: [-5. -6. -7. -8.]\n",
      "new gradient for class: [-5. -6. -7. -8.]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 0\n",
      "softmax numerator: 1.570554880974321e-07\n",
      "softmax denominator: 1678373.2751101807\n",
      "y hat for class: 9.357601817576715e-14\n",
      "y actual for class: 0\n",
      "x gradient: [9.35760182e-14 1.87152036e-13 2.80728055e-13 3.74304073e-13]\n",
      "new gradient for class: [-5. -6. -7. -8.]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 1\n",
      "softmax numerator: 1678369.4814421288\n",
      "softmax denominator: 1678373.2751101807\n",
      "y hat for class: 0.9999977396756083\n",
      "y actual for class: 1\n",
      "x gradient: [-2.26032439e-06 -4.52064878e-06 -6.78097317e-06 -9.04129757e-06]\n",
      "new gradient for class: [4.99999774 5.99999548 6.99999322 7.99999096]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 2\n",
      "softmax numerator: 3.793667894683184\n",
      "softmax denominator: 1678373.2751101807\n",
      "y hat for class: 2.260324297903361e-06\n",
      "y actual for class: 0\n",
      "x gradient: [2.26032430e-06 4.52064860e-06 6.78097289e-06 9.04129719e-06]\n",
      "new gradient for class: [-4.99999774 -5.99999548 -6.99999322 -7.99999096]\n",
      "w for class:  [4.33333333 5.         5.33333333 6.        ]\n",
      "w for class:  [-4.66666441 -4.99999548 -5.66665989 -5.99999096]\n",
      "w for class:  [5.33333107 5.99999548 7.33332655 7.99999096]\n",
      "###################################################\n",
      "step: 3\n",
      "indices for batch: [1, 0]\n",
      "x: [5 6 7 8]\n",
      "y: [1 0 1]\n",
      "class: 0\n",
      "softmax numerator: 3.1502427499508586e+59\n",
      "softmax denominator: 2.0153434282120779e+77\n",
      "y hat for class: 1.5631294923990263e-18\n",
      "y actual for class: 1\n",
      "x gradient: [-5. -6. -7. -8.]\n",
      "new gradient for class: [-5. -6. -7. -8.]\n",
      "x: [5 6 7 8]\n",
      "y: [1 0 1]\n",
      "class: 1\n",
      "softmax numerator: 5.814960472159452e-62\n",
      "softmax denominator: 2.0153434282120779e+77\n",
      "y hat for class: 2.8853446964710246e-139\n",
      "y actual for class: 0\n",
      "x gradient: [1.44267235e-138 1.73120682e-138 2.01974129e-138 2.30827576e-138]\n",
      "new gradient for class: [1.44267235e-138 1.73120682e-138 2.01974129e-138 2.30827576e-138]\n",
      "x: [5 6 7 8]\n",
      "y: [1 0 1]\n",
      "class: 2\n",
      "softmax numerator: 2.0153434282120779e+77\n",
      "softmax denominator: 2.0153434282120779e+77\n",
      "y hat for class: 1.0\n",
      "y actual for class: 1\n",
      "x gradient: [0. 0. 0. 0.]\n",
      "new gradient for class: [0. 0. 0. 0.]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 0\n",
      "softmax numerator: 3.9506344823925756e+23\n",
      "softmax denominator: 9.542092253203176e+30\n",
      "y hat for class: 4.140218285005986e-08\n",
      "y actual for class: 0\n",
      "x gradient: [4.14021829e-08 8.28043657e-08 1.24206549e-07 1.65608731e-07]\n",
      "new gradient for class: [-4.99999996 -5.99999992 -6.99999988 -7.99999983]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 1\n",
      "softmax numerator: 6.672725972292512e-25\n",
      "softmax denominator: 9.542092253203176e+30\n",
      "y hat for class: 6.9929380215880345e-56\n",
      "y actual for class: 1\n",
      "x gradient: [-1. -2. -3. -4.]\n",
      "new gradient for class: [-1. -2. -3. -4.]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 2\n",
      "softmax numerator: 9.542091858139728e+30\n",
      "softmax denominator: 9.542092253203176e+30\n",
      "y hat for class: 0.9999999585978172\n",
      "y actual for class: 0\n",
      "x gradient: [0.99999996 1.99999992 2.99999988 3.99999983]\n",
      "new gradient for class: [0.99999996 1.99999992 2.99999988 3.99999983]\n",
      "w for class:  [ 9.33333329 10.99999992 12.33333321 13.99999983]\n",
      "w for class:  [-3.66666441 -2.99999548 -2.66665989 -1.99999096]\n",
      "w for class:  [4.33333111 3.99999556 4.33332668 3.99999112]\n",
      "###################################################\n",
      "step: 4\n",
      "indices for batch: [2, 0]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 0\n",
      "softmax numerator: 5.05239007420715e+31\n",
      "softmax denominator: 5.05239007420715e+31\n",
      "y hat for class: 1.0\n",
      "y actual for class: 0\n",
      "x gradient: [1. 1. 2. 2.]\n",
      "new gradient for class: [1. 1. 2. 2.]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 1\n",
      "softmax numerator: 1.1253949902551982e-07\n",
      "softmax denominator: 5.05239007420715e+31\n",
      "y hat for class: 2.227450718820046e-39\n",
      "y actual for class: 0\n",
      "x gradient: [2.22745072e-39 2.22745072e-39 4.45490144e-39 4.45490144e-39]\n",
      "new gradient for class: [2.22745072e-39 2.22745072e-39 4.45490144e-39 4.45490144e-39]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 2\n",
      "softmax numerator: 72002183242.38322\n",
      "softmax denominator: 5.05239007420715e+31\n",
      "y hat for class: 1.4251113272104632e-21\n",
      "y actual for class: 1\n",
      "x gradient: [-1. -1. -2. -2.]\n",
      "new gradient for class: [-1. -1. -2. -2.]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 0\n",
      "softmax numerator: 9.937566408561015e+53\n",
      "softmax denominator: 9.937566408561015e+53\n",
      "y hat for class: 1.0\n",
      "y actual for class: 0\n",
      "x gradient: [1. 2. 3. 4.]\n",
      "new gradient for class: [2. 3. 5. 6.]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 1\n",
      "softmax numerator: 7.130791649238178e-12\n",
      "softmax denominator: 9.937566408561015e+53\n",
      "y hat for class: 7.175591443691026e-66\n",
      "y actual for class: 1\n",
      "x gradient: [-1. -2. -3. -4.]\n",
      "new gradient for class: [-1. -2. -3. -4.]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 2\n",
      "softmax numerator: 8.929140884821947e+17\n",
      "softmax denominator: 9.937566408561015e+53\n",
      "y hat for class: 8.98523895863445e-37\n",
      "y actual for class: 0\n",
      "x gradient: [8.98523896e-37 1.79704779e-36 2.69557169e-36 3.59409558e-36]\n",
      "new gradient for class: [-1. -1. -2. -2.]\n",
      "w for class:  [7.33333329 7.99999992 7.33333321 7.99999983]\n",
      "w for class:  [-2.66666441 -0.99999548  0.33334011  2.00000904]\n",
      "w for class:  [5.33333111 4.99999556 6.33332668 5.99999112]\n",
      "###################################################\n",
      "step: 5\n",
      "indices for batch: [2, 1]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 0\n",
      "softmax numerator: 9.496112736868442e+19\n",
      "softmax denominator: 9.496271332231076e+19\n",
      "y hat for class: 0.9999832991963808\n",
      "y actual for class: 0\n",
      "x gradient: [0.9999833 0.9999833 1.9999666 1.9999666]\n",
      "new gradient for class: [0.9999833 0.9999833 1.9999666 1.9999666]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 1\n",
      "softmax numerator: 2.718386281872467\n",
      "softmax denominator: 9.496271332231076e+19\n",
      "y hat for class: 2.862582783040386e-20\n",
      "y actual for class: 0\n",
      "x gradient: [2.86258278e-20 2.86258278e-20 5.72516557e-20 5.72516557e-20]\n",
      "new gradient for class: [2.86258278e-20 2.86258278e-20 5.72516557e-20 5.72516557e-20]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 2\n",
      "softmax numerator: 1585953626339759.5\n",
      "softmax denominator: 9.496271332231076e+19\n",
      "y hat for class: 1.670080361917325e-05\n",
      "y actual for class: 1\n",
      "x gradient: [-0.9999833 -0.9999833 -1.9999666 -1.9999666]\n",
      "new gradient for class: [-0.9999833 -0.9999833 -1.9999666 -1.9999666]\n",
      "x: [5 6 7 8]\n",
      "y: [1 0 1]\n",
      "class: 0\n",
      "softmax numerator: 7.22595282613244e+86\n",
      "softmax denominator: 7.22595282613244e+86\n",
      "y hat for class: 1.0\n",
      "y actual for class: 1\n",
      "x gradient: [0. 0. 0. 0.]\n",
      "new gradient for class: [0.9999833 0.9999833 1.9999666 1.9999666]\n",
      "x: [5 6 7 8]\n",
      "y: [1 0 1]\n",
      "class: 1\n",
      "softmax numerator: 0.36793765267398126\n",
      "softmax denominator: 7.22595282613244e+86\n",
      "y hat for class: 5.091891153002631e-88\n",
      "y actual for class: 0\n",
      "x gradient: [2.54594558e-87 3.05513469e-87 3.56432381e-87 4.07351292e-87]\n",
      "new gradient for class: [2.86258278e-20 2.86258278e-20 5.72516557e-20 5.72516557e-20]\n",
      "x: [5 6 7 8]\n",
      "y: [1 0 1]\n",
      "class: 2\n",
      "softmax numerator: 5.1263747030056474e+64\n",
      "softmax denominator: 7.22595282613244e+86\n",
      "y hat for class: 7.094392706891566e-23\n",
      "y actual for class: 1\n",
      "x gradient: [-5. -6. -7. -8.]\n",
      "new gradient for class: [-5.9999833 -6.9999833 -8.9999666 -9.9999666]\n",
      "w for class:  [6.33334999 7.00001662 5.33336661 6.00003324]\n",
      "w for class:  [-2.66666441 -0.99999548  0.33334011  2.00000904]\n",
      "w for class:  [11.33331441 11.99997886 15.33329327 15.99995772]\n",
      "###################################################\n",
      "step: 6\n",
      "indices for batch: [2, 0]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 0\n",
      "softmax numerator: 4311948582639197.5\n",
      "softmax denominator: 2.2347890291587843e+37\n",
      "y hat for class: 1.9294656123590755e-22\n",
      "y actual for class: 0\n",
      "x gradient: [1.92946561e-22 1.92946561e-22 3.85893122e-22 3.85893122e-22]\n",
      "new gradient for class: [1.92946561e-22 1.92946561e-22 3.85893122e-22 3.85893122e-22]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 1\n",
      "softmax numerator: 2.718386281872467\n",
      "softmax denominator: 2.2347890291587843e+37\n",
      "y hat for class: 1.2163950361326578e-37\n",
      "y actual for class: 0\n",
      "x gradient: [1.21639504e-37 1.21639504e-37 2.43279007e-37 2.43279007e-37]\n",
      "new gradient for class: [1.21639504e-37 1.21639504e-37 2.43279007e-37 2.43279007e-37]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 2\n",
      "softmax numerator: 2.2347890291587843e+37\n",
      "softmax denominator: 2.2347890291587843e+37\n",
      "y hat for class: 1.0\n",
      "y actual for class: 1\n",
      "x gradient: [0. 0. 0. 0.]\n",
      "new gradient for class: [0. 0. 0. 0.]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 0\n",
      "softmax numerator: 1.5942502883490534e+26\n",
      "softmax denominator: 1.310124267453595e+63\n",
      "y hat for class: 1.2168695199026396e-37\n",
      "y actual for class: 0\n",
      "x gradient: [1.21686952e-37 2.43373904e-37 3.65060856e-37 4.86747808e-37]\n",
      "new gradient for class: [1.92946561e-22 1.92946561e-22 3.85893122e-22 3.85893122e-22]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 1\n",
      "softmax numerator: 76.20302370573062\n",
      "softmax denominator: 1.310124267453595e+63\n",
      "y hat for class: 5.816472955946506e-62\n",
      "y actual for class: 1\n",
      "x gradient: [-1. -2. -3. -4.]\n",
      "new gradient for class: [-1. -2. -3. -4.]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 2\n",
      "softmax numerator: 1.310124267453595e+63\n",
      "softmax denominator: 1.310124267453595e+63\n",
      "y hat for class: 1.0\n",
      "y actual for class: 0\n",
      "x gradient: [1. 2. 3. 4.]\n",
      "new gradient for class: [1. 2. 3. 4.]\n",
      "w for class:  [6.33334999 7.00001662 5.33336661 6.00003324]\n",
      "w for class:  [-1.66666441  1.00000452  3.33334011  6.00000904]\n",
      "w for class:  [10.33331441  9.99997886 12.33329327 11.99995772]\n",
      "###################################################\n",
      "step: 7\n",
      "indices for batch: [2, 0]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 0\n",
      "softmax numerator: 4311948582639197.5\n",
      "softmax denominator: 9.251887395341779e+29\n",
      "y hat for class: 4.660615070618148e-15\n",
      "y actual for class: 0\n",
      "x gradient: [4.66061507e-15 4.66061507e-15 9.32123014e-15 9.32123014e-15]\n",
      "new gradient for class: [4.66061507e-15 4.66061507e-15 9.32123014e-15 9.32123014e-15]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 1\n",
      "softmax numerator: 65662492.2045966\n",
      "softmax denominator: 9.251887395341779e+29\n",
      "y hat for class: 7.09719967383703e-23\n",
      "y actual for class: 0\n",
      "x gradient: [7.09719967e-23 7.09719967e-23 1.41943993e-22 1.41943993e-22]\n",
      "new gradient for class: [7.09719967e-23 7.09719967e-23 1.41943993e-22 1.41943993e-22]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 2\n",
      "softmax numerator: 9.251887395341735e+29\n",
      "softmax denominator: 9.251887395341779e+29\n",
      "y hat for class: 0.9999999999999953\n",
      "y actual for class: 1\n",
      "x gradient: [-4.66293670e-15 -4.66293670e-15 -9.32587341e-15 -9.32587341e-15]\n",
      "new gradient for class: [-4.66293670e-15 -4.66293670e-15 -9.32587341e-15 -9.32587341e-15]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 0\n",
      "softmax numerator: 1.5942502883490534e+26\n",
      "softmax denominator: 1.225964893715867e+50\n",
      "y hat for class: 1.3004045193471432e-24\n",
      "y actual for class: 0\n",
      "x gradient: [1.30040452e-24 2.60080904e-24 3.90121356e-24 5.20161808e-24]\n",
      "new gradient for class: [4.66061507e-15 4.66061507e-15 9.32123015e-15 9.32123015e-15]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 1\n",
      "softmax numerator: 814341675866599.9\n",
      "softmax denominator: 1.225964893715867e+50\n",
      "y hat for class: 6.642455098354015e-36\n",
      "y actual for class: 1\n",
      "x gradient: [-1. -2. -3. -4.]\n",
      "new gradient for class: [-1. -2. -3. -4.]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 2\n",
      "softmax numerator: 1.225964893715867e+50\n",
      "softmax denominator: 1.225964893715867e+50\n",
      "y hat for class: 1.0\n",
      "y actual for class: 0\n",
      "x gradient: [1. 2. 3. 4.]\n",
      "new gradient for class: [1. 2. 3. 4.]\n",
      "w for class:  [6.33334999 7.00001662 5.33336661 6.00003324]\n",
      "w for class:  [-0.66666441  3.00000452  6.33334011 10.00000904]\n",
      "w for class:  [9.33331441 7.99997886 9.33329327 7.99995772]\n",
      "###################################################\n",
      "step: 8\n",
      "indices for batch: [2, 1]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 0\n",
      "softmax numerator: 4311948582639013.5\n",
      "softmax denominator: 3.830224349595345e+22\n",
      "y hat for class: 1.125769194980592e-07\n",
      "y actual for class: 0\n",
      "x gradient: [1.12576919e-07 1.12576919e-07 2.25153839e-07 2.25153839e-07]\n",
      "new gradient for class: [1.12576919e-07 1.12576919e-07 2.25153839e-07 2.25153839e-07]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 1\n",
      "softmax numerator: 1586074396884042.8\n",
      "softmax denominator: 3.830224349595345e+22\n",
      "y hat for class: 4.140943851113077e-08\n",
      "y actual for class: 0\n",
      "x gradient: [4.14094385e-08 4.14094385e-08 8.28188770e-08 8.28188770e-08]\n",
      "new gradient for class: [4.14094385e-08 4.14094385e-08 8.28188770e-08 8.28188770e-08]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 2\n",
      "softmax numerator: 3.830223759793047e+22\n",
      "softmax denominator: 3.830224349595345e+22\n",
      "y hat for class: 0.999999846013642\n",
      "y actual for class: 1\n",
      "x gradient: [-1.53986358e-07 -1.53986358e-07 -3.07972716e-07 -3.07972716e-07]\n",
      "new gradient for class: [-1.53986358e-07 -1.53986358e-07 -3.07972716e-07 -3.07972716e-07]\n",
      "x: [5 6 7 8]\n",
      "y: [1 0 1]\n",
      "class: 0\n",
      "softmax numerator: 1.1301048524515737e+69\n",
      "softmax denominator: 1.9124897400741248e+97\n",
      "y hat for class: 5.90907668036835e-29\n",
      "y actual for class: 1\n",
      "x gradient: [-5. -6. -7. -8.]\n",
      "new gradient for class: [-4.99999989 -5.99999989 -6.99999977 -7.99999977]\n",
      "x: [5 6 7 8]\n",
      "y: [1 0 1]\n",
      "class: 1\n",
      "softmax numerator: 2.3281003697857484e+60\n",
      "softmax denominator: 1.9124897400741248e+97\n",
      "y hat for class: 1.21731391337844e-37\n",
      "y actual for class: 0\n",
      "x gradient: [6.08656957e-37 7.30388348e-37 8.52119739e-37 9.73851131e-37]\n",
      "new gradient for class: [4.14094385e-08 4.14094385e-08 8.28188770e-08 8.28188770e-08]\n",
      "x: [5 6 7 8]\n",
      "y: [1 0 1]\n",
      "class: 2\n",
      "softmax numerator: 1.9124897400741248e+97\n",
      "softmax denominator: 1.9124897400741248e+97\n",
      "y hat for class: 1.0\n",
      "y actual for class: 1\n",
      "x gradient: [0. 0. 0. 0.]\n",
      "new gradient for class: [-1.53986358e-07 -1.53986358e-07 -3.07972716e-07 -3.07972716e-07]\n",
      "w for class:  [11.33334988 13.00001651 12.33336639 14.00003301]\n",
      "w for class:  [-0.66666445  3.00000448  6.33334003 10.00000896]\n",
      "w for class:  [9.33331457 7.99997902 9.33329358 7.99995803]\n",
      "###################################################\n",
      "step: 9\n",
      "indices for batch: [0, 2]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 0\n",
      "softmax numerator: 4.010231151618666e+56\n",
      "softmax denominator: 4.010231151618666e+56\n",
      "y hat for class: 1.0\n",
      "y actual for class: 0\n",
      "x gradient: [1. 2. 3. 4.]\n",
      "new gradient for class: [1. 2. 3. 4.]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 1\n",
      "softmax numerator: 8.702435493651744e+27\n",
      "softmax denominator: 4.010231151618666e+56\n",
      "y hat for class: 2.1700583244781648e-29\n",
      "y actual for class: 1\n",
      "x gradient: [-1. -2. -3. -4.]\n",
      "new gradient for class: [-1. -2. -3. -4.]\n",
      "x: [1 2 3 4]\n",
      "y: [0 1 0]\n",
      "class: 2\n",
      "softmax numerator: 1.1472147279809874e+37\n",
      "softmax denominator: 4.010231151618666e+56\n",
      "y hat for class: 2.8607197056906125e-20\n",
      "y actual for class: 0\n",
      "x gradient: [2.86071971e-20 5.72143941e-20 8.58215912e-20 1.14428788e-19]\n",
      "new gradient for class: [2.86071971e-20 5.72143941e-20 8.58215912e-20 1.14428788e-19]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 0\n",
      "softmax numerator: 2.758969139073519e+33\n",
      "softmax denominator: 2.7589691391118215e+33\n",
      "y hat for class: 0.9999999999861171\n",
      "y actual for class: 0\n",
      "x gradient: [1. 1. 2. 2.]\n",
      "new gradient for class: [2. 3. 5. 6.]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 1\n",
      "softmax numerator: 1586073740099673.2\n",
      "softmax denominator: 2.7589691391118215e+33\n",
      "y hat for class: 5.748791161217078e-19\n",
      "y actual for class: 0\n",
      "x gradient: [5.74879116e-19 5.74879116e-19 1.14975823e-18 1.14975823e-18]\n",
      "new gradient for class: [-1. -2. -3. -4.]\n",
      "x: [1 1 2 2]\n",
      "y: [0 0 1]\n",
      "class: 2\n",
      "softmax numerator: 3.830229657819642e+22\n",
      "softmax denominator: 2.7589691391118215e+33\n",
      "y hat for class: 1.388282892882479e-11\n",
      "y actual for class: 1\n",
      "x gradient: [-1. -1. -2. -2.]\n",
      "new gradient for class: [-1. -1. -2. -2.]\n",
      "w for class:  [ 9.33334988 10.00001651  7.33336639  8.00003301]\n",
      "w for class:  [ 0.33333555  5.00000448  9.33334003 14.00000896]\n",
      "w for class:  [10.33331457  8.99997902 11.33329358  9.99995803]\n",
      "###################################################\n"
     ]
    }
   ],
   "source": [
    "xtest = np.asarray([[1, 2, 3, 4], [5, 6, 7, 8], [1, 1, 2, 2]])\n",
    "ytest = np.asarray([[0, 1, 0], [1, 0, 1], [0, 0, 1]])\n",
    "\n",
    "#logisticRegressionModel.fit(xWineTrainingSets[0], yWineTrainingSets[0], gradientDescentModel)\n",
    "\n",
    "logisticRegressionModel.fit(xtest, ytest, gradientDescentModel)\n",
    "\n",
    "\n",
    "\n",
    "#xWineTrainingSets = []\n",
    "#yWineTrainingSets = []\n",
    "#xWineValidationSets = []\n",
    "#yWineValidationSets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
