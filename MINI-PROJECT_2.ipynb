{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniProject 2: Multi-Class Logistic Regression and Gradient Descent\n",
    "## COMP 551\n",
    "### David Castonguay (260804528), Marco Guida (260803123), Sean Smith (260787775)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lia\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "from sklearn import svm \n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_search will perform the gridsearch in order to run the tests to plot graphs\n",
    "#if run_search = False, there will be no plots\n",
    "#ONLY TURN RUN_SEARCH TO TRUE IF YOU HAVE 5 HOURS TO SPARE\n",
    "run_search = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "wine = fetch_openml(name='wine', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of digits and wine data\n",
    "\n",
    "digits_data_norm = []\n",
    "\n",
    "for col in digits.data:\n",
    "    col_norm = col/np.max(col)\n",
    "    digits_data_norm.append(col_norm)\n",
    "\n",
    "digits.data = np.asarray(digits_data_norm)\n",
    "\n",
    "wine_data_norm = []\n",
    "\n",
    "for col in wine.data.T:\n",
    "    col_norm = col/np.amax(col)\n",
    "    wine_data_norm.append(col_norm)\n",
    "    \n",
    "wine.data = np.asarray(wine_data_norm).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Five-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation for digits dataset\n",
    "\n",
    "digitsTrainingSetSize = int(np.ceil(0.8 * len(digits.data)))\n",
    "digitsValidationSetSize = int(len(digits.data) - digitsTrainingSetSize)\n",
    "\n",
    "xDigitsTrainingSets = []\n",
    "yDigitsTrainingSets = []\n",
    "xDigitsValidationSets = []\n",
    "yDigitsValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data[foldIndex*digitsValidationSetSize:((foldIndex*digitsValidationSetSize)+digitsValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(digits.target[index+(foldIndex*digitsValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(digits.target[index])\n",
    "            \n",
    "    xDigitsTrainingSets.append(xTrainingSet)\n",
    "    yDigitsTrainingSets.append(yTrainingSet)\n",
    "    xDigitsValidationSets.append(xValidationSet)\n",
    "    yDigitsValidationSets.append(yValidationSet)\n",
    "    \n",
    "# 5-fold cross validation for wine dataset\n",
    "\n",
    "wineTrainingSetSize = int(np.ceil(0.8 * len(wine.data)))\n",
    "wineValidationSetSize = int(len(wine.data) - wineTrainingSetSize)\n",
    "\n",
    "xWineTrainingSets = []\n",
    "yWineTrainingSets = []\n",
    "xWineValidationSets = []\n",
    "yWineValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "    for index, data in enumerate(wine.data[foldIndex*wineValidationSetSize:((foldIndex*wineValidationSetSize)+wineValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(wine.target[index+(foldIndex*wineValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "    \n",
    "    for index, data in enumerate(wine.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(wine.target[index])\n",
    "            \n",
    "    xWineTrainingSets.append(xTrainingSet)\n",
    "    yWineTrainingSets.append(yTrainingSet)\n",
    "    xWineValidationSets.append(xValidationSet)\n",
    "    yWineValidationSets.append(yValidationSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of y for digits dataset\n",
    "\n",
    "numberOfDigitsTargets = 10\n",
    "numberOfWineTargets = 3\n",
    "\n",
    "for index, fold in enumerate(yDigitsTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yDigitsValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsValidationSets[index] = encodedFold\n",
    "\n",
    "# one-hot encoding of y for wine dataset\n",
    "\n",
    "for index, fold in enumerate(yWineTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yWineValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineValidationSets[index] = encodedFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomIndices(arr, batch_size):\n",
    "    indices = []\n",
    "    \n",
    "    if batch_size > len(arr):\n",
    "        print(\"Error: batch size larger than size of dataset.\")\n",
    "        return\n",
    "    \n",
    "    while batch_size > 0:\n",
    "        x = np.floor(np.random.random() * len(arr))\n",
    "        if x not in indices:\n",
    "            indices.append(int(x))\n",
    "            batch_size -= 1\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent class\n",
    " \n",
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, batch_size, learning_rate=0.5, momentum=0.9, max_termination_condition=25, max_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        self.max_termination_condition = max_termination_condition\n",
    "        self.max_iters = max_iters\n",
    "        self.deltas = []\n",
    "        \n",
    "    def run(self, gradient_fn, x, y, w, vx, vy):\n",
    "        t = 1\n",
    "        \n",
    "        max_cost = np.inf\n",
    "        termination_count = 0        \n",
    "        weight_history = []\n",
    "        error_history = []\n",
    "                \n",
    "        for number_of_targets in range(len(y[0])):\n",
    "            weight_history.append([])\n",
    "        \n",
    "        while termination_count < self.max_termination_condition and t < self.max_iters:\n",
    "            gradients = gradient_fn(x, y, w, self.batch_size)   \n",
    "            \n",
    "            for c in range(len(y[0])):\n",
    "                if(t==1):\n",
    "                    w[c] = w[c] - self.learning_rate * gradients[c]\n",
    "                else:\n",
    "                    delta_w = (self.momentum)*(self.deltas[-(len(y[0]))]) + (1-self.momentum)*gradients[c]\n",
    "                    w[c] = w[c] - (self.learning_rate)*(delta_w)\n",
    "                self.deltas.append(w[c])\n",
    "            \n",
    "            a = np.asarray(vx)\n",
    "            b = np.asarray(w)\n",
    "    \n",
    "            vyh=[]\n",
    "        \n",
    "            for i, vx_c in enumerate(a):\n",
    "                vyh_x=[]\n",
    "\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  b[c] @ vx_c\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    \n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  b[i] @ vx_c\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    vyh_c = num/den\n",
    "                    vyh_x.append(vyh_c)\n",
    "                    \n",
    "                vyh.append(vyh_x)\n",
    "                \n",
    "            step_cost = 0\n",
    "                \n",
    "            def cost(yh, y):\n",
    "                return y * np.log1p(np.exp(-yh)) + (1-yh) * np.log1p(np.exp(yh))\n",
    "                \n",
    "            for sample_index, vyh_x in enumerate(vyh):\n",
    "                c = np.argmax(vy[sample_index])\n",
    "                cst = cost(vyh_x[c], vy[sample_index][c])\n",
    "                step_cost += cst\n",
    "            \n",
    "            for c in range(len(b)):\n",
    "                weight_history[c].append(w[c])\n",
    "            \n",
    "            error_history.append(step_cost)\n",
    "\n",
    "            if step_cost < max_cost:\n",
    "                max_cost = step_cost\n",
    "                termination_count = 0\n",
    "                print(f\"\\t\\tStep {t}: new best cost of {step_cost:.3f}\")\n",
    "            else:\n",
    "                termination_count += 1\n",
    "                print(f\"\\t\\tStep {t}\")\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        index_best = len(error_history)-self.max_termination_condition-1\n",
    "        \n",
    "        w_best = []\n",
    "        \n",
    "        for c in range(len(y[0])):\n",
    "            w_best.append(weight_history[c][index_best])\n",
    "        \n",
    "        return w_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, add_bias=True):\n",
    "        self.add_bias = add_bias\n",
    "        pass\n",
    "            \n",
    "    def fit(self, x, y, optimizer, vx, vy):\n",
    "        def gradient(x, y, w, batch_size):\n",
    "            gradients = np.zeros(len(w)).tolist()\n",
    "\n",
    "            indices = getRandomIndices(x, batch_size)\n",
    "\n",
    "            for index in indices:\n",
    "                a = np.asarray(x[index])\n",
    "                b = np.asarray(y[index])\n",
    "\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  w[c] @ a\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  w[i] @ a\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "\n",
    "                    y_c = b[c]\n",
    "                    \n",
    "                    cost_c = np.dot(yh_c - y_c, a)\n",
    "                    \n",
    "                    gradients[c] += cost_c\n",
    "\n",
    "            return gradients\n",
    "        \n",
    "        if self.add_bias:\n",
    "            x = np.asarray(x)\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "            \n",
    "            vx = np.asarray(vx)\n",
    "            vN = vx.shape[0]\n",
    "            vx = np.column_stack([vx,np.ones(vN)])\n",
    "\n",
    "        w0 = []\n",
    "        for c in range(len(y[0])):\n",
    "            w0.append(np.zeros(len(x[0])))\n",
    "            \n",
    "        self.w = optimizer.run(gradient, x, y, w0, vx, vy)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if self.add_bias:\n",
    "            x = np.asarray(x)\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "\n",
    "        a = np.asarray(x)\n",
    "        b = np.asarray(self.w)\n",
    "\n",
    "        yh=[]\n",
    "        \n",
    "        for i, x_c in enumerate(a):\n",
    "            yh_x=[]\n",
    "            \n",
    "            for c in range(len(b)):\n",
    "                w_x =  b[c] @ x_c\n",
    "                num = np.exp(w_x)\n",
    "\n",
    "                den = 0\n",
    "                \n",
    "                for i in range(len(b)):\n",
    "                    w_x =  b[i] @ x_c\n",
    "                    den += np.exp(w_x)\n",
    "\n",
    "                yh_c = num/den\n",
    "                yh_x.append(yh_c)\n",
    "                \n",
    "            yh.append(yh_x)\n",
    "        \n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def runLogisticRegression(batch_size, learning_rate, momentum):\n",
    "    def accurate(a, b):\n",
    "        return np.argmax(a) == np.argmax(b)\n",
    "\n",
    "    def cost(yh, y):\n",
    "        return y * np.log1p(np.exp(-yh)) + (1-yh) * np.log1p(np.exp(yh))\n",
    "\n",
    "    print(\"Model hyper-parameters:\")\n",
    "    print(\"\\tMini-batch size:\", batch_size)\n",
    "    print(\"\\tLearning rate:\", learning_rate)\n",
    "    print(\"\\tMomentum:\", momentum)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    digits_training_accuracy = 0\n",
    "    digits_training_cost = 0\n",
    "    digits_validation_accuracy = 0\n",
    "    digits_validation_cost = 0\n",
    "    digits_runtime = 0\n",
    "    wine_runtime = 0\n",
    "    \n",
    "    print(\"Digits gradient descent:\")\n",
    "\n",
    "    for fold_index, fold in enumerate(xDigitsTrainingSets):\n",
    "        print(f\"\\tCross-validation fold {fold_index+1}\")\n",
    "\n",
    "        gradientDescentModel = GradientDescent(batch_size, learning_rate, momentum)\n",
    "        logisticRegressionModel = LogisticRegression(add_bias=True)\n",
    "\n",
    "        # Record amount of time taken for fit        \n",
    "        startTime_fit = int(round(time.time() * 1000)) # Milliseconds\n",
    "        logisticRegressionModel.fit(fold, yDigitsTrainingSets[fold_index], gradientDescentModel, xDigitsValidationSets[fold_index], yDigitsValidationSets[fold_index])\n",
    "        endTime_fit = -1 * (startTime_fit - int(round(time.time() * 1000))) # <------------\n",
    "        digits_runtime += endTime_fit\n",
    "        \n",
    "        yh_training = logisticRegressionModel.predict(xDigitsTrainingSets[fold_index])\n",
    "        yh_validation = logisticRegressionModel.predict(xDigitsValidationSets[fold_index])\n",
    "\n",
    "        for sample_index, yh_x in enumerate(yh_training):\n",
    "            if accurate(yh_x, yDigitsTrainingSets[fold_index][sample_index]):\n",
    "                digits_training_accuracy += 1\n",
    "            c = np.argmax(yDigitsTrainingSets[fold_index][sample_index])\n",
    "            cst = cost(yh_x[c], yDigitsTrainingSets[fold_index][sample_index][c])\n",
    "            digits_training_cost += cst\n",
    "\n",
    "        for sample_index, yh_x in enumerate(yh_validation):\n",
    "            if accurate(yh_x, yDigitsValidationSets[fold_index][sample_index]):\n",
    "                digits_validation_accuracy += 1\n",
    "            c = np.argmax(yDigitsValidationSets[fold_index][sample_index])\n",
    "            cst = cost(yh_x[c], yDigitsValidationSets[fold_index][sample_index][c])\n",
    "            digits_validation_cost += cst\n",
    "\n",
    "    digits_training_accuracy /= 4*len(digits.data)\n",
    "    digits_training_cost /= 4\n",
    "    digits_validation_accuracy /= len(digits.data)\n",
    "\n",
    "    wine_training_accuracy = 0\n",
    "    wine_training_cost = 0\n",
    "    wine_validation_accuracy = 0\n",
    "    wine_validation_cost = 0\n",
    "    \n",
    "    \n",
    "    print(\"Wine gradient descent:\")\n",
    "\n",
    "    for fold_index, fold in enumerate(xWineTrainingSets):\n",
    "        print(f\"\\tCross-validation fold {fold_index+1}\")\n",
    "\n",
    "        gradientDescentModel = GradientDescent(batch_size, learning_rate, momentum)\n",
    "        logisticRegressionModel = LogisticRegression(add_bias=True)\n",
    "\n",
    "        # Record amount of time taken for fit\n",
    "        startTime_fit = int(round(time.time() * 1000))\n",
    "        logisticRegressionModel.fit(fold, yWineTrainingSets[fold_index], gradientDescentModel, xWineValidationSets[fold_index], yWineValidationSets[fold_index])\n",
    "        endTime_fit = -1 * (startTime_fit - int(round(time.time() * 1000))) # <------------\n",
    "        wine_runtime += endTime_fit\n",
    "\n",
    "        \n",
    "        yh_training = logisticRegressionModel.predict(xWineTrainingSets[fold_index])\n",
    "        yh_validation = logisticRegressionModel.predict(xWineValidationSets[fold_index])\n",
    "\n",
    "        for sample_index, yh_x in enumerate(yh_training):\n",
    "            if accurate(yh_x, yWineTrainingSets[fold_index][sample_index]):\n",
    "                wine_training_accuracy += 1\n",
    "            c = np.argmax(yWineTrainingSets[fold_index][sample_index])\n",
    "            cst = cost(yh_x[c], yWineTrainingSets[fold_index][sample_index][c])\n",
    "            wine_training_cost += cst\n",
    "\n",
    "        for sample_index, yh_x in enumerate(yh_validation):\n",
    "            if accurate(yh_x, yWineValidationSets[fold_index][sample_index]):\n",
    "                wine_validation_accuracy += 1\n",
    "            c = np.argmax(yWineValidationSets[fold_index][sample_index])\n",
    "            cst = cost(yh_x[c], yWineValidationSets[fold_index][sample_index][c])\n",
    "            wine_validation_cost += cst\n",
    "\n",
    "    wine_training_accuracy /= 4*len(wine.data)\n",
    "    wine_training_cost /= 4\n",
    "    wine_validation_accuracy /= len(wine.data)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(f\"Digits training accuracy: {digits_training_accuracy*100:.1f}%\")\n",
    "    print(f\"Digits training cost: {digits_training_cost:.3f}\")\n",
    "    print(f\"Digits validation accuracy: {digits_validation_accuracy*100:.1f}%\")\n",
    "    print(f\"Digits validation cost: {digits_validation_cost:.3f}\")\n",
    "    print(f\"Wine training accuracy: {wine_training_accuracy*100:.1f}%\")\n",
    "    print(f\"Wine training cost: {wine_training_cost:.3f}\")\n",
    "    print(f\"Wine validation accuracy: {wine_validation_accuracy*100:.1f}%\")\n",
    "    print(f\"Wine validation cost: {wine_validation_cost:.3f}\\n\")\n",
    "    print(f'Fit times digits : {digits_runtime}\\n')\n",
    "    print(f'Fit times wines : {wine_runtime}\\n')\n",
    "\n",
    "    return (digits_validation_accuracy, digits_validation_cost, wine_validation_accuracy, wine_validation_cost, digits_runtime, wine_runtime)\n",
    "    #return (digits_validation_accuracy, digits_validation_cost, wine_validation_accuracy, wine_validation_cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hyper-parameters:\n",
      "\tMini-batch size: 30\n",
      "\tLearning rate: 0.04\n",
      "\tMomentum: 0.2\n",
      "\n",
      "\n",
      "Digits gradient descent:\n",
      "\tCross-validation fold 1\n",
      "\t\tStep 1: new best cost of 463.021\n",
      "\t\tStep 2: new best cost of 457.334\n",
      "\t\tStep 3: new best cost of 450.518\n",
      "\t\tStep 4: new best cost of 442.942\n",
      "\t\tStep 5: new best cost of 436.996\n",
      "\t\tStep 6: new best cost of 429.247\n",
      "\t\tStep 7: new best cost of 421.460\n",
      "\t\tStep 8: new best cost of 420.680\n",
      "\t\tStep 9: new best cost of 403.315\n",
      "\t\tStep 10: new best cost of 397.932\n",
      "\t\tStep 11: new best cost of 396.651\n",
      "\t\tStep 12\n",
      "\t\tStep 13: new best cost of 384.073\n",
      "\t\tStep 14\n",
      "\t\tStep 15: new best cost of 374.115\n",
      "\t\tStep 16: new best cost of 368.372\n",
      "\t\tStep 17: new best cost of 364.069\n",
      "\t\tStep 18: new best cost of 363.442\n",
      "\t\tStep 19: new best cost of 347.416\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22: new best cost of 343.066\n",
      "\t\tStep 23: new best cost of 342.136\n",
      "\t\tStep 24: new best cost of 340.491\n",
      "\t\tStep 25: new best cost of 338.990\n",
      "\t\tStep 26: new best cost of 324.262\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31: new best cost of 323.838\n",
      "\t\tStep 32: new best cost of 314.563\n",
      "\t\tStep 33\n",
      "\t\tStep 34: new best cost of 308.821\n",
      "\t\tStep 35\n",
      "\t\tStep 36: new best cost of 305.934\n",
      "\t\tStep 37\n",
      "\t\tStep 38: new best cost of 303.437\n",
      "\t\tStep 39: new best cost of 300.256\n",
      "\t\tStep 40: new best cost of 296.091\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44: new best cost of 295.202\n",
      "\t\tStep 45\n",
      "\t\tStep 46: new best cost of 285.641\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50: new best cost of 284.098\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54: new best cost of 282.924\n",
      "\t\tStep 55: new best cost of 281.077\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58: new best cost of 279.090\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61: new best cost of 276.516\n",
      "\t\tStep 62: new best cost of 275.405\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65: new best cost of 271.288\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69: new best cost of 271.193\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72: new best cost of 270.085\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76: new best cost of 268.425\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80: new best cost of 267.666\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84: new best cost of 266.885\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88: new best cost of 265.245\n",
      "\t\tStep 89: new best cost of 265.192\n",
      "\t\tStep 90: new best cost of 263.576\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108: new best cost of 263.453\n",
      "\t\tStep 109\n",
      "\t\tStep 110: new best cost of 260.650\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113: new best cost of 259.962\n",
      "\t\tStep 114: new best cost of 259.769\n",
      "\t\tStep 115\n",
      "\t\tStep 116\n",
      "\t\tStep 117: new best cost of 258.878\n",
      "\t\tStep 118\n",
      "\t\tStep 119\n",
      "\t\tStep 120: new best cost of 258.451\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "\t\tStep 126\n",
      "\t\tStep 127\n",
      "\t\tStep 128\n",
      "\t\tStep 129\n",
      "\t\tStep 130\n",
      "\t\tStep 131\n",
      "\t\tStep 132\n",
      "\t\tStep 133\n",
      "\t\tStep 134\n",
      "\t\tStep 135\n",
      "\t\tStep 136\n",
      "\t\tStep 137\n",
      "\t\tStep 138\n",
      "\t\tStep 139\n",
      "\t\tStep 140\n",
      "\t\tStep 141\n",
      "\t\tStep 142\n",
      "\t\tStep 143\n",
      "\t\tStep 144\n",
      "\t\tStep 145\n",
      "\tCross-validation fold 2\n",
      "\t\tStep 1: new best cost of 457.088\n",
      "\t\tStep 2\n",
      "\t\tStep 3: new best cost of 454.200\n",
      "\t\tStep 4: new best cost of 443.977\n",
      "\t\tStep 5: new best cost of 437.642\n",
      "\t\tStep 6: new best cost of 431.692\n",
      "\t\tStep 7: new best cost of 419.510\n",
      "\t\tStep 8: new best cost of 418.144\n",
      "\t\tStep 9: new best cost of 411.423\n",
      "\t\tStep 10: new best cost of 408.242\n",
      "\t\tStep 11: new best cost of 397.185\n",
      "\t\tStep 12\n",
      "\t\tStep 13: new best cost of 396.746\n",
      "\t\tStep 14: new best cost of 380.652\n",
      "\t\tStep 15: new best cost of 376.230\n",
      "\t\tStep 16: new best cost of 374.303\n",
      "\t\tStep 17\n",
      "\t\tStep 18: new best cost of 367.980\n",
      "\t\tStep 19: new best cost of 367.150\n",
      "\t\tStep 20: new best cost of 356.000\n",
      "\t\tStep 21: new best cost of 351.657\n",
      "\t\tStep 22: new best cost of 350.411\n",
      "\t\tStep 23: new best cost of 348.057\n",
      "\t\tStep 24\n",
      "\t\tStep 25: new best cost of 341.847\n",
      "\t\tStep 26\n",
      "\t\tStep 27: new best cost of 341.626\n",
      "\t\tStep 28: new best cost of 333.512\n",
      "\t\tStep 29\n",
      "\t\tStep 30: new best cost of 331.856\n",
      "\t\tStep 31: new best cost of 329.092\n",
      "\t\tStep 32: new best cost of 328.192\n",
      "\t\tStep 33: new best cost of 327.992\n",
      "\t\tStep 34: new best cost of 327.936\n",
      "\t\tStep 35\n",
      "\t\tStep 36: new best cost of 320.631\n",
      "\t\tStep 37: new best cost of 316.200\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40: new best cost of 312.325\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44: new best cost of 312.285\n",
      "\t\tStep 45: new best cost of 311.342\n",
      "\t\tStep 46: new best cost of 310.324\n",
      "\t\tStep 47\n",
      "\t\tStep 48: new best cost of 305.343\n",
      "\t\tStep 49: new best cost of 304.400\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54: new best cost of 301.984\n",
      "\t\tStep 55\n",
      "\t\tStep 56: new best cost of 299.932\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61: new best cost of 298.692\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69: new best cost of 293.532\n",
      "\t\tStep 70\n",
      "\t\tStep 71: new best cost of 292.626\n",
      "\t\tStep 72: new best cost of 291.881\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81: new best cost of 289.537\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84: new best cost of 288.877\n",
      "\t\tStep 85: new best cost of 285.837\n",
      "\t\tStep 86: new best cost of 285.228\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90: new best cost of 284.371\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105: new best cost of 283.012\n",
      "\t\tStep 106\n",
      "\t\tStep 107: new best cost of 283.006\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\t\tStep 114: new best cost of 281.714\n",
      "\t\tStep 115\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\t\tStep 118\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121: new best cost of 280.556\n",
      "\t\tStep 122\n",
      "\t\tStep 123\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "\t\tStep 126: new best cost of 279.987\n",
      "\t\tStep 127\n",
      "\t\tStep 128\n",
      "\t\tStep 129\n",
      "\t\tStep 130\n",
      "\t\tStep 131\n",
      "\t\tStep 132\n",
      "\t\tStep 133: new best cost of 279.129\n",
      "\t\tStep 134\n",
      "\t\tStep 135\n",
      "\t\tStep 136\n",
      "\t\tStep 137\n",
      "\t\tStep 138\n",
      "\t\tStep 139\n",
      "\t\tStep 140\n",
      "\t\tStep 141\n",
      "\t\tStep 142\n",
      "\t\tStep 143\n",
      "\t\tStep 144\n",
      "\t\tStep 145\n",
      "\t\tStep 146\n",
      "\t\tStep 147\n",
      "\t\tStep 148\n",
      "\t\tStep 149: new best cost of 278.649\n",
      "\t\tStep 150: new best cost of 276.888\n",
      "\t\tStep 151\n",
      "\t\tStep 152\n",
      "\t\tStep 153\n",
      "\t\tStep 154\n",
      "\t\tStep 155\n",
      "\t\tStep 156\n",
      "\t\tStep 157\n",
      "\t\tStep 158\n",
      "\t\tStep 159: new best cost of 276.373\n",
      "\t\tStep 160\n",
      "\t\tStep 161\n",
      "\t\tStep 162\n",
      "\t\tStep 163\n",
      "\t\tStep 164\n",
      "\t\tStep 165\n",
      "\t\tStep 166\n",
      "\t\tStep 167\n",
      "\t\tStep 168\n",
      "\t\tStep 169\n",
      "\t\tStep 170\n",
      "\t\tStep 171\n",
      "\t\tStep 172\n",
      "\t\tStep 173\n",
      "\t\tStep 174\n",
      "\t\tStep 175\n",
      "\t\tStep 176\n",
      "\t\tStep 177\n",
      "\t\tStep 178\n",
      "\t\tStep 179\n",
      "\t\tStep 180\n",
      "\t\tStep 181\n",
      "\t\tStep 182\n",
      "\t\tStep 183\n",
      "\t\tStep 184\n",
      "\tCross-validation fold 3\n",
      "\t\tStep 1: new best cost of 457.944\n",
      "\t\tStep 2: new best cost of 456.488\n",
      "\t\tStep 3: new best cost of 452.062\n",
      "\t\tStep 4: new best cost of 447.251\n",
      "\t\tStep 5: new best cost of 437.573\n",
      "\t\tStep 6\n",
      "\t\tStep 7: new best cost of 428.312\n",
      "\t\tStep 8: new best cost of 422.145\n",
      "\t\tStep 9: new best cost of 410.918\n",
      "\t\tStep 10: new best cost of 402.887\n",
      "\t\tStep 11: new best cost of 398.601\n",
      "\t\tStep 12\n",
      "\t\tStep 13: new best cost of 396.090\n",
      "\t\tStep 14: new best cost of 379.751\n",
      "\t\tStep 15\n",
      "\t\tStep 16: new best cost of 369.691\n",
      "\t\tStep 17: new best cost of 369.684\n",
      "\t\tStep 18\n",
      "\t\tStep 19: new best cost of 362.590\n",
      "\t\tStep 20\n",
      "\t\tStep 21: new best cost of 346.442\n",
      "\t\tStep 22: new best cost of 340.215\n",
      "\t\tStep 23\n",
      "\t\tStep 24: new best cost of 338.655\n",
      "\t\tStep 25: new best cost of 329.445\n",
      "\t\tStep 26: new best cost of 328.205\n",
      "\t\tStep 27: new best cost of 324.522\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30: new best cost of 317.230\n",
      "\t\tStep 31: new best cost of 314.903\n",
      "\t\tStep 32\n",
      "\t\tStep 33: new best cost of 312.274\n",
      "\t\tStep 34: new best cost of 308.677\n",
      "\t\tStep 35: new best cost of 306.283\n",
      "\t\tStep 36\n",
      "\t\tStep 37: new best cost of 304.726\n",
      "\t\tStep 38\n",
      "\t\tStep 39: new best cost of 302.628\n",
      "\t\tStep 40: new best cost of 300.406\n",
      "\t\tStep 41: new best cost of 299.070\n",
      "\t\tStep 42: new best cost of 297.915\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45: new best cost of 295.342\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49: new best cost of 290.630\n",
      "\t\tStep 50: new best cost of 290.150\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54: new best cost of 289.032\n",
      "\t\tStep 55: new best cost of 288.121\n",
      "\t\tStep 56: new best cost of 285.331\n",
      "\t\tStep 57: new best cost of 283.264\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62: new best cost of 283.257\n",
      "\t\tStep 63: new best cost of 280.192\n",
      "\t\tStep 64: new best cost of 277.365\n",
      "\t\tStep 65\n",
      "\t\tStep 66: new best cost of 275.092\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71: new best cost of 271.657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79: new best cost of 271.102\n",
      "\t\tStep 80: new best cost of 270.374\n",
      "\t\tStep 81: new best cost of 268.508\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87: new best cost of 267.198\n",
      "\t\tStep 88: new best cost of 266.820\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93: new best cost of 266.636\n",
      "\t\tStep 94: new best cost of 265.984\n",
      "\t\tStep 95: new best cost of 262.895\n",
      "\t\tStep 96: new best cost of 262.721\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104: new best cost of 261.214\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113: new best cost of 259.973\n",
      "\t\tStep 114\n",
      "\t\tStep 115\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\t\tStep 118\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "\t\tStep 126\n",
      "\t\tStep 127\n",
      "\t\tStep 128: new best cost of 258.642\n",
      "\t\tStep 129\n",
      "\t\tStep 130\n",
      "\t\tStep 131\n",
      "\t\tStep 132\n",
      "\t\tStep 133\n",
      "\t\tStep 134\n",
      "\t\tStep 135\n",
      "\t\tStep 136\n",
      "\t\tStep 137\n",
      "\t\tStep 138\n",
      "\t\tStep 139\n",
      "\t\tStep 140: new best cost of 258.289\n",
      "\t\tStep 141\n",
      "\t\tStep 142\n",
      "\t\tStep 143\n",
      "\t\tStep 144\n",
      "\t\tStep 145\n",
      "\t\tStep 146\n",
      "\t\tStep 147\n",
      "\t\tStep 148\n",
      "\t\tStep 149\n",
      "\t\tStep 150\n",
      "\t\tStep 151\n",
      "\t\tStep 152\n",
      "\t\tStep 153\n",
      "\t\tStep 154\n",
      "\t\tStep 155\n",
      "\t\tStep 156\n",
      "\t\tStep 157\n",
      "\t\tStep 158\n",
      "\t\tStep 159\n",
      "\t\tStep 160\n",
      "\t\tStep 161\n",
      "\t\tStep 162\n",
      "\t\tStep 163\n",
      "\t\tStep 164\n",
      "\t\tStep 165\n",
      "\tCross-validation fold 4\n",
      "\t\tStep 1: new best cost of 461.551\n",
      "\t\tStep 2: new best cost of 460.040\n",
      "\t\tStep 3: new best cost of 449.453\n",
      "\t\tStep 4: new best cost of 443.641\n",
      "\t\tStep 5: new best cost of 433.975\n",
      "\t\tStep 6: new best cost of 428.682\n",
      "\t\tStep 7: new best cost of 417.088\n",
      "\t\tStep 8\n",
      "\t\tStep 9: new best cost of 415.573\n",
      "\t\tStep 10: new best cost of 392.315\n",
      "\t\tStep 11\n",
      "\t\tStep 12: new best cost of 383.502\n",
      "\t\tStep 13\n",
      "\t\tStep 14: new best cost of 378.261\n",
      "\t\tStep 15: new best cost of 375.865\n",
      "\t\tStep 16: new best cost of 367.974\n",
      "\t\tStep 17: new best cost of 364.703\n",
      "\t\tStep 18: new best cost of 355.701\n",
      "\t\tStep 19: new best cost of 350.992\n",
      "\t\tStep 20: new best cost of 347.922\n",
      "\t\tStep 21: new best cost of 347.715\n",
      "\t\tStep 22: new best cost of 341.403\n",
      "\t\tStep 23\n",
      "\t\tStep 24: new best cost of 337.135\n",
      "\t\tStep 25: new best cost of 333.248\n",
      "\t\tStep 26: new best cost of 326.171\n",
      "\t\tStep 27: new best cost of 324.665\n",
      "\t\tStep 28: new best cost of 318.661\n",
      "\t\tStep 29: new best cost of 316.601\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32: new best cost of 311.817\n",
      "\t\tStep 33\n",
      "\t\tStep 34: new best cost of 309.241\n",
      "\t\tStep 35: new best cost of 306.034\n",
      "\t\tStep 36: new best cost of 304.625\n",
      "\t\tStep 37\n",
      "\t\tStep 38: new best cost of 296.795\n",
      "\t\tStep 39\n",
      "\t\tStep 40: new best cost of 296.104\n",
      "\t\tStep 41: new best cost of 290.782\n",
      "\t\tStep 42\n",
      "\t\tStep 43: new best cost of 289.863\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46: new best cost of 285.520\n",
      "\t\tStep 47: new best cost of 281.406\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52: new best cost of 281.396\n",
      "\t\tStep 53: new best cost of 276.698\n",
      "\t\tStep 54\n",
      "\t\tStep 55: new best cost of 273.601\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59: new best cost of 270.990\n",
      "\t\tStep 60\n",
      "\t\tStep 61: new best cost of 270.724\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65: new best cost of 270.525\n",
      "\t\tStep 66: new best cost of 269.871\n",
      "\t\tStep 67: new best cost of 269.313\n",
      "\t\tStep 68\n",
      "\t\tStep 69: new best cost of 266.897\n",
      "\t\tStep 70\n",
      "\t\tStep 71: new best cost of 264.468\n",
      "\t\tStep 72: new best cost of 263.397\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75: new best cost of 260.763\n",
      "\t\tStep 76: new best cost of 259.108\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82: new best cost of 257.999\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90: new best cost of 256.904\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106: new best cost of 256.377\n",
      "\t\tStep 107: new best cost of 255.426\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111: new best cost of 254.569\n",
      "\t\tStep 112: new best cost of 254.566\n",
      "\t\tStep 113\n",
      "\t\tStep 114: new best cost of 253.246\n",
      "\t\tStep 115: new best cost of 252.403\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\t\tStep 118: new best cost of 252.195\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123: new best cost of 251.173\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "\t\tStep 126\n",
      "\t\tStep 127\n",
      "\t\tStep 128\n",
      "\t\tStep 129\n",
      "\t\tStep 130\n",
      "\t\tStep 131\n",
      "\t\tStep 132: new best cost of 250.571\n",
      "\t\tStep 133\n",
      "\t\tStep 134\n",
      "\t\tStep 135\n",
      "\t\tStep 136\n",
      "\t\tStep 137\n",
      "\t\tStep 138\n",
      "\t\tStep 139\n",
      "\t\tStep 140\n",
      "\t\tStep 141\n",
      "\t\tStep 142\n",
      "\t\tStep 143\n",
      "\t\tStep 144\n",
      "\t\tStep 145\n",
      "\t\tStep 146\n",
      "\t\tStep 147\n",
      "\t\tStep 148\n",
      "\t\tStep 149: new best cost of 250.382\n",
      "\t\tStep 150: new best cost of 248.734\n",
      "\t\tStep 151\n",
      "\t\tStep 152\n",
      "\t\tStep 153\n",
      "\t\tStep 154\n",
      "\t\tStep 155\n",
      "\t\tStep 156\n",
      "\t\tStep 157\n",
      "\t\tStep 158\n",
      "\t\tStep 159\n",
      "\t\tStep 160\n",
      "\t\tStep 161\n",
      "\t\tStep 162\n",
      "\t\tStep 163\n",
      "\t\tStep 164\n",
      "\t\tStep 165\n",
      "\t\tStep 166\n",
      "\t\tStep 167\n",
      "\t\tStep 168\n",
      "\t\tStep 169\n",
      "\t\tStep 170\n",
      "\t\tStep 171\n",
      "\t\tStep 172\n",
      "\t\tStep 173\n",
      "\t\tStep 174\n",
      "\t\tStep 175\n",
      "\tCross-validation fold 5\n",
      "\t\tStep 1: new best cost of 461.066\n",
      "\t\tStep 2: new best cost of 456.657\n",
      "\t\tStep 3: new best cost of 450.436\n",
      "\t\tStep 4: new best cost of 441.254\n",
      "\t\tStep 5: new best cost of 436.356\n",
      "\t\tStep 6: new best cost of 429.516\n",
      "\t\tStep 7: new best cost of 420.778\n",
      "\t\tStep 8: new best cost of 414.112\n",
      "\t\tStep 9: new best cost of 411.032\n",
      "\t\tStep 10: new best cost of 407.046\n",
      "\t\tStep 11: new best cost of 394.762\n",
      "\t\tStep 12: new best cost of 387.735\n",
      "\t\tStep 13: new best cost of 383.507\n",
      "\t\tStep 14: new best cost of 377.607\n",
      "\t\tStep 15: new best cost of 370.945\n",
      "\t\tStep 16: new best cost of 370.839\n",
      "\t\tStep 17: new best cost of 363.645\n",
      "\t\tStep 18: new best cost of 359.993\n",
      "\t\tStep 19: new best cost of 354.394\n",
      "\t\tStep 20\n",
      "\t\tStep 21: new best cost of 350.518\n",
      "\t\tStep 22: new best cost of 343.716\n",
      "\t\tStep 23\n",
      "\t\tStep 24: new best cost of 332.255\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27: new best cost of 326.718\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30: new best cost of 319.979\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33: new best cost of 317.269\n",
      "\t\tStep 34: new best cost of 313.988\n",
      "\t\tStep 35: new best cost of 310.774\n",
      "\t\tStep 36: new best cost of 306.729\n",
      "\t\tStep 37: new best cost of 302.787\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42: new best cost of 300.498\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45: new best cost of 297.889\n",
      "\t\tStep 46: new best cost of 294.516\n",
      "\t\tStep 47\n",
      "\t\tStep 48: new best cost of 291.411\n",
      "\t\tStep 49: new best cost of 290.968\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58: new best cost of 289.140\n",
      "\t\tStep 59: new best cost of 286.219\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63: new best cost of 283.664\n",
      "\t\tStep 64\n",
      "\t\tStep 65: new best cost of 283.241\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68: new best cost of 280.083\n",
      "\t\tStep 69\n",
      "\t\tStep 70: new best cost of 279.821\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75: new best cost of 279.284\n",
      "\t\tStep 76: new best cost of 276.180\n",
      "\t\tStep 77\n",
      "\t\tStep 78: new best cost of 275.549\n",
      "\t\tStep 79: new best cost of 275.430\n",
      "\t\tStep 80: new best cost of 275.418\n",
      "\t\tStep 81\n",
      "\t\tStep 82: new best cost of 274.393\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85: new best cost of 272.665\n",
      "\t\tStep 86: new best cost of 271.408\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\t\tStep 101: new best cost of 270.475\n",
      "\t\tStep 102\n",
      "\t\tStep 103: new best cost of 268.081\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115: new best cost of 267.329\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\t\tStep 118\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121: new best cost of 266.500\n",
      "\t\tStep 122: new best cost of 265.569\n",
      "\t\tStep 123\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "\t\tStep 126: new best cost of 265.508\n",
      "\t\tStep 127\n",
      "\t\tStep 128\n",
      "\t\tStep 129\n",
      "\t\tStep 130\n",
      "\t\tStep 131\n",
      "\t\tStep 132\n",
      "\t\tStep 133\n",
      "\t\tStep 134\n",
      "\t\tStep 135\n",
      "\t\tStep 136\n",
      "\t\tStep 137\n",
      "\t\tStep 138: new best cost of 265.096\n",
      "\t\tStep 139\n",
      "\t\tStep 140\n",
      "\t\tStep 141\n",
      "\t\tStep 142\n",
      "\t\tStep 143\n",
      "\t\tStep 144\n",
      "\t\tStep 145\n",
      "\t\tStep 146\n",
      "\t\tStep 147\n",
      "\t\tStep 148\n",
      "\t\tStep 149\n",
      "\t\tStep 150\n",
      "\t\tStep 151\n",
      "\t\tStep 152\n",
      "\t\tStep 153\n",
      "\t\tStep 154\n",
      "\t\tStep 155\n",
      "\t\tStep 156\n",
      "\t\tStep 157\n",
      "\t\tStep 158\n",
      "\t\tStep 159\n",
      "\t\tStep 160: new best cost of 265.086\n",
      "\t\tStep 161\n",
      "\t\tStep 162\n",
      "\t\tStep 163\n",
      "\t\tStep 164\n",
      "\t\tStep 165\n",
      "\t\tStep 166\n",
      "\t\tStep 167\n",
      "\t\tStep 168\n",
      "\t\tStep 169\n",
      "\t\tStep 170: new best cost of 264.742\n",
      "\t\tStep 171: new best cost of 262.927\n",
      "\t\tStep 172\n",
      "\t\tStep 173\n",
      "\t\tStep 174\n",
      "\t\tStep 175\n",
      "\t\tStep 176\n",
      "\t\tStep 177\n",
      "\t\tStep 178\n",
      "\t\tStep 179\n",
      "\t\tStep 180\n",
      "\t\tStep 181\n",
      "\t\tStep 182\n",
      "\t\tStep 183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tStep 184\n",
      "\t\tStep 185\n",
      "\t\tStep 186\n",
      "\t\tStep 187\n",
      "\t\tStep 188\n",
      "\t\tStep 189\n",
      "\t\tStep 190\n",
      "\t\tStep 191\n",
      "\t\tStep 192\n",
      "\t\tStep 193\n",
      "\t\tStep 194\n",
      "\t\tStep 195: new best cost of 262.451\n",
      "\t\tStep 196: new best cost of 261.862\n",
      "\t\tStep 197\n",
      "\t\tStep 198\n",
      "\t\tStep 199\n",
      "\t\tStep 200\n",
      "\t\tStep 201\n",
      "\t\tStep 202\n",
      "\t\tStep 203\n",
      "\t\tStep 204\n",
      "\t\tStep 205\n",
      "\t\tStep 206\n",
      "\t\tStep 207\n",
      "\t\tStep 208\n",
      "\t\tStep 209\n",
      "\t\tStep 210\n",
      "\t\tStep 211: new best cost of 261.424\n",
      "\t\tStep 212\n",
      "\t\tStep 213\n",
      "\t\tStep 214\n",
      "\t\tStep 215\n",
      "\t\tStep 216\n",
      "\t\tStep 217\n",
      "\t\tStep 218\n",
      "\t\tStep 219\n",
      "\t\tStep 220\n",
      "\t\tStep 221\n",
      "\t\tStep 222\n",
      "\t\tStep 223\n",
      "\t\tStep 224\n",
      "\t\tStep 225\n",
      "\t\tStep 226\n",
      "\t\tStep 227\n",
      "\t\tStep 228\n",
      "\t\tStep 229\n",
      "\t\tStep 230\n",
      "\t\tStep 231\n",
      "\t\tStep 232\n",
      "\t\tStep 233\n",
      "\t\tStep 234\n",
      "\t\tStep 235\n",
      "\t\tStep 236\n",
      "Wine gradient descent:\n",
      "\tCross-validation fold 1\n",
      "\t\tStep 1: new best cost of 47.226\n",
      "\t\tStep 2: new best cost of 41.455\n",
      "\t\tStep 3\n",
      "\t\tStep 4: new best cost of 41.316\n",
      "\t\tStep 5: new best cost of 38.242\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8\n",
      "\t\tStep 9\n",
      "\t\tStep 10\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23: new best cost of 35.247\n",
      "\t\tStep 24\n",
      "\t\tStep 25: new best cost of 34.351\n",
      "\t\tStep 26\n",
      "\t\tStep 27: new best cost of 31.425\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40: new best cost of 30.821\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53: new best cost of 26.757\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77: new best cost of 26.584\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99: new best cost of 26.502\n",
      "\t\tStep 100\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\t\tStep 118\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123\n",
      "\t\tStep 124\n",
      "\tCross-validation fold 2\n",
      "\t\tStep 1: new best cost of 39.479\n",
      "\t\tStep 2\n",
      "\t\tStep 3\n",
      "\t\tStep 4: new best cost of 38.104\n",
      "\t\tStep 5\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8\n",
      "\t\tStep 9: new best cost of 35.642\n",
      "\t\tStep 10\n",
      "\t\tStep 11: new best cost of 34.777\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17: new best cost of 30.740\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28: new best cost of 30.299\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34: new best cost of 28.905\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51: new best cost of 28.750\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54: new best cost of 27.578\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65: new best cost of 27.402\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79: new best cost of 27.301\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87: new best cost of 25.825\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\tCross-validation fold 3\n",
      "\t\tStep 1: new best cost of 45.147\n",
      "\t\tStep 2: new best cost of 42.300\n",
      "\t\tStep 3\n",
      "\t\tStep 4: new best cost of 39.560\n",
      "\t\tStep 5\n",
      "\t\tStep 6: new best cost of 39.378\n",
      "\t\tStep 7\n",
      "\t\tStep 8\n",
      "\t\tStep 9\n",
      "\t\tStep 10\n",
      "\t\tStep 11: new best cost of 38.067\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19: new best cost of 31.243\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34: new best cost of 28.342\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49: new best cost of 26.828\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\tCross-validation fold 4\n",
      "\t\tStep 1: new best cost of 38.276\n",
      "\t\tStep 2\n",
      "\t\tStep 3\n",
      "\t\tStep 4\n",
      "\t\tStep 5\n",
      "\t\tStep 6: new best cost of 38.114\n",
      "\t\tStep 7\n",
      "\t\tStep 8: new best cost of 37.598\n",
      "\t\tStep 9: new best cost of 34.438\n",
      "\t\tStep 10\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13: new best cost of 30.288\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17: new best cost of 29.408\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24: new best cost of 29.358\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37: new best cost of 29.202\n",
      "\t\tStep 38\n",
      "\t\tStep 39: new best cost of 26.963\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51: new best cost of 26.599\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59: new best cost of 26.107\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76: new best cost of 26.105\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98: new best cost of 25.433\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\t\tStep 118\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123\n",
      "\tCross-validation fold 5\n",
      "\t\tStep 1: new best cost of 47.572\n",
      "\t\tStep 2: new best cost of 46.447\n",
      "\t\tStep 3\n",
      "\t\tStep 4\n",
      "\t\tStep 5\n",
      "\t\tStep 6: new best cost of 45.014\n",
      "\t\tStep 7\n",
      "\t\tStep 8: new best cost of 44.076\n",
      "\t\tStep 9\n",
      "\t\tStep 10: new best cost of 41.952\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22: new best cost of 38.155\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31: new best cost of 33.479\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\n",
      "\n",
      "Digits training accuracy: 94.9%\n",
      "Digits training cost: 1247.123\n",
      "Digits validation accuracy: 91.8%\n",
      "Digits validation cost: 1303.271\n",
      "Wine training accuracy: 86.1%\n",
      "Wine training cost: 141.256\n",
      "Wine validation accuracy: 88.8%\n",
      "Wine validation cost: 138.068\n",
      "\n",
      "Fit times digits : 121318\n",
      "\n",
      "Fit times wines : 1851\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default_run = runLogisticRegression(30, 0.04, 0.2)\n",
    "fitTimes = default_run[4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean = lambda x1, x2: np.sqrt(np.sum((x1 - x2)**2, axis=-1))\n",
    "manhattan = lambda x1, x2: np.sum(np.abs(x1 - x2), axis=-1)\n",
    "\n",
    "class KNN:\n",
    "\n",
    "    def __init__(self, K=1, dist_fn= euclidean):\n",
    "        self.dist_fn = dist_fn\n",
    "        self.K = K\n",
    "        return\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.C = len(y[0])\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        num_test = x_test.shape[0]\n",
    "        distances = self.dist_fn(self.x[None,:,:], x_test[:,None,:])\n",
    "        knns = np.zeros((num_test, self.K), dtype=int)\n",
    "        y_prob = np.zeros((num_test),dtype=int)\n",
    "        counts = np.zeros((num_test, self.C))\n",
    "        \n",
    "        for i in range(num_test):\n",
    "            knns[i,:] = np.argsort(distances[i])[:self.K]\n",
    "            k_count=np.zeros(self.K, dtype=int)\n",
    "            \n",
    "            for s, arr in enumerate(self.y[knns[i,:]]):\n",
    "                k_count[s] = np.argmax(arr)\n",
    "            \n",
    "            y_prob_i, counts_i = np.unique(k_count, return_counts=True)\n",
    "            y_prob[i] = int(y_prob_i[np.argmax(counts_i)])\n",
    "        \n",
    "        return y_prob, knns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN digits validation accuracy: 95.7%\n",
      "KNN wine validation accuracy: 85.4%\n"
     ]
    }
   ],
   "source": [
    "KNNmodel = KNN(K=11)\n",
    "\n",
    "digits_knn_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    y_prob, knns = KNNmodel.fit(np.asarray(xDigitsTrainingSets[fold]), np.asarray(yDigitsTrainingSets[fold])).predict(np.asarray(xDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, prob in enumerate(y_prob):\n",
    "        if prob == np.argmax(yDigitsValidationSets[fold][i]):\n",
    "            digits_knn_accuracy += 1\n",
    "\n",
    "digits_knn_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"KNN digits validation accuracy: {digits_knn_accuracy*100:.1f}%\")\n",
    "\n",
    "KNNmodel = KNN(K=7)\n",
    "\n",
    "wine_knn_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    y_prob, knns = KNNmodel.fit(np.asarray(xWineTrainingSets[fold]), np.asarray(yWineTrainingSets[fold])).predict(np.asarray(xWineValidationSets[fold]))\n",
    "    \n",
    "    for i, prob in enumerate(y_prob):\n",
    "        if prob == np.argmax(yWineValidationSets[fold][i]):\n",
    "            wine_knn_accuracy += 1\n",
    "            \n",
    "wine_knn_accuracy /= len(wine.data)\n",
    "\n",
    "print(f\"KNN wine validation accuracy: {wine_knn_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive base digits validation accuracy: 81.1%\n",
      "Naive base wine validation accuracy: 93.8%\n"
     ]
    }
   ],
   "source": [
    "digits_naive_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    labels_training = np.zeros(len(yDigitsTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(np.asarray(xDigitsTrainingSets[fold]), labels_training).predict(np.asarray(xDigitsValidationSets[fold]))\n",
    "\n",
    "    for i, label in enumerate(y_pred):\n",
    "        if label == labels_validation[i]:\n",
    "            digits_naive_accuracy += 1\n",
    "\n",
    "digits_naive_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"Naive base digits validation accuracy: {digits_naive_accuracy*100:.1f}%\")\n",
    "\n",
    "wine_naive_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    labels_training = np.zeros(len(yWineTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yWineTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yWineValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yWineValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(np.asarray(xWineTrainingSets[fold]), labels_training).predict(np.asarray(xWineValidationSets[fold]))\n",
    "\n",
    "    for i, label in enumerate(y_pred):\n",
    "        if label == labels_validation[i]:\n",
    "            wine_naive_accuracy += 1\n",
    "\n",
    "wine_naive_accuracy /= len(wine.data)\n",
    "\n",
    "print(f\"Naive base wine validation accuracy: {wine_naive_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HoGFeatures(img, cellSize, blockSize, nbins):\n",
    "    cell_size = (cellSize, cellSize)\n",
    "    block_size = (blockSize, blockSize)\n",
    "    \n",
    "    hog = cv.HOGDescriptor(_winSize=(img.shape[1] // cell_size[1] * cell_size[1],\n",
    "                                     img.shape[0] // cell_size[0] * cell_size[0]),\n",
    "                           _blockSize=(block_size[1] * cell_size[1],\n",
    "                                       block_size[0] * cell_size[0]),\n",
    "                           _blockStride=(cell_size[1], cell_size[0]),\n",
    "                           _cellSize=(cell_size[1], cell_size[0]),\n",
    "                           _nbins=nbins\n",
    "    )\n",
    "    \n",
    "    return hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeHoGFeatures(imageArray):\n",
    "    HoG = HoGFeatures(imageArray[0], 2, 2, 2)\n",
    "    features = []\n",
    "    \n",
    "    for i, image in enumerate(imageArray):\n",
    "        features.append(HoG.compute((image*255).astype(np.uint8)))\n",
    "        \n",
    "    features = np.array(np.squeeze(features))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC digits validation accuracy: 89.6%\n"
     ]
    }
   ],
   "source": [
    "digits_svc_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    numbers_training = []\n",
    "    numbers_validation = []\n",
    "    \n",
    "    for i, number in enumerate(xDigitsTrainingSets[fold]):\n",
    "        numbers_training.append(np.asarray(number).reshape(8, 8))\n",
    "        \n",
    "    for i, number in enumerate(xDigitsValidationSets[fold]):\n",
    "        numbers_validation.append(np.asarray(number).reshape(8, 8))\n",
    "        \n",
    "    HoGs_training = makeHoGFeatures(np.asarray(numbers_training))\n",
    "    HoGs_validation = makeHoGFeatures(np.asarray(numbers_validation))\n",
    "\n",
    "    clf = svm.SVC(gamma='auto', C=100)\n",
    "    \n",
    "    labels_training = np.zeros(len(yDigitsTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    clf.fit(HoGs_training, labels_training)\n",
    "\n",
    "    labels_predicted = clf.predict(HoGs_validation)\n",
    "    \n",
    "    for i, label in enumerate(labels_predicted):\n",
    "        if label == labels_validation[i]:\n",
    "            digits_svc_accuracy += 1\n",
    "\n",
    "digits_svc_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"SVC digits validation accuracy: {digits_svc_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Search\n",
    "##### Will not run if \"run_search=False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis of model hyper-parameters\n",
    "\n",
    "batch_size = 30\n",
    "learning_rate = 0.04\n",
    "momentum = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis of changing batch size\n",
    "if run_search:\n",
    "    batch_size_tests = np.array(range(2, 40, 2))#[30, 25, 20, 15, 10, 5]\n",
    "    batch_size_results = []\n",
    "    \n",
    "    for test in batch_size_tests:\n",
    "        batch_size_results.append(runLogisticRegression(test, learning_rate, momentum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of changing learning rate\n",
    "if run_search:\n",
    "    learning_rate_tests = np.array(range(1, 20, 1))/100#[0.02, 0.04, 0.06, 0.08]\n",
    "    learning_rate_results = []\n",
    "    \n",
    "    for test in learning_rate_tests:\n",
    "        learning_rate_results.append(runLogisticRegression(batch_size, test, momentum))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of changing momentum\n",
    "if run_search:\n",
    "    momentum_tests = np.array(range(1, 20, 1))/20#[0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "    momentum_results = []\n",
    "\n",
    "    for test in momentum_tests:\n",
    "        momentum_results.append(runLogisticRegression(batch_size, learning_rate, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(batch_size_results[5][2])\n",
    "\n",
    "if run_search:\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_size_inches(7,4)\n",
    "\n",
    "    ax1.plot(batch_size_tests, np.asarray(batch_size_results).T[0], label = f\"Digits validation accuracy\", color='green',linewidth=4)\n",
    "    ax1.plot(batch_size_tests, np.asarray(batch_size_results).T[2], label = f\"Wine validation accuracy\", color='limegreen',linewidth=2)\n",
    "    ax1.set_ylabel('Validation Accuracy',color='green',fontsize=13)\n",
    "    ax1.tick_params(axis='y', labelcolor='green', labelsize=13)\n",
    "    ax1.set_xlabel('Batch Size',fontsize=13)\n",
    "    ax2 = ax1.twinx() \n",
    "\n",
    "    ax2.plot(batch_size_tests, np.asarray(batch_size_results).T[1]/len(digits.data), label = f\"Digits cost\", color='red',linewidth=4)\n",
    "    ax2.plot(batch_size_tests, np.asarray(batch_size_results).T[3]/len(wine.data), label = f\"Wine cost\", color='maroon',linewidth=2)\n",
    "    ax2.set_ylabel('Cost per sample',color='maroon',fontsize=13)\n",
    "    ax2.tick_params(axis='y', labelcolor='maroon', labelsize=13)\n",
    "\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "    plt.legend(handles+handles2, labels+labels2,loc='lower right')\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(batch_size_results[5][2])\n",
    "\n",
    "if run_search:\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_size_inches(7,4)\n",
    "\n",
    "    ax1.plot(learning_rate_tests, np.asarray(learning_rate_results).T[0], label = f\"Digits validation accuracy\", color='green',linewidth=4)\n",
    "    ax1.plot(learning_rate_tests, np.asarray(learning_rate_results).T[2], label = f\"Wine validation accuracy\", color='limegreen',linewidth=2)\n",
    "    ax1.set_ylabel('Validation Accuracy',color='green',fontsize=13)\n",
    "    ax1.tick_params(axis='y', labelcolor='green', labelsize=13)\n",
    "    ax1.set_xlabel('Learning Rate',fontsize=13)\n",
    "    ax2 = ax1.twinx() \n",
    "\n",
    "    ax2.plot(learning_rate_tests, np.asarray(learning_rate_results).T[1]/len(digits.data), label = f\"Digits cost\", color='red',linewidth=4)\n",
    "    ax2.plot(learning_rate_tests, np.asarray(learning_rate_results).T[3]/len(wine.data), label = f\"Wine cost\", color='maroon',linewidth=2)\n",
    "    ax2.set_ylabel('Cost per sample',color='maroon',fontsize=13)\n",
    "    ax2.tick_params(axis='y', labelcolor='maroon', labelsize=13)\n",
    "\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "    plt.legend(handles+handles2, labels+labels2,loc='lower left')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(batch_size_results[5][2])\n",
    "\n",
    "if run_search:\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_size_inches(7,4)\n",
    "\n",
    "    ax1.plot(momentum_tests, np.asarray(momentum_results).T[0], label = f\"Digits validation accuracy\", color='green',linewidth=4)\n",
    "    ax1.plot(momentum_tests, np.asarray(momentum_results).T[2], label = f\"Wine validation accuracy\", color='limegreen',linewidth=2)\n",
    "    ax1.set_ylabel('Validation Accuracy',color='green',fontsize=13)\n",
    "    ax1.tick_params(axis='y', labelcolor='green', labelsize=13)\n",
    "    ax1.set_xlabel('Momentum',fontsize=13)\n",
    "    ax2 = ax1.twinx() \n",
    "\n",
    "    ax2.plot(momentum_tests, np.asarray(momentum_results).T[1]/len(digits.data), label = f\"Digits cost\", color='red',linewidth=4)\n",
    "    ax2.plot(momentum_tests, np.asarray(momentum_results).T[3]/len(wine.data), label = f\"Wine cost\", color='maroon',linewidth=2)\n",
    "    ax2.set_ylabel('Cost per sample',color='maroon',fontsize=13)\n",
    "    ax2.tick_params(axis='y', labelcolor='maroon', labelsize=13)\n",
    "\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "    plt.legend(handles+handles2, labels+labels2,loc='lower right')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_search:\n",
    "    from matplotlib.lines import Line2D\n",
    "    fig, ax = plt.subplots(2,3)\n",
    "    fig.set_figheight(6)\n",
    "    fig.set_figwidth(11)\n",
    "    fig.suptitle('Comparison of Runtimes for Fit Algorithm with Varying Hyperparameters')\n",
    "    \n",
    "    # Batch size runtime : digits\n",
    "    ax[0,0].plot(\n",
    "        batch_size_tests,\n",
    "        np.asarray(batch_size_results).T[4]/100,\n",
    "        color='green',linewidth=5)\n",
    "    ax[0,0].set_title('Batch size runtime')\n",
    "    ax[0,0].set_ylabel('Runtime (seconds)')\n",
    "    \n",
    "    # Learning rate runtimes : digits\n",
    "    ax[0,1].plot(\n",
    "        learning_rate_tests,\n",
    "        np.asarray(learning_rate_results).T[4]/100,\n",
    "        color='green',linewidth=5)\n",
    "    ax[0,1].set_title('Learning rate runtime')\n",
    "\n",
    "    \n",
    "    # Momentum runtimes: digits\n",
    "    ax[0,2].plot(\n",
    "        momentum_tests,\n",
    "        np.asarray(momentum_results).T[4]/100,\n",
    "        color='green',linewidth=5)\n",
    "    ax[0,2].set_title('Momentum runtime')\n",
    "\n",
    "    \n",
    "    # Batch size runtime: wines\n",
    "    ax[1,0].plot(\n",
    "        batch_size_tests,\n",
    "        np.asarray(batch_size_results).T[5]/100,\n",
    "        color='red',linewidth=5)\n",
    "    ax[1,0].set_ylabel('Runtime (seconds)')\n",
    "    ax[1,0].set_xlabel('Batch size')\n",
    "\n",
    "    \n",
    "    # Learning rate runtimes : wines\n",
    "    ax[1,1].plot(\n",
    "        learning_rate_tests,\n",
    "        np.asarray(learning_rate_results).T[5]/100,\n",
    "        color='red',linewidth=5)\n",
    "    ax[1,1].set_xlabel('Learning rate')\n",
    "\n",
    "    \n",
    "    # Momentum runtimes: wines\n",
    "    ax[1,2].plot(\n",
    "        momentum_tests,\n",
    "        np.asarray(momentum_results).T[5]/100,\n",
    "        color='red',linewidth=5)\n",
    "    ax[1,2].set_xlabel('Momentum')\n",
    "\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], color='g', lw=4, label='digits'),\n",
    "                       Line2D([0], [0], color='r', lw=4, label='wines')]\n",
    "    fig.legend(handles=legend_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Comparisson "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits_validation_accuracies = [default_run[0],digits_knn_accuracy,digits_naive_accuracy,digits_svc_accuracy]\n",
    "wine_validation_accuracies = [default_run[2],wine_knn_accuracy,wine_naive_accuracy,0]\n",
    "\n",
    "labels = [\"Logistic Regression\",\"KNN\",\"Naive Bayes\",\"SVC\"]\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, digits_validation_accuracies, width, label='Digits')\n",
    "rects2 = ax.bar(x + width/2, wine_validation_accuracies, width, label='Wine')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Validation Accuracies')\n",
    "ax.set_title('Comparison between different classifiers')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.set_size_inches(7,3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
