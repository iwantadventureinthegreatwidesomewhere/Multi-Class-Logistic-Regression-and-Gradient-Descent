{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lia\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import cv2 as cv\n",
    "from sklearn import svm \n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "wine = fetch_openml(name='wine', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of wine data via division of each feature by its max value\n",
    "digits_data_norm = []\n",
    "for col in digits.data:\n",
    "    col_norm = col/np.max(col)\n",
    "    digits_data_norm.append(col_norm)\n",
    "\n",
    "digits.data = np.asarray(digits_data_norm)\n",
    "\n",
    "\n",
    "wine_data_norm = []\n",
    "for col in wine.data.T:\n",
    "    col_norm = col/np.amax(col)\n",
    "    wine_data_norm.append(col_norm)\n",
    "    \n",
    "wine.data = np.asarray(wine_data_norm).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation for digits dataset\n",
    "\n",
    "digitsTrainingSetSize = int(np.ceil(0.8 * len(digits.data)))\n",
    "digitsValidationSetSize = int(len(digits.data) - digitsTrainingSetSize)\n",
    "\n",
    "xDigitsTrainingSets = []\n",
    "yDigitsTrainingSets = []\n",
    "xDigitsValidationSets = []\n",
    "yDigitsValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data[foldIndex*digitsValidationSetSize:((foldIndex*digitsValidationSetSize)+digitsValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(digits.target[index+(foldIndex*digitsValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(digits.target[index])\n",
    "            \n",
    "    xDigitsTrainingSets.append(xTrainingSet)\n",
    "    yDigitsTrainingSets.append(yTrainingSet)\n",
    "    xDigitsValidationSets.append(xValidationSet)\n",
    "    yDigitsValidationSets.append(yValidationSet)\n",
    "    \n",
    "# 5-fold cross validation for wine dataset\n",
    "\n",
    "wineTrainingSetSize = int(np.ceil(0.8 * len(wine.data)))\n",
    "wineValidationSetSize = int(len(wine.data) - wineTrainingSetSize)\n",
    "\n",
    "xWineTrainingSets = []\n",
    "yWineTrainingSets = []\n",
    "xWineValidationSets = []\n",
    "yWineValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "    for index, data in enumerate(wine.data[foldIndex*wineValidationSetSize:((foldIndex*wineValidationSetSize)+wineValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(wine.target[index+(foldIndex*wineValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "    \n",
    "    for index, data in enumerate(wine.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(wine.target[index])\n",
    "            \n",
    "    xWineTrainingSets.append(xTrainingSet)\n",
    "    yWineTrainingSets.append(yTrainingSet)\n",
    "    xWineValidationSets.append(xValidationSet)\n",
    "    yWineValidationSets.append(yValidationSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of y for digits dataset\n",
    "\n",
    "numberOfDigitsTargets = 10\n",
    "numberOfWineTargets = 3\n",
    "\n",
    "for index, fold in enumerate(yDigitsTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yDigitsValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsValidationSets[index] = encodedFold\n",
    "\n",
    "# one-hot encoding of y for wine dataset\n",
    "\n",
    "for index, fold in enumerate(yWineTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yWineValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineValidationSets[index] = encodedFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomIndices(arr, batch_size):\n",
    "    indices = []\n",
    "    \n",
    "    if batch_size > len(arr):\n",
    "        print(\"Error: batch size larger than size of dataset.\")\n",
    "        return\n",
    "    \n",
    "    while batch_size > 0:\n",
    "        x = np.floor(np.random.random() * len(arr))\n",
    "        if x not in indices:\n",
    "            indices.append(int(x))\n",
    "            batch_size -= 1\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent class\n",
    " \n",
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, batch_size, learning_rate=0.5, momentum=0.9, max_termination=10, max_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        self.max_termination = max_termination\n",
    "        self.max_iterations = max_iterations\n",
    "        self.deltas = []\n",
    "        \n",
    "    def run(self, gradient_fn, x, y, w):\n",
    "        t = 1\n",
    "        \n",
    "        min_cost = np.inf\n",
    "        termination_count = 0        \n",
    "        weight_history = []\n",
    "        error_history = []\n",
    "                \n",
    "        for number_of_targets in range(len(y[0])):\n",
    "            weight_history.append([])\n",
    "        \n",
    "        while termination_count < self.max_termination and t < self.max_iterations:\n",
    "            \n",
    "            gradients = gradient_fn(x, y, w, self.batch_size)   \n",
    "            \n",
    "            for c in range(len(y[0])):\n",
    "                if(t==1):\n",
    "                    w[c] = w[c] - self.learning_rate * gradients[c]\n",
    "                else:\n",
    "                    delta_w = (self.momentum)*(self.deltas[-(len(y[0]))]) + (1-self.momentum)*gradients[c]\n",
    "                    w[c] = w[c] - (self.learning_rate)*(delta_w)\n",
    "                self.deltas.append(w[c])\n",
    "            \n",
    "            a = np.asarray(x)\n",
    "            b = np.asarray(w)\n",
    "\n",
    "#             if self.add_bias:\n",
    "#                 x = np.column_stack([x,np.ones(N)])\n",
    "    \n",
    "            yh=[]\n",
    "            for i, x_c in enumerate(a):\n",
    "                yh_x=[]\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  b[c] @ x_c\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  b[i] @ x_c\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "                    yh_x.append(yh_c)\n",
    "                yh.append(yh_x)\n",
    "                \n",
    "            step_cost = 0\n",
    "                \n",
    "            def cost(yh, y):\n",
    "                return y * np.log1p(np.exp(-yh)) + (1-yh) * np.log1p(np.exp(yh))\n",
    "                \n",
    "            for sample_index, yh_x in enumerate(yh):\n",
    "                c = np.argmax(y[sample_index])\n",
    "                cst = cost(yh_x[c], y[sample_index][c])\n",
    "                step_cost += cst\n",
    "            \n",
    "            for c in range(len(b)):\n",
    "                weight_history[c].append(w[c])\n",
    "                \n",
    "            error_history.append(step_cost)\n",
    "            \n",
    "            # We found an alternate termination condition that terminates faster as\n",
    "            # the suggested condition would run for a longtime for us (~1hr).\n",
    "            \n",
    "            # We track the lowest step_cost encountered, and if the \n",
    "            # next max_termination-number of steps do not have a better cost, then\n",
    "            # it terminates.\n",
    "            \n",
    "            if step_cost < min_cost:\n",
    "                min_cost = step_cost\n",
    "                termination_count = 0\n",
    "                print(f\"\\t\\tStep {t}: new best cost of {min_cost:.3f}\")\n",
    "            else:\n",
    "                termination_count += 1\n",
    "                print(f\"\\t\\tStep {t}\")\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        # take the weight prior to the last max_termination-number of weights\n",
    "        # (as it is guaranteed to be the best prior to termination).\n",
    "        \n",
    "        index_best = len(error_history)-self.max_termination-1\n",
    "        \n",
    "        w_best = []\n",
    "        \n",
    "        for c in range(len(y[0])):\n",
    "            w_best.append(weight_history[c][index_best])\n",
    "        \n",
    "        return w_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, add_bias=True):\n",
    "        self.add_bias = add_bias\n",
    "        pass\n",
    "            \n",
    "    def fit(self, x, y, optimizer):\n",
    "        # TODO: add bias\n",
    "        \n",
    "        def gradient(x, y, w, batch_size):\n",
    "            gradients = np.zeros(len(w)).tolist()\n",
    "\n",
    "            indices = getRandomIndices(x, batch_size)\n",
    "\n",
    "            for index in indices:\n",
    "                a = np.asarray(x[index])\n",
    "                b = np.asarray(y[index])\n",
    "\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  w[c] @ a\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  w[i] @ a\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "\n",
    "                    y_c = b[c]\n",
    "                    \n",
    "                    cost_c = np.dot(yh_c - y_c, a)\n",
    "                    \n",
    "                    gradients[c] += cost_c\n",
    "\n",
    "            return gradients\n",
    "        \n",
    "        if self.add_bias:\n",
    "            print(x[0])\n",
    "            x = np.asarray(x)\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "            print(x[0])\n",
    "\n",
    "        w0 = []\n",
    "        for c in range(len(y[0])):\n",
    "            w0.append(np.zeros(len(x[0])))\n",
    "            \n",
    "        self.w = optimizer.run(gradient, x, y, w0)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \n",
    "        if self.add_bias:\n",
    "            x = np.asarray(x)\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "\n",
    "        a = np.asarray(x)\n",
    "        b = np.asarray(self.w)\n",
    "        \n",
    "#       if self.add_bias:\n",
    "#           x = np.column_stack([x,np.ones(N)])\n",
    "\n",
    "        yh=[]\n",
    "        for i, x_c in enumerate(a):\n",
    "            yh_x=[]\n",
    "            for c in range(len(b)):\n",
    "                w_x =  b[c] @ x_c\n",
    "                num = np.exp(w_x)\n",
    "\n",
    "                den = 0\n",
    "                for i in range(len(b)):\n",
    "                    w_x =  b[i] @ x_c\n",
    "                    den += np.exp(w_x)\n",
    "\n",
    "                yh_c = num/den\n",
    "                yh_x.append(yh_c)\n",
    "            yh.append(yh_x)\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hyper-parameters:\n",
      "\tMini-batch size: 30\n",
      "\tLearning rate: 0.08\n",
      "\tMomentum: 0.2\n",
      "\n",
      "\n",
      "Digits gradient descent:\n",
      "\tCross-validation fold 1\n",
      "\t\tStep 1: new best cost of 1813.667\n",
      "\t\tStep 2: new best cost of 1749.120\n",
      "\t\tStep 3: new best cost of 1717.970\n",
      "\t\tStep 4: new best cost of 1631.516\n",
      "\t\tStep 5: new best cost of 1596.680\n",
      "\t\tStep 6\n",
      "\t\tStep 7: new best cost of 1567.951\n",
      "\t\tStep 8: new best cost of 1509.268\n",
      "\t\tStep 9: new best cost of 1404.830\n",
      "\t\tStep 10\n",
      "\t\tStep 11: new best cost of 1379.846\n",
      "\t\tStep 12: new best cost of 1289.011\n",
      "\t\tStep 13\n",
      "\t\tStep 14: new best cost of 1261.859\n",
      "\t\tStep 15: new best cost of 1255.698\n",
      "\t\tStep 16: new best cost of 1221.986\n",
      "\t\tStep 17: new best cost of 1197.997\n",
      "\t\tStep 18\n",
      "\t\tStep 19: new best cost of 1171.208\n",
      "\t\tStep 20: new best cost of 1159.772\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25: new best cost of 1095.626\n",
      "\t\tStep 26\n",
      "\t\tStep 27: new best cost of 1079.594\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35: new best cost of 1049.296\n",
      "\t\tStep 36: new best cost of 1049.264\n",
      "\t\tStep 37\n",
      "\t\tStep 38: new best cost of 1042.646\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41: new best cost of 1041.953\n",
      "\t\tStep 42: new best cost of 1015.730\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48: new best cost of 1001.875\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56: new best cost of 974.003\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\tCross-validation fold 2\n",
      "\t\tStep 1: new best cost of 1799.157\n",
      "\t\tStep 2\n",
      "\t\tStep 3: new best cost of 1756.639\n",
      "\t\tStep 4: new best cost of 1641.905\n",
      "\t\tStep 5: new best cost of 1606.141\n",
      "\t\tStep 6: new best cost of 1512.474\n",
      "\t\tStep 7\n",
      "\t\tStep 8: new best cost of 1491.965\n",
      "\t\tStep 9: new best cost of 1429.228\n",
      "\t\tStep 10: new best cost of 1414.395\n",
      "\t\tStep 11: new best cost of 1356.613\n",
      "\t\tStep 12: new best cost of 1345.930\n",
      "\t\tStep 13: new best cost of 1327.540\n",
      "\t\tStep 14: new best cost of 1199.696\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17: new best cost of 1160.443\n",
      "\t\tStep 18: new best cost of 1149.777\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21: new best cost of 1141.101\n",
      "\t\tStep 22: new best cost of 1119.998\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25: new best cost of 1088.368\n",
      "\t\tStep 26: new best cost of 1069.799\n",
      "\t\tStep 27\n",
      "\t\tStep 28: new best cost of 1057.036\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33: new best cost of 1044.079\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36: new best cost of 1042.244\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39: new best cost of 1026.569\n",
      "\t\tStep 40\n",
      "\t\tStep 41: new best cost of 1022.542\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49: new best cost of 1013.060\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52: new best cost of 1005.382\n",
      "\t\tStep 53: new best cost of 1000.082\n",
      "\t\tStep 54: new best cost of 987.484\n",
      "\t\tStep 55\n",
      "\t\tStep 56: new best cost of 972.441\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\tCross-validation fold 3\n",
      "\t\tStep 1: new best cost of 1793.152\n",
      "\t\tStep 2: new best cost of 1785.320\n",
      "\t\tStep 3: new best cost of 1713.201\n",
      "\t\tStep 4: new best cost of 1670.529\n",
      "\t\tStep 5: new best cost of 1641.983\n",
      "\t\tStep 6: new best cost of 1630.829\n",
      "\t\tStep 7: new best cost of 1513.867\n",
      "\t\tStep 8\n",
      "\t\tStep 9: new best cost of 1449.839\n",
      "\t\tStep 10\n",
      "\t\tStep 11: new best cost of 1362.166\n",
      "\t\tStep 12: new best cost of 1338.729\n",
      "\t\tStep 13: new best cost of 1251.762\n",
      "\t\tStep 14: new best cost of 1223.862\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17: new best cost of 1173.886\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24: new best cost of 1121.018\n",
      "\t\tStep 25: new best cost of 1051.527\n",
      "\t\tStep 26: new best cost of 1035.986\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34: new best cost of 1029.389\n",
      "\t\tStep 35: new best cost of 1019.792\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39: new best cost of 1011.045\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43: new best cost of 1004.044\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51: new best cost of 1000.562\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54: new best cost of 998.669\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64: new best cost of 991.413\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\tCross-validation fold 4\n",
      "\t\tStep 1: new best cost of 1790.316\n",
      "\t\tStep 2: new best cost of 1769.839\n",
      "\t\tStep 3: new best cost of 1761.872\n",
      "\t\tStep 4: new best cost of 1631.320\n",
      "\t\tStep 5: new best cost of 1561.865\n",
      "\t\tStep 6: new best cost of 1554.220\n",
      "\t\tStep 7: new best cost of 1518.977\n",
      "\t\tStep 8: new best cost of 1478.409\n",
      "\t\tStep 9\n",
      "\t\tStep 10: new best cost of 1414.868\n",
      "\t\tStep 11: new best cost of 1342.200\n",
      "\t\tStep 12: new best cost of 1308.227\n",
      "\t\tStep 13\n",
      "\t\tStep 14: new best cost of 1281.022\n",
      "\t\tStep 15: new best cost of 1208.630\n",
      "\t\tStep 16: new best cost of 1192.735\n",
      "\t\tStep 17: new best cost of 1150.666\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26: new best cost of 1068.304\n",
      "\t\tStep 27: new best cost of 1062.074\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33: new best cost of 1023.835\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40: new best cost of 1020.024\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47: new best cost of 1020.013\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54: new best cost of 1005.428\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\tCross-validation fold 5\n",
      "\t\tStep 1: new best cost of 1799.824\n",
      "\t\tStep 2: new best cost of 1789.044\n",
      "\t\tStep 3: new best cost of 1737.552\n",
      "\t\tStep 4: new best cost of 1722.543\n",
      "\t\tStep 5: new best cost of 1625.609\n",
      "\t\tStep 6: new best cost of 1536.019\n",
      "\t\tStep 7: new best cost of 1519.968\n",
      "\t\tStep 8\n",
      "\t\tStep 9: new best cost of 1509.664\n",
      "\t\tStep 10\n",
      "\t\tStep 11: new best cost of 1354.415\n",
      "\t\tStep 12\n",
      "\t\tStep 13: new best cost of 1257.538\n",
      "\t\tStep 14: new best cost of 1231.691\n",
      "\t\tStep 15: new best cost of 1179.912\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18: new best cost of 1152.997\n",
      "\t\tStep 19: new best cost of 1132.814\n",
      "\t\tStep 20: new best cost of 1121.496\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23: new best cost of 1099.689\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27: new best cost of 1053.285\n",
      "\t\tStep 28: new best cost of 1042.027\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33: new best cost of 1028.229\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42: new best cost of 1025.907\n",
      "\t\tStep 43: new best cost of 998.043\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46: new best cost of 987.746\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51: new best cost of 979.872\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "Wine gradient descent:\n",
      "\tCross-validation fold 1\n",
      "\t\tStep 1: new best cost of 138.240\n",
      "\t\tStep 2\n",
      "\t\tStep 3: new best cost of 127.471\n",
      "\t\tStep 4\n",
      "\t\tStep 5: new best cost of 123.489\n",
      "\t\tStep 6\n",
      "\t\tStep 7: new best cost of 120.033\n",
      "\t\tStep 8\n",
      "\t\tStep 9\n",
      "\t\tStep 10\n",
      "\t\tStep 11: new best cost of 116.768\n",
      "\t\tStep 12: new best cost of 115.201\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16: new best cost of 110.155\n",
      "\t\tStep 17\n",
      "\t\tStep 18: new best cost of 105.476\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21: new best cost of 103.611\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27: new best cost of 101.265\n",
      "\t\tStep 28\n",
      "\t\tStep 29: new best cost of 100.195\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38: new best cost of 99.135\n",
      "\t\tStep 39: new best cost of 97.626\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42: new best cost of 97.591\n",
      "\t\tStep 43\n",
      "\t\tStep 44: new best cost of 96.194\n",
      "\t\tStep 45\n",
      "\t\tStep 46: new best cost of 95.414\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56: new best cost of 93.325\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60: new best cost of 93.280\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63: new best cost of 92.367\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70: new best cost of 92.227\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73: new best cost of 91.873\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\tCross-validation fold 2\n",
      "\t\tStep 1: new best cost of 148.055\n",
      "\t\tStep 2\n",
      "\t\tStep 3: new best cost of 136.985\n",
      "\t\tStep 4\n",
      "\t\tStep 5\n",
      "\t\tStep 6: new best cost of 136.266\n",
      "\t\tStep 7: new best cost of 131.883\n",
      "\t\tStep 8\n",
      "\t\tStep 9: new best cost of 126.009\n",
      "\t\tStep 10\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13: new best cost of 123.831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tStep 14: new best cost of 122.766\n",
      "\t\tStep 15: new best cost of 118.471\n",
      "\t\tStep 16\n",
      "\t\tStep 17: new best cost of 115.953\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21: new best cost of 107.999\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25: new best cost of 107.377\n",
      "\t\tStep 26\n",
      "\t\tStep 27: new best cost of 107.152\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32: new best cost of 104.597\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36: new best cost of 100.223\n",
      "\t\tStep 37\n",
      "\t\tStep 38: new best cost of 99.241\n",
      "\t\tStep 39: new best cost of 99.166\n",
      "\t\tStep 40: new best cost of 98.444\n",
      "\t\tStep 41: new best cost of 97.689\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47: new best cost of 96.697\n",
      "\t\tStep 48\n",
      "\t\tStep 49: new best cost of 95.113\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55: new best cost of 94.897\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\tCross-validation fold 3\n",
      "\t\tStep 1: new best cost of 148.750\n",
      "\t\tStep 2\n",
      "\t\tStep 3: new best cost of 133.991\n",
      "\t\tStep 4\n",
      "\t\tStep 5: new best cost of 128.811\n",
      "\t\tStep 6\n",
      "\t\tStep 7: new best cost of 118.765\n",
      "\t\tStep 8\n",
      "\t\tStep 9\n",
      "\t\tStep 10\n",
      "\t\tStep 11\n",
      "\t\tStep 12: new best cost of 110.019\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16: new best cost of 109.018\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20: new best cost of 108.998\n",
      "\t\tStep 21\n",
      "\t\tStep 22: new best cost of 101.826\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27: new best cost of 101.109\n",
      "\t\tStep 28\n",
      "\t\tStep 29: new best cost of 100.216\n",
      "\t\tStep 30: new best cost of 99.150\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36: new best cost of 97.833\n",
      "\t\tStep 37\n",
      "\t\tStep 38: new best cost of 96.555\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48: new best cost of 95.433\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\tCross-validation fold 4\n",
      "\t\tStep 1: new best cost of 148.225\n",
      "\t\tStep 2\n",
      "\t\tStep 3: new best cost of 140.188\n",
      "\t\tStep 4\n",
      "\t\tStep 5: new best cost of 131.259\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8\n",
      "\t\tStep 9: new best cost of 129.451\n",
      "\t\tStep 10\n",
      "\t\tStep 11: new best cost of 121.855\n",
      "\t\tStep 12\n",
      "\t\tStep 13: new best cost of 112.727\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19: new best cost of 110.102\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25: new best cost of 106.688\n",
      "\t\tStep 26: new best cost of 103.740\n",
      "\t\tStep 27\n",
      "\t\tStep 28: new best cost of 102.989\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32: new best cost of 102.018\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36: new best cost of 100.239\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42: new best cost of 99.664\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48: new best cost of 98.859\n",
      "\t\tStep 49\n",
      "\t\tStep 50: new best cost of 96.589\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55: new best cost of 96.177\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60: new best cost of 95.718\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63: new best cost of 95.566\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71: new best cost of 95.239\n",
      "\t\tStep 72: new best cost of 94.856\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79: new best cost of 94.822\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\tCross-validation fold 5\n",
      "\t\tStep 1: new best cost of 137.338\n",
      "\t\tStep 2: new best cost of 136.305\n",
      "\t\tStep 3: new best cost of 123.127\n",
      "\t\tStep 4\n",
      "\t\tStep 5\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8\n",
      "\t\tStep 9\n",
      "\t\tStep 10\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13: new best cost of 117.567\n",
      "\t\tStep 14: new best cost of 115.228\n",
      "\t\tStep 15: new best cost of 114.240\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\n",
      "\n",
      "Digits training accuracy: 93.1%\n",
      "Digits training cost: 1230.789\n",
      "Digits validation accuracy: 89.5%\n",
      "Digits validation cost: 1297.539\n",
      "Wine training accuracy: 87.8%\n",
      "Wine training cost: 122.816\n",
      "Wine validation accuracy: 39.9%\n",
      "Wine validation cost: 183.226\n"
     ]
    }
   ],
   "source": [
    "def accurate(a, b):\n",
    "    return np.argmax(a) == np.argmax(b)\n",
    "\n",
    "def cost(yh, y):\n",
    "    return y * np.log1p(np.exp(-yh)) + (1-yh) * np.log1p(np.exp(yh))\n",
    "\n",
    "# TODO: grid-search to find lowest cost combination of model hyper-parameters\n",
    "\n",
    "batch_size = 30\n",
    "learning_rate = 0.08\n",
    "momentum = 0.2\n",
    "\n",
    "print(\"Model hyper-parameters:\")\n",
    "print(\"\\tMini-batch size:\", batch_size)\n",
    "print(\"\\tLearning rate:\", learning_rate)\n",
    "print(\"\\tMomentum:\", momentum)\n",
    "print(\"\\n\")\n",
    "\n",
    "digits_training_accuracy = 0\n",
    "digits_training_cost = 0\n",
    "digits_validation_accuracy = 0\n",
    "digits_validation_cost = 0\n",
    "\n",
    "print(\"Digits gradient descent:\")\n",
    "\n",
    "for fold_index, fold in enumerate(xDigitsTrainingSets):\n",
    "    print(f\"\\tCross-validation fold {fold_index+1}\")\n",
    "    \n",
    "    gradientDescentModel = GradientDescent(batch_size, learning_rate, momentum)\n",
    "    logisticRegressionModel = LogisticRegression(add_bias=False)\n",
    "    \n",
    "    logisticRegressionModel.fit(fold, yDigitsTrainingSets[fold_index], gradientDescentModel)\n",
    "    yh_training = logisticRegressionModel.predict(xDigitsTrainingSets[fold_index])\n",
    "    yh_validation = logisticRegressionModel.predict(xDigitsValidationSets[fold_index])\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh_training):\n",
    "        if accurate(yh_x, yDigitsTrainingSets[fold_index][sample_index]):\n",
    "            digits_training_accuracy += 1\n",
    "        c = np.argmax(yDigitsTrainingSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yDigitsTrainingSets[fold_index][sample_index][c])\n",
    "        digits_training_cost += cst\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh_validation):\n",
    "        if accurate(yh_x, yDigitsValidationSets[fold_index][sample_index]):\n",
    "            digits_validation_accuracy += 1\n",
    "        c = np.argmax(yDigitsValidationSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yDigitsValidationSets[fold_index][sample_index][c])\n",
    "        digits_validation_cost += cst\n",
    "        \n",
    "digits_training_accuracy /= 4*len(digits.data)\n",
    "digits_training_cost /= 4\n",
    "digits_validation_accuracy /= len(digits.data)\n",
    "\n",
    "wine_training_accuracy = 0\n",
    "wine_training_cost = 0\n",
    "wine_validation_accuracy = 0\n",
    "wine_validation_cost = 0\n",
    "\n",
    "print(\"Wine gradient descent:\")\n",
    "\n",
    "for fold_index, fold in enumerate(xWineTrainingSets):\n",
    "    print(f\"\\tCross-validation fold {fold_index+1}\")\n",
    "    \n",
    "    gradientDescentModel = GradientDescent(batch_size, learning_rate, momentum)\n",
    "    logisticRegressionModel = LogisticRegression(add_bias=False)\n",
    "    \n",
    "    logisticRegressionModel.fit(fold, yWineTrainingSets[fold_index], gradientDescentModel)\n",
    "    yh_training = logisticRegressionModel.predict(xWineTrainingSets[fold_index])\n",
    "    yh_validation = logisticRegressionModel.predict(xWineValidationSets[fold_index])\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh_training):\n",
    "        if accurate(yh_x, yWineTrainingSets[fold_index][sample_index]):\n",
    "            wine_training_accuracy += 1\n",
    "        c = np.argmax(yWineTrainingSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yWineTrainingSets[fold_index][sample_index][c])\n",
    "        wine_training_cost += cst\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh_validation):\n",
    "        if accurate(yh_x, yWineValidationSets[fold_index][sample_index]):\n",
    "            wine_validation_accuracy += 1\n",
    "        c = np.argmax(yWineValidationSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yWineValidationSets[fold_index][sample_index][c])\n",
    "        wine_validation_cost += cst\n",
    "\n",
    "wine_training_accuracy /= 4*len(wine.data)\n",
    "wine_training_cost /= 4\n",
    "wine_validation_accuracy /= len(wine.data)\n",
    "    \n",
    "print(\"\\n\")\n",
    "print(f\"Digits training accuracy: {digits_training_accuracy*100:.1f}%\")\n",
    "print(f\"Digits training cost: {digits_training_cost:.3f}\")\n",
    "print(f\"Digits validation accuracy: {digits_validation_accuracy*100:.1f}%\")\n",
    "print(f\"Digits validation cost: {digits_validation_cost:.3f}\")\n",
    "print(f\"Wine training accuracy: {wine_training_accuracy*100:.1f}%\")\n",
    "print(f\"Wine training cost: {wine_training_cost:.3f}\")\n",
    "print(f\"Wine validation accuracy: {wine_validation_accuracy*100:.1f}%\")\n",
    "print(f\"Wine validation cost: {wine_validation_cost:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean = lambda x1, x2: np.sqrt(np.sum((x1 - x2)**2, axis=-1))\n",
    "manhattan = lambda x1, x2: np.sum(np.abs(x1 - x2), axis=-1)\n",
    "\n",
    "class KNN:\n",
    "\n",
    "    def __init__(self, K=1, dist_fn= euclidean):\n",
    "        self.dist_fn = dist_fn\n",
    "        self.K = K\n",
    "        return\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.C = len(y[0])\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        num_test = x_test.shape[0]\n",
    "        distances = self.dist_fn(self.x[None,:,:], x_test[:,None,:])\n",
    "        knns = np.zeros((num_test, self.K), dtype=int)\n",
    "        y_prob = np.zeros((num_test),dtype=int)\n",
    "        counts = np.zeros((num_test, self.C))\n",
    "        \n",
    "        for i in range(num_test):\n",
    "            knns[i,:] = np.argsort(distances[i])[:self.K]\n",
    "            k_count=np.zeros(self.K, dtype=int)\n",
    "            \n",
    "            for s, arr in enumerate(self.y[knns[i,:]]):\n",
    "                k_count[s] = np.argmax(arr)\n",
    "            \n",
    "            y_prob_i, counts_i = np.unique(k_count, return_counts=True)\n",
    "            y_prob[i] = int(y_prob_i[np.argmax(counts_i)])\n",
    "        \n",
    "        return y_prob, knns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN digits validation accuracy: 95.7%\n",
      "KNN wine validation accuracy: 85.4%\n"
     ]
    }
   ],
   "source": [
    "KNNmodel = KNN(K=11)\n",
    "\n",
    "digits_knn_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    y_prob, knns = KNNmodel.fit(np.asarray(xDigitsTrainingSets[fold]), np.asarray(yDigitsTrainingSets[fold])).predict(np.asarray(xDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, prob in enumerate(y_prob):\n",
    "        if prob == np.argmax(yDigitsValidationSets[fold][i]):\n",
    "            digits_knn_accuracy += 1\n",
    "\n",
    "digits_knn_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"KNN digits validation accuracy: {digits_knn_accuracy*100:.1f}%\")\n",
    "\n",
    "KNNmodel = KNN(K=7)\n",
    "\n",
    "wine_knn_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    y_prob, knns = KNNmodel.fit(np.asarray(xWineTrainingSets[fold]), np.asarray(yWineTrainingSets[fold])).predict(np.asarray(xWineValidationSets[fold]))\n",
    "    \n",
    "    for i, prob in enumerate(y_prob):\n",
    "        if prob == np.argmax(yWineValidationSets[fold][i]):\n",
    "            wine_knn_accuracy += 1\n",
    "            \n",
    "wine_knn_accuracy /= len(wine.data)\n",
    "\n",
    "print(f\"KNN wine validation accuracy: {wine_knn_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HoGFeatures(img, cellSize, blockSize, nbins):\n",
    "    cell_size = (cellSize, cellSize)\n",
    "    block_size = (blockSize, blockSize)\n",
    "    \n",
    "    hog = cv.HOGDescriptor(_winSize=(img.shape[1] // cell_size[1] * cell_size[1],\n",
    "                                     img.shape[0] // cell_size[0] * cell_size[0]),\n",
    "                           _blockSize=(block_size[1] * cell_size[1],\n",
    "                                       block_size[0] * cell_size[0]),\n",
    "                           _blockStride=(cell_size[1], cell_size[0]),\n",
    "                           _cellSize=(cell_size[1], cell_size[0]),\n",
    "                           _nbins=nbins\n",
    "    )\n",
    "    \n",
    "    return hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeHoGFeatures(imageArray):\n",
    "    HoG = HoGFeatures(imageArray[0], 2, 2, 2)\n",
    "    features = []\n",
    "    \n",
    "    for i, image in enumerate(imageArray):\n",
    "        features.append(HoG.compute((image*255).astype(np.uint8)))\n",
    "        \n",
    "    features = np.array(np.squeeze(features))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC digits validation accuracy: 89.6%\n"
     ]
    }
   ],
   "source": [
    "digits_svc_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    numbers_training = []\n",
    "    numbers_validation = []\n",
    "    \n",
    "    for i, number in enumerate(xDigitsTrainingSets[fold]):\n",
    "        numbers_training.append(np.asarray(number).reshape(8, 8))\n",
    "        \n",
    "    for i, number in enumerate(xDigitsValidationSets[fold]):\n",
    "        numbers_validation.append(np.asarray(number).reshape(8, 8))  \n",
    "        \n",
    "    HoGs_training = makeHoGFeatures(np.asarray(numbers_training))\n",
    "    HoGs_validation = makeHoGFeatures(np.asarray(numbers_validation))\n",
    "\n",
    "    clf = svm.SVC(gamma='auto', C=100) \n",
    "    \n",
    "    labels_training = np.zeros(len(yDigitsTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    clf.fit(HoGs_training, labels_training)\n",
    "\n",
    "    labels_predicted = clf.predict(HoGs_validation)\n",
    "    \n",
    "    for i, label in enumerate(labels_predicted):\n",
    "        if label == labels_validation[i]:\n",
    "            digits_svc_accuracy += 1\n",
    "\n",
    "digits_svc_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"SVC digits validation accuracy: {digits_svc_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive base digits validation accuracy: 81.1%\n",
      "\n",
      "Naive base wine validation accuracy: 93.8%\n"
     ]
    }
   ],
   "source": [
    "digits_naive_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    labels_training = np.zeros(len(yDigitsTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(np.asarray(xDigitsTrainingSets[fold]), labels_training).predict(np.asarray(xDigitsValidationSets[fold]))\n",
    "\n",
    "    for i, label in enumerate(y_pred):\n",
    "        if label == labels_validation[i]:\n",
    "            digits_naive_accuracy += 1\n",
    "\n",
    "digits_naive_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"Naive base digits validation accuracy: {digits_naive_accuracy*100:.1f}%\\n\")\n",
    "\n",
    "wine_naive_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    labels_training = np.zeros(len(yWineTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yWineTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yWineValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yWineValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(np.asarray(xWineTrainingSets[fold]), labels_training).predict(np.asarray(xWineValidationSets[fold]))\n",
    "\n",
    "    for i, label in enumerate(y_pred):\n",
    "        if label == labels_validation[i]:\n",
    "            wine_naive_accuracy += 1\n",
    "\n",
    "wine_naive_accuracy /= len(wine.data)\n",
    "\n",
    "print(f\"Naive base wine validation accuracy: {wine_naive_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
