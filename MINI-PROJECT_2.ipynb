{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lia\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "from sklearn import svm \n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_search will perform the gridsearch in order to run the tests to plot graphs\n",
    "#if run_search = False, there will be no plots\n",
    "#ONLY TURN RUN_SEARCH TO TRUE IF YOU HAVE 5 HOURS TO SPARE\n",
    "run_search = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "wine = fetch_openml(name='wine', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of digits and wine data\n",
    "\n",
    "digits_data_norm = []\n",
    "\n",
    "for col in digits.data:\n",
    "    col_norm = col/np.max(col)\n",
    "    digits_data_norm.append(col_norm)\n",
    "\n",
    "digits.data = np.asarray(digits_data_norm)\n",
    "\n",
    "wine_data_norm = []\n",
    "\n",
    "for col in wine.data.T:\n",
    "    col_norm = col/np.amax(col)\n",
    "    wine_data_norm.append(col_norm)\n",
    "    \n",
    "wine.data = np.asarray(wine_data_norm).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation for digits dataset\n",
    "\n",
    "digitsTrainingSetSize = int(np.ceil(0.8 * len(digits.data)))\n",
    "digitsValidationSetSize = int(len(digits.data) - digitsTrainingSetSize)\n",
    "\n",
    "xDigitsTrainingSets = []\n",
    "yDigitsTrainingSets = []\n",
    "xDigitsValidationSets = []\n",
    "yDigitsValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data[foldIndex*digitsValidationSetSize:((foldIndex*digitsValidationSetSize)+digitsValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(digits.target[index+(foldIndex*digitsValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(digits.target[index])\n",
    "            \n",
    "    xDigitsTrainingSets.append(xTrainingSet)\n",
    "    yDigitsTrainingSets.append(yTrainingSet)\n",
    "    xDigitsValidationSets.append(xValidationSet)\n",
    "    yDigitsValidationSets.append(yValidationSet)\n",
    "    \n",
    "# 5-fold cross validation for wine dataset\n",
    "\n",
    "wineTrainingSetSize = int(np.ceil(0.8 * len(wine.data)))\n",
    "wineValidationSetSize = int(len(wine.data) - wineTrainingSetSize)\n",
    "\n",
    "xWineTrainingSets = []\n",
    "yWineTrainingSets = []\n",
    "xWineValidationSets = []\n",
    "yWineValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "    for index, data in enumerate(wine.data[foldIndex*wineValidationSetSize:((foldIndex*wineValidationSetSize)+wineValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(wine.target[index+(foldIndex*wineValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "    \n",
    "    for index, data in enumerate(wine.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(wine.target[index])\n",
    "            \n",
    "    xWineTrainingSets.append(xTrainingSet)\n",
    "    yWineTrainingSets.append(yTrainingSet)\n",
    "    xWineValidationSets.append(xValidationSet)\n",
    "    yWineValidationSets.append(yValidationSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of y for digits dataset\n",
    "\n",
    "numberOfDigitsTargets = 10\n",
    "numberOfWineTargets = 3\n",
    "\n",
    "for index, fold in enumerate(yDigitsTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yDigitsValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsValidationSets[index] = encodedFold\n",
    "\n",
    "# one-hot encoding of y for wine dataset\n",
    "\n",
    "for index, fold in enumerate(yWineTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yWineValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineValidationSets[index] = encodedFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomIndices(arr, batch_size):\n",
    "    indices = []\n",
    "    \n",
    "    if batch_size > len(arr):\n",
    "        print(\"Error: batch size larger than size of dataset.\")\n",
    "        return\n",
    "    \n",
    "    while batch_size > 0:\n",
    "        x = np.floor(np.random.random() * len(arr))\n",
    "        if x not in indices:\n",
    "            indices.append(int(x))\n",
    "            batch_size -= 1\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent class\n",
    " \n",
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, batch_size, learning_rate=0.5, momentum=0.9, max_termination_condition=25, max_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        self.max_termination_condition = max_termination_condition\n",
    "        self.max_iters = max_iters\n",
    "        self.deltas = []\n",
    "        \n",
    "    def run(self, gradient_fn, x, y, w, vx, vy):\n",
    "        t = 1\n",
    "        \n",
    "        max_accuracy = -1\n",
    "        termination_count = 0        \n",
    "        weight_history = []\n",
    "        error_history = []\n",
    "                \n",
    "        for number_of_targets in range(len(y[0])):\n",
    "            weight_history.append([])\n",
    "        \n",
    "        while termination_count < self.max_termination_condition and t < self.max_iters:\n",
    "            gradients = gradient_fn(x, y, w, self.batch_size)   \n",
    "            \n",
    "            for c in range(len(y[0])):\n",
    "                if(t==1):\n",
    "                    w[c] = w[c] - self.learning_rate * gradients[c]\n",
    "                else:\n",
    "                    delta_w = (self.momentum)*(self.deltas[-(len(y[0]))]) + (1-self.momentum)*gradients[c]\n",
    "                    w[c] = w[c] - (self.learning_rate)*(delta_w)\n",
    "                self.deltas.append(w[c])\n",
    "            \n",
    "            a = np.asarray(vx)\n",
    "            b = np.asarray(w)\n",
    "    \n",
    "            vyh=[]\n",
    "        \n",
    "            for i, vx_c in enumerate(a):\n",
    "                vyh_x=[]\n",
    "\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  b[c] @ vx_c\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    \n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  b[i] @ vx_c\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    vyh_c = num/den\n",
    "                    vyh_x.append(vyh_c)\n",
    "                    \n",
    "                vyh.append(vyh_x)\n",
    "                \n",
    "            step_accuracy = 0\n",
    "                \n",
    "            def accurate(a, b):\n",
    "                return np.argmax(a) == np.argmax(b)\n",
    "                \n",
    "            for sample_index, vyh_x in enumerate(vyh):\n",
    "                if accurate(vyh_x, vy[sample_index]):\n",
    "                    step_accuracy += 1\n",
    "                    \n",
    "            step_accuracy /= len(vx)\n",
    "            \n",
    "            for c in range(len(b)):\n",
    "                weight_history[c].append(w[c])\n",
    "            \n",
    "            error_history.append(step_accuracy)\n",
    "            \n",
    "            # We use an alternate termination condition that terminates faster as\n",
    "            # the suggested condition ran for a longtime for us (~1hr).\n",
    "            \n",
    "            # We track the best training accuracy encountered, and if the \n",
    "            # next max_termination_condition-number of steps do not have a better accuracy, then\n",
    "            # it terminates.\n",
    "\n",
    "            if step_accuracy > max_accuracy:\n",
    "                max_accuracy = step_accuracy\n",
    "                termination_count = 0\n",
    "                print(f\"\\t\\tStep {t}: new best accuracy of {max_accuracy:.3f}\")\n",
    "            else:\n",
    "                termination_count += 1\n",
    "                print(f\"\\t\\tStep {t}\")\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        # take the weight prior to the last max_termination_condition-number of weights\n",
    "        # (as it is guaranteed to be the best prior to termination).\n",
    "        \n",
    "        index_best = len(error_history)-self.max_termination_condition-1\n",
    "        \n",
    "        w_best = []\n",
    "        \n",
    "        for c in range(len(y[0])):\n",
    "            w_best.append(weight_history[c][index_best])\n",
    "        \n",
    "        return w_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, add_bias=True):\n",
    "        self.add_bias = add_bias\n",
    "        pass\n",
    "            \n",
    "    def fit(self, x, y, optimizer, vx, vy):\n",
    "        def gradient(x, y, w, batch_size):\n",
    "            gradients = np.zeros(len(w)).tolist()\n",
    "\n",
    "            indices = getRandomIndices(x, batch_size)\n",
    "\n",
    "            for index in indices:\n",
    "                a = np.asarray(x[index])\n",
    "                b = np.asarray(y[index])\n",
    "\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  w[c] @ a\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  w[i] @ a\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "\n",
    "                    y_c = b[c]\n",
    "                    \n",
    "                    cost_c = np.dot(yh_c - y_c, a)\n",
    "                    \n",
    "                    gradients[c] += cost_c\n",
    "\n",
    "            return gradients\n",
    "        \n",
    "        if self.add_bias:\n",
    "            x = np.asarray(x)\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "            \n",
    "            vx = np.asarray(vx)\n",
    "            vN = vx.shape[0]\n",
    "            vx = np.column_stack([vx,np.ones(vN)])\n",
    "\n",
    "        w0 = []\n",
    "        for c in range(len(y[0])):\n",
    "            w0.append(np.zeros(len(x[0])))\n",
    "            \n",
    "        self.w = optimizer.run(gradient, x, y, w0, vx, vy)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if self.add_bias:\n",
    "            x = np.asarray(x)\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "\n",
    "        a = np.asarray(x)\n",
    "        b = np.asarray(self.w)\n",
    "\n",
    "        yh=[]\n",
    "        \n",
    "        for i, x_c in enumerate(a):\n",
    "            yh_x=[]\n",
    "            \n",
    "            for c in range(len(b)):\n",
    "                w_x =  b[c] @ x_c\n",
    "                num = np.exp(w_x)\n",
    "\n",
    "                den = 0\n",
    "                \n",
    "                for i in range(len(b)):\n",
    "                    w_x =  b[i] @ x_c\n",
    "                    den += np.exp(w_x)\n",
    "\n",
    "                yh_c = num/den\n",
    "                yh_x.append(yh_c)\n",
    "                \n",
    "            yh.append(yh_x)\n",
    "        \n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def runLogisticRegression(batch_size, learning_rate, momentum):\n",
    "    def accurate(a, b):\n",
    "        return np.argmax(a) == np.argmax(b)\n",
    "\n",
    "    def cost(yh, y):\n",
    "        return y * np.log1p(np.exp(-yh)) + (1-yh) * np.log1p(np.exp(yh))\n",
    "\n",
    "    print(\"Model hyper-parameters:\")\n",
    "    print(\"\\tMini-batch size:\", batch_size)\n",
    "    print(\"\\tLearning rate:\", learning_rate)\n",
    "    print(\"\\tMomentum:\", momentum)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    digits_training_accuracy = 0\n",
    "    digits_training_cost = 0\n",
    "    digits_validation_accuracy = 0\n",
    "    digits_validation_cost = 0\n",
    "\n",
    "    print(\"Digits gradient descent:\")\n",
    "\n",
    "    for fold_index, fold in enumerate(xDigitsTrainingSets):\n",
    "        print(f\"\\tCross-validation fold {fold_index+1}\")\n",
    "\n",
    "        gradientDescentModel = GradientDescent(batch_size, learning_rate, momentum)\n",
    "        logisticRegressionModel = LogisticRegression(add_bias=True)\n",
    "\n",
    "        logisticRegressionModel.fit(fold, yDigitsTrainingSets[fold_index], gradientDescentModel, xDigitsValidationSets[fold_index], yDigitsValidationSets[fold_index])\n",
    "        yh_training = logisticRegressionModel.predict(xDigitsTrainingSets[fold_index])\n",
    "        yh_validation = logisticRegressionModel.predict(xDigitsValidationSets[fold_index])\n",
    "\n",
    "        for sample_index, yh_x in enumerate(yh_training):\n",
    "            if accurate(yh_x, yDigitsTrainingSets[fold_index][sample_index]):\n",
    "                digits_training_accuracy += 1\n",
    "            c = np.argmax(yDigitsTrainingSets[fold_index][sample_index])\n",
    "            cst = cost(yh_x[c], yDigitsTrainingSets[fold_index][sample_index][c])\n",
    "            digits_training_cost += cst\n",
    "\n",
    "        for sample_index, yh_x in enumerate(yh_validation):\n",
    "            if accurate(yh_x, yDigitsValidationSets[fold_index][sample_index]):\n",
    "                digits_validation_accuracy += 1\n",
    "            c = np.argmax(yDigitsValidationSets[fold_index][sample_index])\n",
    "            cst = cost(yh_x[c], yDigitsValidationSets[fold_index][sample_index][c])\n",
    "            digits_validation_cost += cst\n",
    "\n",
    "    digits_training_accuracy /= 4*len(digits.data)\n",
    "    digits_training_cost /= 4\n",
    "    digits_validation_accuracy /= len(digits.data)\n",
    "\n",
    "    wine_training_accuracy = 0\n",
    "    wine_training_cost = 0\n",
    "    wine_validation_accuracy = 0\n",
    "    wine_validation_cost = 0\n",
    "\n",
    "    print(\"Wine gradient descent:\")\n",
    "\n",
    "    for fold_index, fold in enumerate(xWineTrainingSets):\n",
    "        print(f\"\\tCross-validation fold {fold_index+1}\")\n",
    "\n",
    "        gradientDescentModel = GradientDescent(batch_size, learning_rate, momentum)\n",
    "        logisticRegressionModel = LogisticRegression(add_bias=True)\n",
    "\n",
    "        logisticRegressionModel.fit(fold, yWineTrainingSets[fold_index], gradientDescentModel, xWineValidationSets[fold_index], yWineValidationSets[fold_index])\n",
    "        yh_training = logisticRegressionModel.predict(xWineTrainingSets[fold_index])\n",
    "        yh_validation = logisticRegressionModel.predict(xWineValidationSets[fold_index])\n",
    "\n",
    "        for sample_index, yh_x in enumerate(yh_training):\n",
    "            if accurate(yh_x, yWineTrainingSets[fold_index][sample_index]):\n",
    "                wine_training_accuracy += 1\n",
    "            c = np.argmax(yWineTrainingSets[fold_index][sample_index])\n",
    "            cst = cost(yh_x[c], yWineTrainingSets[fold_index][sample_index][c])\n",
    "            wine_training_cost += cst\n",
    "\n",
    "        for sample_index, yh_x in enumerate(yh_validation):\n",
    "            if accurate(yh_x, yWineValidationSets[fold_index][sample_index]):\n",
    "                wine_validation_accuracy += 1\n",
    "            c = np.argmax(yWineValidationSets[fold_index][sample_index])\n",
    "            cst = cost(yh_x[c], yWineValidationSets[fold_index][sample_index][c])\n",
    "            wine_validation_cost += cst\n",
    "\n",
    "    wine_training_accuracy /= 4*len(wine.data)\n",
    "    wine_training_cost /= 4\n",
    "    wine_validation_accuracy /= len(wine.data)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(f\"Digits training accuracy: {digits_training_accuracy*100:.1f}%\")\n",
    "    print(f\"Digits training cost: {digits_training_cost:.3f}\")\n",
    "    print(f\"Digits validation accuracy: {digits_validation_accuracy*100:.1f}%\")\n",
    "    print(f\"Digits validation cost: {digits_validation_cost:.3f}\")\n",
    "    print(f\"Wine training accuracy: {wine_training_accuracy*100:.1f}%\")\n",
    "    print(f\"Wine training cost: {wine_training_cost:.3f}\")\n",
    "    print(f\"Wine validation accuracy: {wine_validation_accuracy*100:.1f}%\")\n",
    "    print(f\"Wine validation cost: {wine_validation_cost:.3f}\")\n",
    "    \n",
    "    return (digits_validation_accuracy, digits_validation_cost, wine_validation_accuracy, wine_validation_cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hyper-parameters:\n",
      "\tMini-batch size: 30\n",
      "\tLearning rate: 0.04\n",
      "\tMomentum: 0.2\n",
      "\n",
      "\n",
      "Digits gradient descent:\n",
      "\tCross-validation fold 1\n",
      "\t\tStep 1: new best accuracy of 0.100\n",
      "\t\tStep 2: new best accuracy of 0.354\n",
      "\t\tStep 3\n",
      "\t\tStep 4: new best accuracy of 0.504\n",
      "\t\tStep 5\n",
      "\t\tStep 6\n",
      "\t\tStep 7: new best accuracy of 0.646\n",
      "\t\tStep 8\n",
      "\t\tStep 9\n",
      "\t\tStep 10: new best accuracy of 0.649\n",
      "\t\tStep 11: new best accuracy of 0.766\n",
      "\t\tStep 12: new best accuracy of 0.791\n",
      "\t\tStep 13: new best accuracy of 0.819\n",
      "\t\tStep 14\n",
      "\t\tStep 15: new best accuracy of 0.855\n",
      "\t\tStep 16\n",
      "\t\tStep 17: new best accuracy of 0.858\n",
      "\t\tStep 18: new best accuracy of 0.869\n",
      "\t\tStep 19\n",
      "\t\tStep 20: new best accuracy of 0.891\n",
      "\t\tStep 21: new best accuracy of 0.905\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44: new best accuracy of 0.919\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47: new best accuracy of 0.925\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\tCross-validation fold 2\n",
      "\t\tStep 1: new best accuracy of 0.181\n",
      "\t\tStep 2\n",
      "\t\tStep 3: new best accuracy of 0.387\n",
      "\t\tStep 4\n",
      "\t\tStep 5\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8: new best accuracy of 0.390\n",
      "\t\tStep 9: new best accuracy of 0.604\n",
      "\t\tStep 10: new best accuracy of 0.724\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14: new best accuracy of 0.744\n",
      "\t\tStep 15: new best accuracy of 0.772\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19: new best accuracy of 0.788\n",
      "\t\tStep 20: new best accuracy of 0.830\n",
      "\t\tStep 21: new best accuracy of 0.833\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25: new best accuracy of 0.844\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28: new best accuracy of 0.883\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\tCross-validation fold 3\n",
      "\t\tStep 1: new best accuracy of 0.092\n",
      "\t\tStep 2: new best accuracy of 0.106\n",
      "\t\tStep 3: new best accuracy of 0.290\n",
      "\t\tStep 4: new best accuracy of 0.343\n",
      "\t\tStep 5: new best accuracy of 0.454\n",
      "\t\tStep 6\n",
      "\t\tStep 7: new best accuracy of 0.540\n",
      "\t\tStep 8: new best accuracy of 0.691\n",
      "\t\tStep 9\n",
      "\t\tStep 10: new best accuracy of 0.708\n",
      "\t\tStep 11\n",
      "\t\tStep 12: new best accuracy of 0.769\n",
      "\t\tStep 13: new best accuracy of 0.783\n",
      "\t\tStep 14\n",
      "\t\tStep 15: new best accuracy of 0.786\n",
      "\t\tStep 16\n",
      "\t\tStep 17: new best accuracy of 0.808\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20: new best accuracy of 0.825\n",
      "\t\tStep 21\n",
      "\t\tStep 22: new best accuracy of 0.830\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25: new best accuracy of 0.850\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28: new best accuracy of 0.894\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35: new best accuracy of 0.914\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52: new best accuracy of 0.916\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\tCross-validation fold 4\n",
      "\t\tStep 1: new best accuracy of 0.100\n",
      "\t\tStep 2: new best accuracy of 0.148\n",
      "\t\tStep 3: new best accuracy of 0.153\n",
      "\t\tStep 4: new best accuracy of 0.226\n",
      "\t\tStep 5: new best accuracy of 0.565\n",
      "\t\tStep 6\n",
      "\t\tStep 7: new best accuracy of 0.616\n",
      "\t\tStep 8\n",
      "\t\tStep 9: new best accuracy of 0.799\n",
      "\t\tStep 10\n",
      "\t\tStep 11\n",
      "\t\tStep 12: new best accuracy of 0.816\n",
      "\t\tStep 13: new best accuracy of 0.825\n",
      "\t\tStep 14\n",
      "\t\tStep 15: new best accuracy of 0.852\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18: new best accuracy of 0.858\n",
      "\t\tStep 19: new best accuracy of 0.861\n",
      "\t\tStep 20: new best accuracy of 0.886\n",
      "\t\tStep 21: new best accuracy of 0.911\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33: new best accuracy of 0.919\n",
      "\t\tStep 34: new best accuracy of 0.928\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39: new best accuracy of 0.955\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\tCross-validation fold 5\n",
      "\t\tStep 1: new best accuracy of 0.123\n",
      "\t\tStep 2: new best accuracy of 0.373\n",
      "\t\tStep 3: new best accuracy of 0.404\n",
      "\t\tStep 4\n",
      "\t\tStep 5\n",
      "\t\tStep 6: new best accuracy of 0.513\n",
      "\t\tStep 7: new best accuracy of 0.563\n",
      "\t\tStep 8\n",
      "\t\tStep 9: new best accuracy of 0.696\n",
      "\t\tStep 10\n",
      "\t\tStep 11\n",
      "\t\tStep 12: new best accuracy of 0.727\n",
      "\t\tStep 13: new best accuracy of 0.780\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17: new best accuracy of 0.802\n",
      "\t\tStep 18: new best accuracy of 0.825\n",
      "\t\tStep 19: new best accuracy of 0.830\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38: new best accuracy of 0.855\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46: new best accuracy of 0.864\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64: new best accuracy of 0.866\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67: new best accuracy of 0.872\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "Wine gradient descent:\n",
      "\tCross-validation fold 1\n",
      "\t\tStep 1: new best accuracy of 0.000\n",
      "\t\tStep 2\n",
      "\t\tStep 3\n",
      "\t\tStep 4\n",
      "\t\tStep 5\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8\n",
      "\t\tStep 9\n",
      "\t\tStep 10: new best accuracy of 0.657\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32: new best accuracy of 0.714\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\tCross-validation fold 2\n",
      "\t\tStep 1: new best accuracy of 0.000\n",
      "\t\tStep 2: new best accuracy of 0.314\n",
      "\t\tStep 3: new best accuracy of 0.686\n",
      "\t\tStep 4\n",
      "\t\tStep 5\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8\n",
      "\t\tStep 9\n",
      "\t\tStep 10: new best accuracy of 0.971\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\tCross-validation fold 3\n",
      "\t\tStep 1: new best accuracy of 0.000\n",
      "\t\tStep 2\n",
      "\t\tStep 3\n",
      "\t\tStep 4: new best accuracy of 0.743\n",
      "\t\tStep 5\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8\n",
      "\t\tStep 9\n",
      "\t\tStep 10\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15: new best accuracy of 1.000\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\tCross-validation fold 4\n",
      "\t\tStep 1: new best accuracy of 0.000\n",
      "\t\tStep 2: new best accuracy of 0.286\n",
      "\t\tStep 3\n",
      "\t\tStep 4: new best accuracy of 0.714\n",
      "\t\tStep 5\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8\n",
      "\t\tStep 9\n",
      "\t\tStep 10: new best accuracy of 0.743\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16: new best accuracy of 0.914\n",
      "\t\tStep 17\n",
      "\t\tStep 18: new best accuracy of 0.943\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23: new best accuracy of 0.971\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\tCross-validation fold 5\n",
      "\t\tStep 1: new best accuracy of 0.000\n",
      "\t\tStep 2\n",
      "\t\tStep 3\n",
      "\t\tStep 4\n",
      "\t\tStep 5\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8\n",
      "\t\tStep 9\n",
      "\t\tStep 10\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18: new best accuracy of 0.029\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29: new best accuracy of 0.286\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50: new best accuracy of 0.457\n",
      "\t\tStep 51: new best accuracy of 0.514\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58: new best accuracy of 0.686\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76: new best accuracy of 0.829\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84: new best accuracy of 0.857\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103: new best accuracy of 1.000\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\t\tStep 118\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "\t\tStep 126\n",
      "\t\tStep 127\n",
      "\t\tStep 128\n",
      "\n",
      "\n",
      "Digits training accuracy: 92.4%\n",
      "Digits training cost: 1440.348\n",
      "Digits validation accuracy: 90.9%\n",
      "Digits validation cost: 1491.439\n",
      "Wine training accuracy: 84.8%\n",
      "Wine training cost: 159.448\n",
      "Wine validation accuracy: 91.6%\n",
      "Wine validation cost: 161.621\n"
     ]
    }
   ],
   "source": [
    "\n",
    "default_run = runLogisticRegression(30, 0.04, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean = lambda x1, x2: np.sqrt(np.sum((x1 - x2)**2, axis=-1))\n",
    "manhattan = lambda x1, x2: np.sum(np.abs(x1 - x2), axis=-1)\n",
    "\n",
    "class KNN:\n",
    "\n",
    "    def __init__(self, K=1, dist_fn= euclidean):\n",
    "        self.dist_fn = dist_fn\n",
    "        self.K = K\n",
    "        return\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.C = len(y[0])\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        num_test = x_test.shape[0]\n",
    "        distances = self.dist_fn(self.x[None,:,:], x_test[:,None,:])\n",
    "        knns = np.zeros((num_test, self.K), dtype=int)\n",
    "        y_prob = np.zeros((num_test),dtype=int)\n",
    "        counts = np.zeros((num_test, self.C))\n",
    "        \n",
    "        for i in range(num_test):\n",
    "            knns[i,:] = np.argsort(distances[i])[:self.K]\n",
    "            k_count=np.zeros(self.K, dtype=int)\n",
    "            \n",
    "            for s, arr in enumerate(self.y[knns[i,:]]):\n",
    "                k_count[s] = np.argmax(arr)\n",
    "            \n",
    "            y_prob_i, counts_i = np.unique(k_count, return_counts=True)\n",
    "            y_prob[i] = int(y_prob_i[np.argmax(counts_i)])\n",
    "        \n",
    "        return y_prob, knns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN digits validation accuracy: 95.7%\n",
      "KNN wine validation accuracy: 85.4%\n"
     ]
    }
   ],
   "source": [
    "KNNmodel = KNN(K=11)\n",
    "\n",
    "digits_knn_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    y_prob, knns = KNNmodel.fit(np.asarray(xDigitsTrainingSets[fold]), np.asarray(yDigitsTrainingSets[fold])).predict(np.asarray(xDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, prob in enumerate(y_prob):\n",
    "        if prob == np.argmax(yDigitsValidationSets[fold][i]):\n",
    "            digits_knn_accuracy += 1\n",
    "\n",
    "digits_knn_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"KNN digits validation accuracy: {digits_knn_accuracy*100:.1f}%\")\n",
    "\n",
    "KNNmodel = KNN(K=7)\n",
    "\n",
    "wine_knn_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    y_prob, knns = KNNmodel.fit(np.asarray(xWineTrainingSets[fold]), np.asarray(yWineTrainingSets[fold])).predict(np.asarray(xWineValidationSets[fold]))\n",
    "    \n",
    "    for i, prob in enumerate(y_prob):\n",
    "        if prob == np.argmax(yWineValidationSets[fold][i]):\n",
    "            wine_knn_accuracy += 1\n",
    "            \n",
    "wine_knn_accuracy /= len(wine.data)\n",
    "\n",
    "print(f\"KNN wine validation accuracy: {wine_knn_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive base digits validation accuracy: 81.1%\n",
      "Naive base wine validation accuracy: 93.8%\n"
     ]
    }
   ],
   "source": [
    "digits_naive_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    labels_training = np.zeros(len(yDigitsTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(np.asarray(xDigitsTrainingSets[fold]), labels_training).predict(np.asarray(xDigitsValidationSets[fold]))\n",
    "\n",
    "    for i, label in enumerate(y_pred):\n",
    "        if label == labels_validation[i]:\n",
    "            digits_naive_accuracy += 1\n",
    "\n",
    "digits_naive_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"Naive base digits validation accuracy: {digits_naive_accuracy*100:.1f}%\")\n",
    "\n",
    "wine_naive_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    labels_training = np.zeros(len(yWineTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yWineTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yWineValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yWineValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(np.asarray(xWineTrainingSets[fold]), labels_training).predict(np.asarray(xWineValidationSets[fold]))\n",
    "\n",
    "    for i, label in enumerate(y_pred):\n",
    "        if label == labels_validation[i]:\n",
    "            wine_naive_accuracy += 1\n",
    "\n",
    "wine_naive_accuracy /= len(wine.data)\n",
    "\n",
    "print(f\"Naive base wine validation accuracy: {wine_naive_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HoGFeatures(img, cellSize, blockSize, nbins):\n",
    "    cell_size = (cellSize, cellSize)\n",
    "    block_size = (blockSize, blockSize)\n",
    "    \n",
    "    hog = cv.HOGDescriptor(_winSize=(img.shape[1] // cell_size[1] * cell_size[1],\n",
    "                                     img.shape[0] // cell_size[0] * cell_size[0]),\n",
    "                           _blockSize=(block_size[1] * cell_size[1],\n",
    "                                       block_size[0] * cell_size[0]),\n",
    "                           _blockStride=(cell_size[1], cell_size[0]),\n",
    "                           _cellSize=(cell_size[1], cell_size[0]),\n",
    "                           _nbins=nbins\n",
    "    )\n",
    "    \n",
    "    return hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeHoGFeatures(imageArray):\n",
    "    HoG = HoGFeatures(imageArray[0], 2, 2, 2)\n",
    "    features = []\n",
    "    \n",
    "    for i, image in enumerate(imageArray):\n",
    "        features.append(HoG.compute((image*255).astype(np.uint8)))\n",
    "        \n",
    "    features = np.array(np.squeeze(features))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC digits validation accuracy: 89.6%\n"
     ]
    }
   ],
   "source": [
    "digits_svc_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    numbers_training = []\n",
    "    numbers_validation = []\n",
    "    \n",
    "    for i, number in enumerate(xDigitsTrainingSets[fold]):\n",
    "        numbers_training.append(np.asarray(number).reshape(8, 8))\n",
    "        \n",
    "    for i, number in enumerate(xDigitsValidationSets[fold]):\n",
    "        numbers_validation.append(np.asarray(number).reshape(8, 8))  \n",
    "        \n",
    "    HoGs_training = makeHoGFeatures(np.asarray(numbers_training))\n",
    "    HoGs_validation = makeHoGFeatures(np.asarray(numbers_validation))\n",
    "\n",
    "    clf = svm.SVC(gamma='auto', C=100) \n",
    "    \n",
    "    labels_training = np.zeros(len(yDigitsTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    clf.fit(HoGs_training, labels_training)\n",
    "\n",
    "    labels_predicted = clf.predict(HoGs_validation)\n",
    "    \n",
    "    for i, label in enumerate(labels_predicted):\n",
    "        if label == labels_validation[i]:\n",
    "            digits_svc_accuracy += 1\n",
    "\n",
    "digits_svc_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"SVC digits validation accuracy: {digits_svc_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis of model hyper-parameters\n",
    "\n",
    "batch_size = 30\n",
    "learning_rate = 0.04\n",
    "momentum = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis of changing batch size\n",
    "if run_search:\n",
    "    batch_size_tests = np.array(range(2, 40, 2))#[30, 25, 20, 15, 10, 5]\n",
    "    batch_size_results = []\n",
    "\n",
    "    for test in batch_size_tests:\n",
    "        batch_size_results.append(runLogisticRegression(test, learning_rate, momentum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of changing learning rate\n",
    "if run_search:\n",
    "    learning_rate_tests = np.array(range(1, 20, 1))/100#[0.02, 0.04, 0.06, 0.08]\n",
    "    learning_rate_results = []\n",
    "\n",
    "    for test in learning_rate_tests:\n",
    "        learning_rate_results.append(runLogisticRegression(batch_size, test, momentum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of changing momentum\n",
    "if run_search:\n",
    "    momentum_tests = np.array(range(1, 20, 1))/20#[0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "    momentum_results = []\n",
    "\n",
    "    for test in momentum_tests:\n",
    "        momentum_results.append(runLogisticRegression(batch_size, learning_rate, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(batch_size_results[5][2])\n",
    "\n",
    "if run_search:\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_size_inches(7,7)\n",
    "\n",
    "    ax1.plot(batch_size_tests, np.asarray(batch_size_results).T[0], label = f\"Digits validation accuracy\", color='green',linewidth=4)\n",
    "    ax1.plot(batch_size_tests, np.asarray(batch_size_results).T[2], label = f\"Wine validation accuracy\", color='limegreen',linewidth=2)\n",
    "    ax1.set_ylabel('Validation Accuracy',color='green',fontsize=13)\n",
    "    ax1.tick_params(axis='y', labelcolor='green', labelsize=13)\n",
    "    ax1.set_xlabel('Batch Size',fontsize=13)\n",
    "    ax2 = ax1.twinx() \n",
    "\n",
    "    ax2.plot(batch_size_tests, np.asarray(batch_size_results).T[1]/len(digits.data), label = f\"Digits cost\", color='red',linewidth=4)\n",
    "    ax2.plot(batch_size_tests, np.asarray(batch_size_results).T[3]/len(wine.data), label = f\"Wine cost\", color='maroon',linewidth=2)\n",
    "    ax2.set_ylabel('Cost per sample',color='maroon',fontsize=13)\n",
    "    ax2.tick_params(axis='y', labelcolor='maroon', labelsize=13)\n",
    "\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "    plt.legend(handles+handles2, labels+labels2,loc='lower right')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(batch_size_results[5][2])\n",
    "\n",
    "if run_search:\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_size_inches(7,7)\n",
    "\n",
    "    ax1.plot(learning_rate_tests, np.asarray(learning_rate_results).T[0], label = f\"Digits validation accuracy\", color='green',linewidth=4)\n",
    "    ax1.plot(learning_rate_tests, np.asarray(learning_rate_results).T[2], label = f\"Wine validation accuracy\", color='limegreen',linewidth=2)\n",
    "    ax1.set_ylabel('Validation Accuracy',color='green',fontsize=13)\n",
    "    ax1.tick_params(axis='y', labelcolor='green', labelsize=13)\n",
    "    ax1.set_xlabel('Learning Rate',fontsize=13)\n",
    "    ax2 = ax1.twinx() \n",
    "\n",
    "    ax2.plot(learning_rate_tests, np.asarray(learning_rate_results).T[1]/len(digits.data), label = f\"Digits cost\", color='red',linewidth=4)\n",
    "    ax2.plot(learning_rate_tests, np.asarray(learning_rate_results).T[3]/len(wine.data), label = f\"Wine cost\", color='maroon',linewidth=2)\n",
    "    ax2.set_ylabel('Cost per sample',color='maroon',fontsize=13)\n",
    "    ax2.tick_params(axis='y', labelcolor='maroon', labelsize=13)\n",
    "\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "    plt.legend(handles+handles2, labels+labels2,loc='lower left')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(batch_size_results[5][2])\n",
    "\n",
    "if run_search:\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_size_inches(7,7)\n",
    "\n",
    "    ax1.plot(momentum_tests, np.asarray(momentum_results).T[0], label = f\"Digits validation accuracy\", color='green',linewidth=4)\n",
    "    ax1.plot(momentum_tests, np.asarray(momentum_results).T[2], label = f\"Wine validation accuracy\", color='limegreen',linewidth=2)\n",
    "    ax1.set_ylabel('Validation Accuracy',color='green',fontsize=13)\n",
    "    ax1.tick_params(axis='y', labelcolor='green', labelsize=13)\n",
    "    ax1.set_xlabel('Momentum',fontsize=13)\n",
    "    ax2 = ax1.twinx() \n",
    "\n",
    "    ax2.plot(momentum_tests, np.asarray(momentum_results).T[1]/len(digits.data), label = f\"Digits cost\", color='red',linewidth=4)\n",
    "    ax2.plot(momentum_tests, np.asarray(momentum_results).T[3]/len(wine.data), label = f\"Wine cost\", color='maroon',linewidth=2)\n",
    "    ax2.set_ylabel('Cost per sample',color='maroon',fontsize=13)\n",
    "    ax2.tick_params(axis='y', labelcolor='maroon', labelsize=13)\n",
    "\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "    plt.legend(handles+handles2, labels+labels2,loc='lower right')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits_validation_accuracies = [default_run[0],digits_knn_accuracy,digits_naive_accuracy,digits_svc_accuracy]\n",
    "wine_validation_accuracies = [default_run[2],wine_knn_accuracy,wine_naive_accuracy,0]\n",
    "\n",
    "labels = [\"Logistic Regression\",\"KNN\",\"Naive Bayes\",\"SVC\"]\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, digits_validation_accuracies, width, label='Digits')\n",
    "rects2 = ax.bar(x + width/2, wine_validation_accuracies, width, label='Wine')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Validation Accuracies')\n",
    "ax.set_title('Comparison between different classifiers')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
