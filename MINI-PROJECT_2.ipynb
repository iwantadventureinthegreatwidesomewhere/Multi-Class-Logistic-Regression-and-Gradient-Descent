{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797\n",
      "1797\n"
     ]
    }
   ],
   "source": [
    "print(len(digits.data))\n",
    "print(digits.target.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "wine = fetch_openml(name='wine', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n",
      "178\n"
     ]
    }
   ],
   "source": [
    "print(len(wine.data))\n",
    "print(wine.target.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation for digits dataset\n",
    "\n",
    "digitsTrainingSetSize = int(np.ceil(0.8 * len(digits.data)))\n",
    "digitsValidationSetSize = int(len(digits.data) - digitsTrainingSetSize)\n",
    "\n",
    "xDigitsTrainingSets = []\n",
    "yDigitsTrainingSets = []\n",
    "xDigitsValidationSets = []\n",
    "yDigitsValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data[foldIndex*digitsValidationSetSize:((foldIndex*digitsValidationSetSize)+digitsValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(digits.target[index])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(digits.target[index])\n",
    "            \n",
    "    xDigitsTrainingSets.append(xTrainingSet)\n",
    "    yDigitsTrainingSets.append(yTrainingSet)\n",
    "    xDigitsValidationSets.append(xValidationSet)\n",
    "    yDigitsValidationSets.append(yValidationSet)\n",
    "    \n",
    "# 5-fold cross validation for wine dataset\n",
    "\n",
    "wineTrainingSetSize = int(np.ceil(0.8 * len(wine.data)))\n",
    "wineValidationSetSize = int(len(wine.data) - wineTrainingSetSize)\n",
    "\n",
    "xWineTrainingSets = []\n",
    "yWineTrainingSets = []\n",
    "xWineValidationSets = []\n",
    "yWineValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "\n",
    "    for index, data in enumerate(wine.data[foldIndex*wineValidationSetSize:((foldIndex*wineValidationSetSize)+wineValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(wine.target[index])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "\n",
    "    for index, data in enumerate(wine.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(wine.target[index])\n",
    "            \n",
    "    xWineTrainingSets.append(xTrainingSet)\n",
    "    yWineTrainingSets.append(yTrainingSet)\n",
    "    xWineValidationSets.append(xValidationSet)\n",
    "    yWineValidationSets.append(yValidationSet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of y for digits dataset\n",
    "\n",
    "numberOfDigitsTargets = 10\n",
    "numberOfWineTargets = 3\n",
    "\n",
    "for index, fold in enumerate(yDigitsTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yDigitsValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsValidationSets[index] = encodedFold\n",
    "\n",
    "# one-hot encoding of y for wine dataset\n",
    "\n",
    "for index, fold in enumerate(yWineTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yWineValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineValidationSets[index] = encodedFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomIndices(arr, batch_size):\n",
    "    indices = []\n",
    "    \n",
    "    if batch_size > len(arr):\n",
    "        print(\"Error: batch size larger than size of dataset.\")\n",
    "        return\n",
    "    \n",
    "    while batch_size > 0:\n",
    "        x = np.floor(np.random.random() * len(arr))\n",
    "        if x not in indices:\n",
    "            indices.append(int(x))\n",
    "            batch_size -= 1\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent class\n",
    " \n",
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, batch_size, learning_rate=.001, momentum=0.9, max_iters=10, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iters = max_iters\n",
    "        self.epsilon = epsilon\n",
    "            \n",
    "    def run(self, gradient_fn, x, y, w):\n",
    "        # TODO: use epsilon\n",
    "        \n",
    "        grad = np.inf\n",
    "        t = 1\n",
    "        \n",
    "        while np.linalg.norm(grad) > self.epsilon and t < self.max_iters:\n",
    "            # TODO: implement momentum here\n",
    "            \n",
    "            print(\"gradient descent step:\", t)\n",
    "            \n",
    "            gradients = gradient_fn(x, y, w, self.batch_size)\n",
    "            \n",
    "            for c in range(len(y[0])):\n",
    "                w[c] = w[c] - self.learning_rate * gradients[c]\n",
    "                print(\"w for class: \", w[c])\n",
    "            \n",
    "            print(\"###################################################\")\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, add_bias=True):\n",
    "        self.add_bias = add_bias\n",
    "        pass\n",
    "            \n",
    "    def fit(self, x, y, optimizer):\n",
    "        # TODO: add bias\n",
    "        \n",
    "        def gradient(x, y, w, batch_size):\n",
    "            gradients = np.zeros(len(w)).tolist()\n",
    "\n",
    "            indices = getRandomIndices(x, batch_size)\n",
    "            \n",
    "            print(\"indices for batch:\", indices)\n",
    "\n",
    "            for index in indices:\n",
    "                a = np.asarray(x[index])\n",
    "                b = np.asarray(y[index])\n",
    "                \n",
    "                print(\"x:\", a)\n",
    "                print(\"y:\", b)\n",
    "                \n",
    "                # do max normalization on input for\n",
    "                # numerical stability during softmax\n",
    "                \n",
    "                max_x = np.amax(a)\n",
    "                a = a - max_x\n",
    "\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  w[c] @ a\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  w[i] @ a\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "\n",
    "                    y_c = b[c]\n",
    "                    cost_c = np.dot(yh_c - y_c, a)\n",
    "                    \n",
    "                    gradients[c] += cost_c\n",
    "                    \n",
    "                    print(\"class:\", c)\n",
    "                    print(\"softmax numerator:\", num)\n",
    "                    print(\"softmax denominator:\", den)\n",
    "                    print(\"y hat for class:\", yh_c)\n",
    "                    print(\"y actual for class:\", y_c)\n",
    "                    print(\"x gradient:\", cost_c)\n",
    "                    print(\"new gradient for class:\", gradients[c])\n",
    "\n",
    "            return gradients\n",
    "        \n",
    "        w0 = []\n",
    "        for c in range(len(y[0])):\n",
    "            w0.append(np.zeros(len(x[0])))\n",
    "            \n",
    "        self.w = optimizer.run(gradient, x, y, w0)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # TODO: not tested yet, so not sure if it works\n",
    "        \n",
    "        if self.add_bias:\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "        yh = x@self.w\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientDescentModel = GradientDescent(2)\n",
    "logisticRegressionModel = LogisticRegression(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient descent step: 1\n",
      "indices for batch: [120, 123]\n",
      "x: [1.317e+01 5.190e+00 2.320e+00 2.200e+01 9.300e+01 1.740e+00 6.300e-01\n",
      " 6.100e-01 1.550e+00 7.900e+00 6.000e-01 1.480e+00 7.250e+02]\n",
      "y: [0. 0. 1.]\n",
      "class: 0\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 0.0\n",
      "x gradient: [-237.27666667 -239.93666667 -240.89333333 -234.33333333 -210.66666667\n",
      " -241.08666667 -241.45666667 -241.46333333 -241.15       -239.03333333\n",
      " -241.46666667 -241.17333333    0.        ]\n",
      "new gradient for class: [-237.27666667 -239.93666667 -240.89333333 -234.33333333 -210.66666667\n",
      " -241.08666667 -241.45666667 -241.46333333 -241.15       -239.03333333\n",
      " -241.46666667 -241.17333333    0.        ]\n",
      "class: 1\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 0.0\n",
      "x gradient: [-237.27666667 -239.93666667 -240.89333333 -234.33333333 -210.66666667\n",
      " -241.08666667 -241.45666667 -241.46333333 -241.15       -239.03333333\n",
      " -241.46666667 -241.17333333    0.        ]\n",
      "new gradient for class: [-237.27666667 -239.93666667 -240.89333333 -234.33333333 -210.66666667\n",
      " -241.08666667 -241.45666667 -241.46333333 -241.15       -239.03333333\n",
      " -241.46666667 -241.17333333    0.        ]\n",
      "class: 2\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 1.0\n",
      "x gradient: [474.55333333 479.87333333 481.78666667 468.66666667 421.33333333\n",
      " 482.17333333 482.91333333 482.92666667 482.3        478.06666667\n",
      " 482.93333333 482.34666667   0.        ]\n",
      "new gradient for class: [474.55333333 479.87333333 481.78666667 468.66666667 421.33333333\n",
      " 482.17333333 482.91333333 482.92666667 482.3        478.06666667\n",
      " 482.93333333 482.34666667   0.        ]\n",
      "x: [1.434e+01 1.680e+00 2.700e+00 2.500e+01 9.800e+01 2.800e+00 1.310e+00\n",
      " 5.300e-01 2.700e+00 1.300e+01 5.700e-01 1.960e+00 6.600e+02]\n",
      "y: [0. 0. 1.]\n",
      "class: 0\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 0.0\n",
      "x gradient: [-215.22       -219.44       -219.1        -211.66666667 -187.33333333\n",
      " -219.06666667 -219.56333333 -219.82333333 -219.1        -215.66666667\n",
      " -219.81       -219.34666667    0.        ]\n",
      "new gradient for class: [-452.49666667 -459.37666667 -459.99333333 -446.         -398.\n",
      " -460.15333333 -461.02       -461.28666667 -460.25       -454.7\n",
      " -461.27666667 -460.52          0.        ]\n",
      "class: 1\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 0.0\n",
      "x gradient: [-215.22       -219.44       -219.1        -211.66666667 -187.33333333\n",
      " -219.06666667 -219.56333333 -219.82333333 -219.1        -215.66666667\n",
      " -219.81       -219.34666667    0.        ]\n",
      "new gradient for class: [-452.49666667 -459.37666667 -459.99333333 -446.         -398.\n",
      " -460.15333333 -461.02       -461.28666667 -460.25       -454.7\n",
      " -461.27666667 -460.52          0.        ]\n",
      "class: 2\n",
      "softmax numerator: 1.0\n",
      "softmax denominator: 3.0\n",
      "y hat for class: 0.3333333333333333\n",
      "y actual for class: 1.0\n",
      "x gradient: [430.44       438.88       438.2        423.33333333 374.66666667\n",
      " 438.13333333 439.12666667 439.64666667 438.2        431.33333333\n",
      " 439.62       438.69333333   0.        ]\n",
      "new gradient for class: [904.99333333 918.75333333 919.98666667 892.         796.\n",
      " 920.30666667 922.04       922.57333333 920.5        909.4\n",
      " 922.55333333 921.04         0.        ]\n",
      "w for class:  [0.45249667 0.45937667 0.45999333 0.446      0.398      0.46015333\n",
      " 0.46102    0.46128667 0.46025    0.4547     0.46127667 0.46052\n",
      " 0.        ]\n",
      "w for class:  [0.45249667 0.45937667 0.45999333 0.446      0.398      0.46015333\n",
      " 0.46102    0.46128667 0.46025    0.4547     0.46127667 0.46052\n",
      " 0.        ]\n",
      "w for class:  [-0.90499333 -0.91875333 -0.91998667 -0.892      -0.796      -0.92030667\n",
      " -0.92204    -0.92257333 -0.9205     -0.9094     -0.92255333 -0.92104\n",
      "  0.        ]\n",
      "###################################################\n",
      "gradient descent step: 2\n",
      "indices for batch: [58, 65]\n",
      "x: [1.229e+01 2.830e+00 2.220e+00 1.800e+01 8.800e+01 2.450e+00 2.250e+00\n",
      " 2.500e-01 1.990e+00 2.150e+00 1.150e+00 3.300e+00 2.900e+02]\n",
      "y: [0. 1. 0.]\n",
      "class: 0\n",
      "softmax numerator: 0.0\n",
      "softmax denominator: inf\n",
      "y hat for class: 0.0\n",
      "y actual for class: 0.0\n",
      "x gradient: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "new gradient for class: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "class: 1\n",
      "softmax numerator: 0.0\n",
      "softmax denominator: inf\n",
      "y hat for class: 0.0\n",
      "y actual for class: 1.0\n",
      "x gradient: [277.71 287.17 287.78 272.   202.   287.55 287.75 289.75 288.01 287.85\n",
      " 288.85 286.7    0.  ]\n",
      "new gradient for class: [277.71 287.17 287.78 272.   202.   287.55 287.75 289.75 288.01 287.85\n",
      " 288.85 286.7    0.  ]\n",
      "class: 2\n",
      "softmax numerator: inf\n",
      "softmax denominator: inf\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "x: [1.208e+01 2.080e+00 1.700e+00 1.750e+01 9.700e+01 2.230e+00 2.170e+00\n",
      " 2.600e-01 1.400e+00 3.300e+00 1.270e+00 2.960e+00 7.100e+02]\n",
      "y: [0. 1. 0.]\n",
      "class: 0\n",
      "softmax numerator: 0.0\n",
      "softmax denominator: inf\n",
      "y hat for class: 0.0\n",
      "y actual for class: 0.0\n",
      "x gradient: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "new gradient for class: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "class: 1\n",
      "softmax numerator: 0.0\n",
      "softmax denominator: inf\n",
      "y hat for class: 0.0\n",
      "y actual for class: 1.0\n",
      "x gradient: [697.92 707.92 708.3  692.5  613.   707.77 707.83 709.74 708.6  706.7\n",
      " 708.73 707.04   0.  ]\n",
      "new gradient for class: [975.63 995.09 996.08 964.5  815.   995.32 995.58 999.49 996.61 994.55\n",
      " 997.58 993.74   0.  ]\n",
      "class: 2\n",
      "softmax numerator: inf\n",
      "softmax denominator: inf\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [0.45249667 0.45937667 0.45999333 0.446      0.398      0.46015333\n",
      " 0.46102    0.46128667 0.46025    0.4547     0.46127667 0.46052\n",
      " 0.        ]\n",
      "w for class:  [-0.52313333 -0.53571333 -0.53608667 -0.5185     -0.417      -0.53516667\n",
      " -0.53456    -0.53820333 -0.53636    -0.53985    -0.53630333 -0.53322\n",
      "  0.        ]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "###################################################\n",
      "gradient descent step: 3\n",
      "indices for batch: [35, 15]\n",
      "x: [1.229e+01 1.610e+00 2.210e+00 2.040e+01 1.030e+02 1.100e+00 1.020e+00\n",
      " 3.700e-01 1.460e+00 3.050e+00 9.060e-01 1.820e+00 8.700e+02]\n",
      "y: [0. 1. 0.]\n",
      "class: 0\n",
      "softmax numerator: 0.0\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 1\n",
      "softmax numerator: inf\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 1.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 2\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "x: [1.305e+01 1.730e+00 2.040e+00 1.240e+01 9.200e+01 2.720e+00 3.270e+00\n",
      " 1.700e-01 2.910e+00 7.200e+00 1.120e+00 2.910e+00 1.150e+03]\n",
      "y: [1. 0. 0.]\n",
      "class: 0\n",
      "softmax numerator: 0.0\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 1.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 1\n",
      "softmax numerator: inf\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 2\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "###################################################\n",
      "gradient descent step: 4\n",
      "indices for batch: [83, 49]\n",
      "x: [ 12.77   3.43   1.98  16.    80.     1.63   1.25   0.43   0.83   3.4\n",
      "   0.7    2.12 372.  ]\n",
      "y: [0. 1. 0.]\n",
      "class: 0\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 1\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 1.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 2\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "x: [1.184e+01 8.900e-01 2.580e+00 1.800e+01 9.400e+01 2.200e+00 2.210e+00\n",
      " 2.200e-01 2.350e+00 3.050e+00 7.900e-01 3.080e+00 5.200e+02]\n",
      "y: [0. 1. 0.]\n",
      "class: 0\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 1\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 1.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 2\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "###################################################\n",
      "gradient descent step: 5\n",
      "indices for batch: [44, 123]\n",
      "x: [1.27e+01 3.87e+00 2.40e+00 2.30e+01 1.01e+02 2.83e+00 2.55e+00 4.30e-01\n",
      " 1.95e+00 2.57e+00 1.19e+00 3.13e+00 4.63e+02]\n",
      "y: [0. 1. 0.]\n",
      "class: 0\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 1\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 1.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 2\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "x: [1.434e+01 1.680e+00 2.700e+00 2.500e+01 9.800e+01 2.800e+00 1.310e+00\n",
      " 5.300e-01 2.700e+00 1.300e+01 5.700e-01 1.960e+00 6.600e+02]\n",
      "y: [0. 0. 1.]\n",
      "class: 0\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 1\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 2\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 1.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "###################################################\n",
      "gradient descent step: 6\n",
      "indices for batch: [83, 106]\n",
      "x: [ 12.77   3.43   1.98  16.    80.     1.63   1.25   0.43   0.83   3.4\n",
      "   0.7    2.12 372.  ]\n",
      "y: [0. 1. 0.]\n",
      "class: 0\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 1\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 1.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 2\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "x: [1.336e+01 2.560e+00 2.350e+00 2.000e+01 8.900e+01 1.400e+00 5.000e-01\n",
      " 3.700e-01 6.400e-01 5.600e+00 7.000e-01 2.470e+00 7.800e+02]\n",
      "y: [0. 0. 1.]\n",
      "class: 0\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 1\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 2\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 1.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "###################################################\n",
      "gradient descent step: 7\n",
      "indices for batch: [112, 96]\n",
      "x: [1.287e+01 4.610e+00 2.480e+00 2.150e+01 8.600e+01 1.700e+00 6.500e-01\n",
      " 4.700e-01 8.600e-01 7.650e+00 5.400e-01 1.860e+00 6.250e+02]\n",
      "y: [0. 0. 1.]\n",
      "class: 0\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 1\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 2\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 1.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "x: [1.288e+01 2.990e+00 2.400e+00 2.000e+01 1.040e+02 1.300e+00 1.220e+00\n",
      " 2.400e-01 8.300e-01 5.400e+00 7.400e-01 1.420e+00 5.300e+02]\n",
      "y: [0. 0. 1.]\n",
      "class: 0\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 1\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 2\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 1.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "###################################################\n",
      "gradient descent step: 8\n",
      "indices for batch: [20, 133]\n",
      "x: [1.356e+01 1.730e+00 2.460e+00 2.050e+01 1.160e+02 2.960e+00 2.780e+00\n",
      " 2.000e-01 2.450e+00 6.250e+00 9.800e-01 3.030e+00 1.120e+03]\n",
      "y: [1. 0. 0.]\n",
      "class: 0\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 1.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 1\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 2\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "x: [1.358e+01 2.580e+00 2.690e+00 2.450e+01 1.050e+02 1.550e+00 8.400e-01\n",
      " 3.900e-01 1.540e+00 8.660e+00 7.400e-01 1.800e+00 7.500e+02]\n",
      "y: [0. 0. 1.]\n",
      "class: 0\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 1\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 2\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 1.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "###################################################\n",
      "gradient descent step: 9\n",
      "indices for batch: [121, 5]\n",
      "x: [ 13.84   4.12   2.38  19.5   89.     1.8    0.83   0.48   1.56   9.01\n",
      "   0.57   1.64 480.  ]\n",
      "y: [0. 0. 1.]\n",
      "class: 0\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 1\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 2\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 1.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "x: [1.356e+01 1.710e+00 2.310e+00 1.620e+01 1.170e+02 3.150e+00 3.290e+00\n",
      " 3.400e-01 2.340e+00 6.130e+00 9.500e-01 3.380e+00 7.950e+02]\n",
      "y: [1. 0. 0.]\n",
      "class: 0\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 1.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 1\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "class: 2\n",
      "softmax numerator: nan\n",
      "softmax denominator: nan\n",
      "y hat for class: nan\n",
      "y actual for class: 0.0\n",
      "x gradient: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "new gradient for class: [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "w for class:  [nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "###################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-6ab88e174752>:38: RuntimeWarning: overflow encountered in exp\n",
      "  den += np.exp(w_x)\n",
      "<ipython-input-10-6ab88e174752>:33: RuntimeWarning: overflow encountered in exp\n",
      "  num = np.exp(w_x)\n",
      "<ipython-input-10-6ab88e174752>:40: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  yh_c = num/den\n"
     ]
    }
   ],
   "source": [
    "xtest = np.asarray([[1, 2, 3, 4], [5, 6, 7, 8], [1, 1, 2, 2]])\n",
    "ytest = np.asarray([[0, 1, 0], [1, 0, 1], [0, 0, 1]])\n",
    "\n",
    "logisticRegressionModel.fit(xWineTrainingSets[0], yWineTrainingSets[0], gradientDescentModel)\n",
    "\n",
    "#logisticRegressionModel.fit(xtest, ytest, gradientDescentModel)\n",
    "\n",
    "\n",
    "\n",
    "#xWineTrainingSets = []\n",
    "#yWineTrainingSets = []\n",
    "#xWineValidationSets = []\n",
    "#yWineValidationSets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
