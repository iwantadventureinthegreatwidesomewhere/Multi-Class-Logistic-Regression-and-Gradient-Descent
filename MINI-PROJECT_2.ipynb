{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lia\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import cv2 as cv\n",
    "from sklearn import svm \n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "wine = fetch_openml(name='wine', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of digits and wine data\n",
    "\n",
    "digits_data_norm = []\n",
    "\n",
    "for col in digits.data:\n",
    "    col_norm = col/np.max(col)\n",
    "    digits_data_norm.append(col_norm)\n",
    "\n",
    "digits.data = np.asarray(digits_data_norm)\n",
    "\n",
    "wine_data_norm = []\n",
    "\n",
    "for col in wine.data.T:\n",
    "    col_norm = col/np.amax(col)\n",
    "    wine_data_norm.append(col_norm)\n",
    "    \n",
    "wine.data = np.asarray(wine_data_norm).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation for digits dataset\n",
    "\n",
    "digitsTrainingSetSize = int(np.ceil(0.8 * len(digits.data)))\n",
    "digitsValidationSetSize = int(len(digits.data) - digitsTrainingSetSize)\n",
    "\n",
    "xDigitsTrainingSets = []\n",
    "yDigitsTrainingSets = []\n",
    "xDigitsValidationSets = []\n",
    "yDigitsValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data[foldIndex*digitsValidationSetSize:((foldIndex*digitsValidationSetSize)+digitsValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(digits.target[index+(foldIndex*digitsValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(digits.target[index])\n",
    "            \n",
    "    xDigitsTrainingSets.append(xTrainingSet)\n",
    "    yDigitsTrainingSets.append(yTrainingSet)\n",
    "    xDigitsValidationSets.append(xValidationSet)\n",
    "    yDigitsValidationSets.append(yValidationSet)\n",
    "    \n",
    "# 5-fold cross validation for wine dataset\n",
    "\n",
    "wineTrainingSetSize = int(np.ceil(0.8 * len(wine.data)))\n",
    "wineValidationSetSize = int(len(wine.data) - wineTrainingSetSize)\n",
    "\n",
    "xWineTrainingSets = []\n",
    "yWineTrainingSets = []\n",
    "xWineValidationSets = []\n",
    "yWineValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "    for index, data in enumerate(wine.data[foldIndex*wineValidationSetSize:((foldIndex*wineValidationSetSize)+wineValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(wine.target[index+(foldIndex*wineValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "    \n",
    "    for index, data in enumerate(wine.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(wine.target[index])\n",
    "            \n",
    "    xWineTrainingSets.append(xTrainingSet)\n",
    "    yWineTrainingSets.append(yTrainingSet)\n",
    "    xWineValidationSets.append(xValidationSet)\n",
    "    yWineValidationSets.append(yValidationSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of y for digits dataset\n",
    "\n",
    "numberOfDigitsTargets = 10\n",
    "numberOfWineTargets = 3\n",
    "\n",
    "for index, fold in enumerate(yDigitsTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yDigitsValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsValidationSets[index] = encodedFold\n",
    "\n",
    "# one-hot encoding of y for wine dataset\n",
    "\n",
    "for index, fold in enumerate(yWineTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yWineValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineValidationSets[index] = encodedFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomIndices(arr, batch_size):\n",
    "    indices = []\n",
    "    \n",
    "    if batch_size > len(arr):\n",
    "        print(\"Error: batch size larger than size of dataset.\")\n",
    "        return\n",
    "    \n",
    "    while batch_size > 0:\n",
    "        x = np.floor(np.random.random() * len(arr))\n",
    "        if x not in indices:\n",
    "            indices.append(int(x))\n",
    "            batch_size -= 1\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent class\n",
    " \n",
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, batch_size, learning_rate=0.5, momentum=0.9, max_termination=25, max_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        self.max_termination = max_termination\n",
    "        self.max_iters = max_iters\n",
    "        self.deltas = []\n",
    "        \n",
    "    def run(self, gradient_fn, x, y, w):\n",
    "        t = 1\n",
    "        \n",
    "        max_accuracy = -1\n",
    "        termination_count = 0        \n",
    "        weight_history = []\n",
    "        error_history = []\n",
    "                \n",
    "        for number_of_targets in range(len(y[0])):\n",
    "            weight_history.append([])\n",
    "        \n",
    "        while termination_count < self.max_termination and t < self.max_iters:\n",
    "            gradients = gradient_fn(x, y, w, self.batch_size)   \n",
    "            \n",
    "            for c in range(len(y[0])):\n",
    "                if(t==1):\n",
    "                    w[c] = w[c] - self.learning_rate * gradients[c]\n",
    "                else:\n",
    "                    delta_w = (self.momentum)*(self.deltas[-(len(y[0]))]) + (1-self.momentum)*gradients[c]\n",
    "                    w[c] = w[c] - (self.learning_rate)*(delta_w)\n",
    "                self.deltas.append(w[c])\n",
    "            \n",
    "            a = np.asarray(x)\n",
    "            b = np.asarray(w)\n",
    "    \n",
    "            yh=[]\n",
    "            for i, x_c in enumerate(a):\n",
    "                yh_x=[]\n",
    "\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  b[c] @ x_c\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  b[i] @ x_c\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "                    yh_x.append(yh_c)\n",
    "                    \n",
    "                yh.append(yh_x)\n",
    "                \n",
    "            step_accuracy = 0\n",
    "                \n",
    "            def accurate(a, b):\n",
    "                return np.argmax(a) == np.argmax(b)\n",
    "                \n",
    "            for sample_index, yh_x in enumerate(yh):\n",
    "                if accurate(yh_x, y[sample_index]):\n",
    "                    step_accuracy += 1\n",
    "                    \n",
    "            step_accuracy /= len(x)\n",
    "            \n",
    "            for c in range(len(b)):\n",
    "                weight_history[c].append(w[c])\n",
    "            \n",
    "            error_history.append(step_accuracy)\n",
    "            \n",
    "            # We use an alternate termination condition that terminates faster as\n",
    "            # the suggested condition ran for a longtime for us (~1hr).\n",
    "            \n",
    "            # We track the best training accuracy encountered, and if the \n",
    "            # next max_termination-number of steps do not have a better accuracy, then\n",
    "            # it terminates.\n",
    "\n",
    "            if step_accuracy > max_accuracy:\n",
    "                max_accuracy = step_accuracy\n",
    "                termination_count = 0\n",
    "                print(f\"\\t\\tStep {t}: new best accuracy of {max_accuracy:.3f}\")\n",
    "            else:\n",
    "                termination_count += 1\n",
    "                print(f\"\\t\\tStep {t}\")\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        # take the weight prior to the last max_termination-number of weights\n",
    "        # (as it is guaranteed to be the best prior to termination).\n",
    "        \n",
    "        index_best = len(error_history)-self.max_termination-1\n",
    "        \n",
    "        w_best = []\n",
    "        \n",
    "        for c in range(len(y[0])):\n",
    "            w_best.append(weight_history[c][index_best])\n",
    "        \n",
    "        return w_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, add_bias=True):\n",
    "        self.add_bias = add_bias\n",
    "        pass\n",
    "            \n",
    "    def fit(self, x, y, optimizer):\n",
    "        def gradient(x, y, w, batch_size):\n",
    "            gradients = np.zeros(len(w)).tolist()\n",
    "\n",
    "            indices = getRandomIndices(x, batch_size)\n",
    "\n",
    "            for index in indices:\n",
    "                a = np.asarray(x[index])\n",
    "                b = np.asarray(y[index])\n",
    "\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  w[c] @ a\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  w[i] @ a\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "\n",
    "                    y_c = b[c]\n",
    "                    \n",
    "                    cost_c = np.dot(yh_c - y_c, a)\n",
    "                    \n",
    "                    gradients[c] += cost_c\n",
    "\n",
    "            return gradients\n",
    "        \n",
    "        if self.add_bias:\n",
    "            x = np.asarray(x)\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "\n",
    "        w0 = []\n",
    "        for c in range(len(y[0])):\n",
    "            w0.append(np.zeros(len(x[0])))\n",
    "            \n",
    "        self.w = optimizer.run(gradient, x, y, w0)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if self.add_bias:\n",
    "            x = np.asarray(x)\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "\n",
    "        a = np.asarray(x)\n",
    "        b = np.asarray(self.w)\n",
    "\n",
    "        yh=[]\n",
    "        \n",
    "        for i, x_c in enumerate(a):\n",
    "            yh_x=[]\n",
    "            \n",
    "            for c in range(len(b)):\n",
    "                w_x =  b[c] @ x_c\n",
    "                num = np.exp(w_x)\n",
    "\n",
    "                den = 0\n",
    "                for i in range(len(b)):\n",
    "                    w_x =  b[i] @ x_c\n",
    "                    den += np.exp(w_x)\n",
    "\n",
    "                yh_c = num/den\n",
    "                yh_x.append(yh_c)\n",
    "                \n",
    "            yh.append(yh_x)\n",
    "        \n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hyper-parameters:\n",
      "\tMini-batch size: 30\n",
      "\tLearning rate: 0.04\n",
      "\tMomentum: 0.2\n",
      "\n",
      "\n",
      "Digits gradient descent:\n",
      "\tCross-validation fold 1\n",
      "\t\tStep 1: new best accuracy of 0.178\n",
      "\t\tStep 2: new best accuracy of 0.197\n",
      "\t\tStep 3: new best accuracy of 0.381\n",
      "\t\tStep 4\n",
      "\t\tStep 5: new best accuracy of 0.545\n",
      "\t\tStep 6\n",
      "\t\tStep 7: new best accuracy of 0.634\n",
      "\t\tStep 8: new best accuracy of 0.708\n",
      "\t\tStep 9: new best accuracy of 0.750\n",
      "\t\tStep 10\n",
      "\t\tStep 11: new best accuracy of 0.800\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19: new best accuracy of 0.866\n",
      "\t\tStep 20: new best accuracy of 0.871\n",
      "\t\tStep 21: new best accuracy of 0.890\n",
      "\t\tStep 22: new best accuracy of 0.908\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31: new best accuracy of 0.918\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40: new best accuracy of 0.930\n",
      "\t\tStep 41\n",
      "\t\tStep 42: new best accuracy of 0.939\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58: new best accuracy of 0.942\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67: new best accuracy of 0.943\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79: new best accuracy of 0.944\n",
      "\t\tStep 80: new best accuracy of 0.945\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91: new best accuracy of 0.947\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115\n",
      "\t\tStep 116\n",
      "Best accuracy: 0.9471488178025035\n",
      "\tCross-validation fold 2\n",
      "\t\tStep 1: new best accuracy of 0.273\n",
      "\t\tStep 2: new best accuracy of 0.445\n",
      "\t\tStep 3\n",
      "\t\tStep 4: new best accuracy of 0.460\n",
      "\t\tStep 5: new best accuracy of 0.567\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8: new best accuracy of 0.578\n",
      "\t\tStep 9: new best accuracy of 0.619\n",
      "\t\tStep 10: new best accuracy of 0.715\n",
      "\t\tStep 11\n",
      "\t\tStep 12: new best accuracy of 0.762\n",
      "\t\tStep 13: new best accuracy of 0.782\n",
      "\t\tStep 14: new best accuracy of 0.807\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17: new best accuracy of 0.894\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25: new best accuracy of 0.905\n",
      "\t\tStep 26\n",
      "\t\tStep 27: new best accuracy of 0.922\n",
      "\t\tStep 28\n",
      "\t\tStep 29: new best accuracy of 0.924\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42: new best accuracy of 0.925\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45: new best accuracy of 0.926\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49: new best accuracy of 0.931\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65: new best accuracy of 0.932\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68: new best accuracy of 0.933\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79: new best accuracy of 0.942\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94: new best accuracy of 0.945\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112: new best accuracy of 0.946\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\t\tStep 118\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "\t\tStep 126\n",
      "\t\tStep 127\n",
      "\t\tStep 128\n",
      "\t\tStep 129\n",
      "\t\tStep 130\n",
      "\t\tStep 131\n",
      "\t\tStep 132\n",
      "\t\tStep 133\n",
      "\t\tStep 134\n",
      "\t\tStep 135\n",
      "\t\tStep 136\n",
      "\t\tStep 137: new best accuracy of 0.952\n",
      "\t\tStep 138\n",
      "\t\tStep 139: new best accuracy of 0.953\n",
      "\t\tStep 140\n",
      "\t\tStep 141\n",
      "\t\tStep 142\n",
      "\t\tStep 143\n",
      "\t\tStep 144\n",
      "\t\tStep 145\n",
      "\t\tStep 146\n",
      "\t\tStep 147\n",
      "\t\tStep 148\n",
      "\t\tStep 149\n",
      "\t\tStep 150\n",
      "\t\tStep 151\n",
      "\t\tStep 152\n",
      "\t\tStep 153\n",
      "\t\tStep 154\n",
      "\t\tStep 155\n",
      "\t\tStep 156\n",
      "\t\tStep 157\n",
      "\t\tStep 158\n",
      "\t\tStep 159\n",
      "\t\tStep 160: new best accuracy of 0.954\n",
      "\t\tStep 161\n",
      "\t\tStep 162\n",
      "\t\tStep 163\n",
      "\t\tStep 164\n",
      "\t\tStep 165\n",
      "\t\tStep 166\n",
      "\t\tStep 167: new best accuracy of 0.955\n",
      "\t\tStep 168\n",
      "\t\tStep 169\n",
      "\t\tStep 170\n",
      "\t\tStep 171\n",
      "\t\tStep 172\n",
      "\t\tStep 173\n",
      "\t\tStep 174\n",
      "\t\tStep 175\n",
      "\t\tStep 176\n",
      "\t\tStep 177\n",
      "\t\tStep 178\n",
      "\t\tStep 179\n",
      "\t\tStep 180\n",
      "\t\tStep 181\n",
      "\t\tStep 182\n",
      "\t\tStep 183\n",
      "\t\tStep 184\n",
      "\t\tStep 185\n",
      "\t\tStep 186\n",
      "\t\tStep 187\n",
      "\t\tStep 188\n",
      "\t\tStep 189\n",
      "\t\tStep 190\n",
      "\t\tStep 191\n",
      "\t\tStep 192\n",
      "Best accuracy: 0.954798331015299\n",
      "\tCross-validation fold 3\n",
      "\t\tStep 1: new best accuracy of 0.204\n",
      "\t\tStep 2: new best accuracy of 0.355\n",
      "\t\tStep 3: new best accuracy of 0.499\n",
      "\t\tStep 4\n",
      "\t\tStep 5\n",
      "\t\tStep 6: new best accuracy of 0.711\n",
      "\t\tStep 7\n",
      "\t\tStep 8: new best accuracy of 0.753\n",
      "\t\tStep 9\n",
      "\t\tStep 10\n",
      "\t\tStep 11\n",
      "\t\tStep 12: new best accuracy of 0.836\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16: new best accuracy of 0.853\n",
      "\t\tStep 17: new best accuracy of 0.860\n",
      "\t\tStep 18\n",
      "\t\tStep 19: new best accuracy of 0.887\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24: new best accuracy of 0.899\n",
      "\t\tStep 25: new best accuracy of 0.900\n",
      "\t\tStep 26\n",
      "\t\tStep 27: new best accuracy of 0.901\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32: new best accuracy of 0.902\n",
      "\t\tStep 33: new best accuracy of 0.924\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51: new best accuracy of 0.926\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59: new best accuracy of 0.935\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63: new best accuracy of 0.939\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68: new best accuracy of 0.941\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74: new best accuracy of 0.942\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86: new best accuracy of 0.942\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99: new best accuracy of 0.944\n",
      "\t\tStep 100\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104: new best accuracy of 0.946\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\t\tStep 118: new best accuracy of 0.947\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "\t\tStep 126\n",
      "\t\tStep 127\n",
      "\t\tStep 128\n",
      "\t\tStep 129\n",
      "\t\tStep 130\n",
      "\t\tStep 131\n",
      "\t\tStep 132\n",
      "\t\tStep 133: new best accuracy of 0.950\n",
      "\t\tStep 134\n",
      "\t\tStep 135\n",
      "\t\tStep 136\n",
      "\t\tStep 137\n",
      "\t\tStep 138\n",
      "\t\tStep 139\n",
      "\t\tStep 140\n",
      "\t\tStep 141\n",
      "\t\tStep 142\n",
      "\t\tStep 143\n",
      "\t\tStep 144\n",
      "\t\tStep 145\n",
      "\t\tStep 146\n",
      "\t\tStep 147\n",
      "\t\tStep 148\n",
      "\t\tStep 149\n",
      "\t\tStep 150\n",
      "\t\tStep 151\n",
      "\t\tStep 152: new best accuracy of 0.951\n",
      "\t\tStep 153\n",
      "\t\tStep 154\n",
      "\t\tStep 155\n",
      "\t\tStep 156\n",
      "\t\tStep 157\n",
      "\t\tStep 158\n",
      "\t\tStep 159\n",
      "\t\tStep 160\n",
      "\t\tStep 161\n",
      "\t\tStep 162\n",
      "\t\tStep 163\n",
      "\t\tStep 164\n",
      "\t\tStep 165\n",
      "\t\tStep 166\n",
      "\t\tStep 167\n",
      "\t\tStep 168: new best accuracy of 0.954\n",
      "\t\tStep 169\n",
      "\t\tStep 170\n",
      "\t\tStep 171\n",
      "\t\tStep 172\n",
      "\t\tStep 173\n",
      "\t\tStep 174\n",
      "\t\tStep 175\n",
      "\t\tStep 176\n",
      "\t\tStep 177\n",
      "\t\tStep 178\n",
      "\t\tStep 179\n",
      "\t\tStep 180\n",
      "\t\tStep 181\n",
      "\t\tStep 182\n",
      "\t\tStep 183\n",
      "\t\tStep 184\n",
      "\t\tStep 185\n",
      "\t\tStep 186\n",
      "\t\tStep 187\n",
      "\t\tStep 188\n",
      "\t\tStep 189\n",
      "\t\tStep 190\n",
      "\t\tStep 191\n",
      "\t\tStep 192\n",
      "\t\tStep 193\n",
      "Best accuracy: 0.9541029207232267\n",
      "\tCross-validation fold 4\n",
      "\t\tStep 1: new best accuracy of 0.099\n",
      "\t\tStep 2: new best accuracy of 0.186\n",
      "\t\tStep 3: new best accuracy of 0.372\n",
      "\t\tStep 4: new best accuracy of 0.386\n",
      "\t\tStep 5\n",
      "\t\tStep 6: new best accuracy of 0.663\n",
      "\t\tStep 7\n",
      "\t\tStep 8\n",
      "\t\tStep 9: new best accuracy of 0.734\n",
      "\t\tStep 10\n",
      "\t\tStep 11: new best accuracy of 0.780\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14: new best accuracy of 0.798\n",
      "\t\tStep 15: new best accuracy of 0.873\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23: new best accuracy of 0.875\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27: new best accuracy of 0.882\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31: new best accuracy of 0.897\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36: new best accuracy of 0.903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40: new best accuracy of 0.905\n",
      "\t\tStep 41: new best accuracy of 0.921\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58: new best accuracy of 0.924\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63: new best accuracy of 0.929\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72: new best accuracy of 0.935\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77: new best accuracy of 0.937\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\t\tStep 101: new best accuracy of 0.939\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109: new best accuracy of 0.942\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\t\tStep 118\n",
      "\t\tStep 119: new best accuracy of 0.947\n",
      "\t\tStep 120\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "\t\tStep 126\n",
      "\t\tStep 127\n",
      "\t\tStep 128\n",
      "\t\tStep 129\n",
      "\t\tStep 130\n",
      "\t\tStep 131\n",
      "\t\tStep 132\n",
      "\t\tStep 133\n",
      "\t\tStep 134\n",
      "\t\tStep 135\n",
      "\t\tStep 136\n",
      "\t\tStep 137\n",
      "\t\tStep 138\n",
      "\t\tStep 139\n",
      "\t\tStep 140\n",
      "\t\tStep 141\n",
      "\t\tStep 142\n",
      "\t\tStep 143\n",
      "\t\tStep 144\n",
      "Best accuracy: 0.9471488178025035\n",
      "\tCross-validation fold 5\n",
      "\t\tStep 1: new best accuracy of 0.251\n",
      "\t\tStep 2: new best accuracy of 0.273\n",
      "\t\tStep 3: new best accuracy of 0.276\n",
      "\t\tStep 4: new best accuracy of 0.451\n",
      "\t\tStep 5: new best accuracy of 0.519\n",
      "\t\tStep 6: new best accuracy of 0.565\n",
      "\t\tStep 7: new best accuracy of 0.636\n",
      "\t\tStep 8\n",
      "\t\tStep 9: new best accuracy of 0.697\n",
      "\t\tStep 10: new best accuracy of 0.779\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15: new best accuracy of 0.846\n",
      "\t\tStep 16: new best accuracy of 0.902\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22: new best accuracy of 0.917\n",
      "\t\tStep 23\n",
      "\t\tStep 24: new best accuracy of 0.924\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33: new best accuracy of 0.928\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39: new best accuracy of 0.933\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52: new best accuracy of 0.937\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59: new best accuracy of 0.938\n",
      "\t\tStep 60: new best accuracy of 0.941\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71: new best accuracy of 0.950\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88: new best accuracy of 0.951\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99\n",
      "\t\tStep 100: new best accuracy of 0.954\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111: new best accuracy of 0.957\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\t\tStep 118\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "\t\tStep 126\n",
      "\t\tStep 127\n",
      "\t\tStep 128\n",
      "\t\tStep 129\n",
      "\t\tStep 130\n",
      "\t\tStep 131\n",
      "\t\tStep 132\n",
      "\t\tStep 133\n",
      "\t\tStep 134\n",
      "\t\tStep 135\n",
      "\t\tStep 136\n",
      "Best accuracy: 0.9568845618915159\n",
      "Wine gradient descent:\n",
      "\tCross-validation fold 1\n",
      "\t\tStep 1: new best accuracy of 0.497\n",
      "\t\tStep 2\n",
      "\t\tStep 3: new best accuracy of 0.650\n",
      "\t\tStep 4\n",
      "\t\tStep 5\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8: new best accuracy of 0.741\n",
      "\t\tStep 9: new best accuracy of 0.804\n",
      "\t\tStep 10: new best accuracy of 0.839\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18: new best accuracy of 0.916\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39: new best accuracy of 0.937\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46: new best accuracy of 0.944\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57: new best accuracy of 0.958\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65: new best accuracy of 0.965\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77: new best accuracy of 0.979\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "Best accuracy: 0.9790209790209791\n",
      "\tCross-validation fold 2\n",
      "\t\tStep 1: new best accuracy of 0.420\n",
      "\t\tStep 2: new best accuracy of 0.545\n",
      "\t\tStep 3: new best accuracy of 0.657\n",
      "\t\tStep 4: new best accuracy of 0.713\n",
      "\t\tStep 5\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8: new best accuracy of 0.867\n",
      "\t\tStep 9\n",
      "\t\tStep 10\n",
      "\t\tStep 11: new best accuracy of 0.902\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26: new best accuracy of 0.923\n",
      "\t\tStep 27: new best accuracy of 0.937\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37: new best accuracy of 0.944\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40: new best accuracy of 0.951\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "Best accuracy: 0.951048951048951\n",
      "\tCross-validation fold 3\n",
      "\t\tStep 1: new best accuracy of 0.413\n",
      "\t\tStep 2\n",
      "\t\tStep 3\n",
      "\t\tStep 4\n",
      "\t\tStep 5\n",
      "\t\tStep 6: new best accuracy of 0.580\n",
      "\t\tStep 7\n",
      "\t\tStep 8\n",
      "\t\tStep 9\n",
      "\t\tStep 10: new best accuracy of 0.741\n",
      "\t\tStep 11: new best accuracy of 0.902\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18: new best accuracy of 0.937\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41: new best accuracy of 0.951\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "Best accuracy: 0.951048951048951\n",
      "\tCross-validation fold 4\n",
      "\t\tStep 1: new best accuracy of 0.322\n",
      "\t\tStep 2: new best accuracy of 0.413\n",
      "\t\tStep 3\n",
      "\t\tStep 4: new best accuracy of 0.420\n",
      "\t\tStep 5: new best accuracy of 0.636\n",
      "\t\tStep 6\n",
      "\t\tStep 7: new best accuracy of 0.643\n",
      "\t\tStep 8\n",
      "\t\tStep 9\n",
      "\t\tStep 10: new best accuracy of 0.832\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13: new best accuracy of 0.853\n",
      "\t\tStep 14\n",
      "\t\tStep 15: new best accuracy of 0.895\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22: new best accuracy of 0.909\n",
      "\t\tStep 23\n",
      "\t\tStep 24: new best accuracy of 0.951\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46: new best accuracy of 0.958\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "Best accuracy: 0.958041958041958\n",
      "\tCross-validation fold 5\n",
      "\t\tStep 1: new best accuracy of 0.497\n",
      "\t\tStep 2: new best accuracy of 0.643\n",
      "\t\tStep 3\n",
      "\t\tStep 4\n",
      "\t\tStep 5\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8\n",
      "\t\tStep 9\n",
      "\t\tStep 10\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14: new best accuracy of 0.839\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26: new best accuracy of 0.860\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tStep 39\n",
      "\t\tStep 40: new best accuracy of 0.867\n",
      "\t\tStep 41: new best accuracy of 0.874\n",
      "\t\tStep 42: new best accuracy of 0.881\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48: new best accuracy of 0.930\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62: new best accuracy of 0.965\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "Best accuracy: 0.965034965034965\n",
      "\n",
      "\n",
      "Digits training accuracy: 95.2%\n",
      "Digits training cost: 1262.505\n",
      "Digits validation accuracy: 90.2%\n",
      "Digits validation cost: 1329.227\n",
      "Wine training accuracy: 96.5%\n",
      "Wine training cost: 141.312\n",
      "Wine validation accuracy: 88.2%\n",
      "Wine validation cost: 150.209\n"
     ]
    }
   ],
   "source": [
    "def accurate(a, b):\n",
    "    return np.argmax(a) == np.argmax(b)\n",
    "\n",
    "def cost(yh, y):\n",
    "    return y * np.log1p(np.exp(-yh)) + (1-yh) * np.log1p(np.exp(yh))\n",
    "\n",
    "batch_size = 30\n",
    "learning_rate = 0.04\n",
    "momentum = 0.2\n",
    "\n",
    "print(\"Model hyper-parameters:\")\n",
    "print(\"\\tMini-batch size:\", batch_size)\n",
    "print(\"\\tLearning rate:\", learning_rate)\n",
    "print(\"\\tMomentum:\", momentum)\n",
    "print(\"\\n\")\n",
    "\n",
    "digits_training_accuracy = 0\n",
    "digits_training_cost = 0\n",
    "digits_validation_accuracy = 0\n",
    "digits_validation_cost = 0\n",
    "\n",
    "print(\"Digits gradient descent:\")\n",
    "\n",
    "for fold_index, fold in enumerate(xDigitsTrainingSets):\n",
    "    print(f\"\\tCross-validation fold {fold_index+1}\")\n",
    "    \n",
    "    gradientDescentModel = GradientDescent(batch_size, learning_rate, momentum)\n",
    "    logisticRegressionModel = LogisticRegression(add_bias=True)\n",
    "    \n",
    "    logisticRegressionModel.fit(fold, yDigitsTrainingSets[fold_index], gradientDescentModel)\n",
    "    yh_training = logisticRegressionModel.predict(xDigitsTrainingSets[fold_index])\n",
    "    yh_validation = logisticRegressionModel.predict(xDigitsValidationSets[fold_index])\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh_training):\n",
    "        if accurate(yh_x, yDigitsTrainingSets[fold_index][sample_index]):\n",
    "            digits_training_accuracy += 1\n",
    "        c = np.argmax(yDigitsTrainingSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yDigitsTrainingSets[fold_index][sample_index][c])\n",
    "        digits_training_cost += cst\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh_validation):\n",
    "        if accurate(yh_x, yDigitsValidationSets[fold_index][sample_index]):\n",
    "            digits_validation_accuracy += 1\n",
    "        c = np.argmax(yDigitsValidationSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yDigitsValidationSets[fold_index][sample_index][c])\n",
    "        digits_validation_cost += cst\n",
    "        \n",
    "digits_training_accuracy /= 4*len(digits.data)\n",
    "digits_training_cost /= 4\n",
    "digits_validation_accuracy /= len(digits.data)\n",
    "\n",
    "wine_training_accuracy = 0\n",
    "wine_training_cost = 0\n",
    "wine_validation_accuracy = 0\n",
    "wine_validation_cost = 0\n",
    "\n",
    "print(\"Wine gradient descent:\")\n",
    "\n",
    "for fold_index, fold in enumerate(xWineTrainingSets):\n",
    "    print(f\"\\tCross-validation fold {fold_index+1}\")\n",
    "    \n",
    "    gradientDescentModel = GradientDescent(batch_size, learning_rate, momentum)\n",
    "    logisticRegressionModel = LogisticRegression(add_bias=True)\n",
    "    \n",
    "    logisticRegressionModel.fit(fold, yWineTrainingSets[fold_index], gradientDescentModel)\n",
    "    yh_training = logisticRegressionModel.predict(xWineTrainingSets[fold_index])\n",
    "    yh_validation = logisticRegressionModel.predict(xWineValidationSets[fold_index])\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh_training):\n",
    "        if accurate(yh_x, yWineTrainingSets[fold_index][sample_index]):\n",
    "            wine_training_accuracy += 1\n",
    "        c = np.argmax(yWineTrainingSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yWineTrainingSets[fold_index][sample_index][c])\n",
    "        wine_training_cost += cst\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh_validation):\n",
    "        if accurate(yh_x, yWineValidationSets[fold_index][sample_index]):\n",
    "            wine_validation_accuracy += 1\n",
    "        c = np.argmax(yWineValidationSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yWineValidationSets[fold_index][sample_index][c])\n",
    "        wine_validation_cost += cst\n",
    "\n",
    "wine_training_accuracy /= 4*len(wine.data)\n",
    "wine_training_cost /= 4\n",
    "wine_validation_accuracy /= len(wine.data)\n",
    "    \n",
    "print(\"\\n\")\n",
    "print(f\"Digits training accuracy: {digits_training_accuracy*100:.1f}%\")\n",
    "print(f\"Digits training cost: {digits_training_cost:.3f}\")\n",
    "print(f\"Digits validation accuracy: {digits_validation_accuracy*100:.1f}%\")\n",
    "print(f\"Digits validation cost: {digits_validation_cost:.3f}\")\n",
    "print(f\"Wine training accuracy: {wine_training_accuracy*100:.1f}%\")\n",
    "print(f\"Wine training cost: {wine_training_cost:.3f}\")\n",
    "print(f\"Wine validation accuracy: {wine_validation_accuracy*100:.1f}%\")\n",
    "print(f\"Wine validation cost: {wine_validation_cost:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean = lambda x1, x2: np.sqrt(np.sum((x1 - x2)**2, axis=-1))\n",
    "manhattan = lambda x1, x2: np.sum(np.abs(x1 - x2), axis=-1)\n",
    "\n",
    "class KNN:\n",
    "\n",
    "    def __init__(self, K=1, dist_fn= euclidean):\n",
    "        self.dist_fn = dist_fn\n",
    "        self.K = K\n",
    "        return\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.C = len(y[0])\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        num_test = x_test.shape[0]\n",
    "        distances = self.dist_fn(self.x[None,:,:], x_test[:,None,:])\n",
    "        knns = np.zeros((num_test, self.K), dtype=int)\n",
    "        y_prob = np.zeros((num_test),dtype=int)\n",
    "        counts = np.zeros((num_test, self.C))\n",
    "        \n",
    "        for i in range(num_test):\n",
    "            knns[i,:] = np.argsort(distances[i])[:self.K]\n",
    "            k_count=np.zeros(self.K, dtype=int)\n",
    "            \n",
    "            for s, arr in enumerate(self.y[knns[i,:]]):\n",
    "                k_count[s] = np.argmax(arr)\n",
    "            \n",
    "            y_prob_i, counts_i = np.unique(k_count, return_counts=True)\n",
    "            y_prob[i] = int(y_prob_i[np.argmax(counts_i)])\n",
    "        \n",
    "        return y_prob, knns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN digits validation accuracy: 95.7%\n",
      "KNN wine validation accuracy: 85.4%\n"
     ]
    }
   ],
   "source": [
    "KNNmodel = KNN(K=11)\n",
    "\n",
    "digits_knn_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    y_prob, knns = KNNmodel.fit(np.asarray(xDigitsTrainingSets[fold]), np.asarray(yDigitsTrainingSets[fold])).predict(np.asarray(xDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, prob in enumerate(y_prob):\n",
    "        if prob == np.argmax(yDigitsValidationSets[fold][i]):\n",
    "            digits_knn_accuracy += 1\n",
    "\n",
    "digits_knn_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"KNN digits validation accuracy: {digits_knn_accuracy*100:.1f}%\")\n",
    "\n",
    "KNNmodel = KNN(K=7)\n",
    "\n",
    "wine_knn_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    y_prob, knns = KNNmodel.fit(np.asarray(xWineTrainingSets[fold]), np.asarray(yWineTrainingSets[fold])).predict(np.asarray(xWineValidationSets[fold]))\n",
    "    \n",
    "    for i, prob in enumerate(y_prob):\n",
    "        if prob == np.argmax(yWineValidationSets[fold][i]):\n",
    "            wine_knn_accuracy += 1\n",
    "            \n",
    "wine_knn_accuracy /= len(wine.data)\n",
    "\n",
    "print(f\"KNN wine validation accuracy: {wine_knn_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HoGFeatures(img, cellSize, blockSize, nbins):\n",
    "    cell_size = (cellSize, cellSize)\n",
    "    block_size = (blockSize, blockSize)\n",
    "    \n",
    "    hog = cv.HOGDescriptor(_winSize=(img.shape[1] // cell_size[1] * cell_size[1],\n",
    "                                     img.shape[0] // cell_size[0] * cell_size[0]),\n",
    "                           _blockSize=(block_size[1] * cell_size[1],\n",
    "                                       block_size[0] * cell_size[0]),\n",
    "                           _blockStride=(cell_size[1], cell_size[0]),\n",
    "                           _cellSize=(cell_size[1], cell_size[0]),\n",
    "                           _nbins=nbins\n",
    "    )\n",
    "    \n",
    "    return hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeHoGFeatures(imageArray):\n",
    "    HoG = HoGFeatures(imageArray[0], 2, 2, 2)\n",
    "    features = []\n",
    "    \n",
    "    for i, image in enumerate(imageArray):\n",
    "        features.append(HoG.compute((image*255).astype(np.uint8)))\n",
    "        \n",
    "    features = np.array(np.squeeze(features))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC digits validation accuracy: 89.6%\n"
     ]
    }
   ],
   "source": [
    "digits_svc_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    numbers_training = []\n",
    "    numbers_validation = []\n",
    "    \n",
    "    for i, number in enumerate(xDigitsTrainingSets[fold]):\n",
    "        numbers_training.append(np.asarray(number).reshape(8, 8))\n",
    "        \n",
    "    for i, number in enumerate(xDigitsValidationSets[fold]):\n",
    "        numbers_validation.append(np.asarray(number).reshape(8, 8))  \n",
    "        \n",
    "    HoGs_training = makeHoGFeatures(np.asarray(numbers_training))\n",
    "    HoGs_validation = makeHoGFeatures(np.asarray(numbers_validation))\n",
    "\n",
    "    clf = svm.SVC(gamma='auto', C=100) \n",
    "    \n",
    "    labels_training = np.zeros(len(yDigitsTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    clf.fit(HoGs_training, labels_training)\n",
    "\n",
    "    labels_predicted = clf.predict(HoGs_validation)\n",
    "    \n",
    "    for i, label in enumerate(labels_predicted):\n",
    "        if label == labels_validation[i]:\n",
    "            digits_svc_accuracy += 1\n",
    "\n",
    "digits_svc_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"SVC digits validation accuracy: {digits_svc_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive base digits validation accuracy: 81.1%\n",
      "\n",
      "Naive base wine validation accuracy: 93.8%\n"
     ]
    }
   ],
   "source": [
    "digits_naive_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    labels_training = np.zeros(len(yDigitsTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(np.asarray(xDigitsTrainingSets[fold]), labels_training).predict(np.asarray(xDigitsValidationSets[fold]))\n",
    "\n",
    "    for i, label in enumerate(y_pred):\n",
    "        if label == labels_validation[i]:\n",
    "            digits_naive_accuracy += 1\n",
    "\n",
    "digits_naive_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"Naive base digits validation accuracy: {digits_naive_accuracy*100:.1f}%\\n\")\n",
    "\n",
    "wine_naive_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    labels_training = np.zeros(len(yWineTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yWineTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yWineValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yWineValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(np.asarray(xWineTrainingSets[fold]), labels_training).predict(np.asarray(xWineValidationSets[fold]))\n",
    "\n",
    "    for i, label in enumerate(y_pred):\n",
    "        if label == labels_validation[i]:\n",
    "            wine_naive_accuracy += 1\n",
    "\n",
    "wine_naive_accuracy /= len(wine.data)\n",
    "\n",
    "print(f\"Naive base wine validation accuracy: {wine_naive_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
