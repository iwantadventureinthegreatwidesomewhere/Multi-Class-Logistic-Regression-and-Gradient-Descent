{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lia\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "from sklearn import svm \n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_search will perform the gridsearch in order to run the tests to plot graphs\n",
    "#if run_search = False, there will be no plots\n",
    "#ONLY TURN RUN_SEARCH TO TRUE IF YOU HAVE 5 HOURS TO SPARE\n",
    "run_search = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "wine = fetch_openml(name='wine', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of digits and wine data\n",
    "\n",
    "digits_data_norm = []\n",
    "\n",
    "for col in digits.data:\n",
    "    col_norm = col/np.max(col)\n",
    "    digits_data_norm.append(col_norm)\n",
    "\n",
    "digits.data = np.asarray(digits_data_norm)\n",
    "\n",
    "wine_data_norm = []\n",
    "\n",
    "for col in wine.data.T:\n",
    "    col_norm = col/np.amax(col)\n",
    "    wine_data_norm.append(col_norm)\n",
    "    \n",
    "wine.data = np.asarray(wine_data_norm).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation for digits dataset\n",
    "\n",
    "digitsTrainingSetSize = int(np.ceil(0.8 * len(digits.data)))\n",
    "digitsValidationSetSize = int(len(digits.data) - digitsTrainingSetSize)\n",
    "\n",
    "xDigitsTrainingSets = []\n",
    "yDigitsTrainingSets = []\n",
    "xDigitsValidationSets = []\n",
    "yDigitsValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data[foldIndex*digitsValidationSetSize:((foldIndex*digitsValidationSetSize)+digitsValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(digits.target[index+(foldIndex*digitsValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(digits.target[index])\n",
    "            \n",
    "    xDigitsTrainingSets.append(xTrainingSet)\n",
    "    yDigitsTrainingSets.append(yTrainingSet)\n",
    "    xDigitsValidationSets.append(xValidationSet)\n",
    "    yDigitsValidationSets.append(yValidationSet)\n",
    "    \n",
    "# 5-fold cross validation for wine dataset\n",
    "\n",
    "wineTrainingSetSize = int(np.ceil(0.8 * len(wine.data)))\n",
    "wineValidationSetSize = int(len(wine.data) - wineTrainingSetSize)\n",
    "\n",
    "xWineTrainingSets = []\n",
    "yWineTrainingSets = []\n",
    "xWineValidationSets = []\n",
    "yWineValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "    for index, data in enumerate(wine.data[foldIndex*wineValidationSetSize:((foldIndex*wineValidationSetSize)+wineValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(wine.target[index+(foldIndex*wineValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "    \n",
    "    for index, data in enumerate(wine.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(wine.target[index])\n",
    "            \n",
    "    xWineTrainingSets.append(xTrainingSet)\n",
    "    yWineTrainingSets.append(yTrainingSet)\n",
    "    xWineValidationSets.append(xValidationSet)\n",
    "    yWineValidationSets.append(yValidationSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of y for digits dataset\n",
    "\n",
    "numberOfDigitsTargets = 10\n",
    "numberOfWineTargets = 3\n",
    "\n",
    "for index, fold in enumerate(yDigitsTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yDigitsValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsValidationSets[index] = encodedFold\n",
    "\n",
    "# one-hot encoding of y for wine dataset\n",
    "\n",
    "for index, fold in enumerate(yWineTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yWineValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineValidationSets[index] = encodedFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomIndices(arr, batch_size):\n",
    "    indices = []\n",
    "    \n",
    "    if batch_size > len(arr):\n",
    "        print(\"Error: batch size larger than size of dataset.\")\n",
    "        return\n",
    "    \n",
    "    while batch_size > 0:\n",
    "        x = np.floor(np.random.random() * len(arr))\n",
    "        if x not in indices:\n",
    "            indices.append(int(x))\n",
    "            batch_size -= 1\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent class\n",
    " \n",
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, batch_size, learning_rate=0.5, momentum=0.9, max_termination_condition=25, max_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        self.max_termination_condition = max_termination_condition\n",
    "        self.max_iters = max_iters\n",
    "        self.deltas = []\n",
    "        \n",
    "    def run(self, gradient_fn, x, y, w, vx, vy):\n",
    "        t = 1\n",
    "        \n",
    "        max_cost = np.inf\n",
    "        termination_count = 0        \n",
    "        weight_history = []\n",
    "        error_history = []\n",
    "                \n",
    "        for number_of_targets in range(len(y[0])):\n",
    "            weight_history.append([])\n",
    "        \n",
    "        while termination_count < self.max_termination_condition and t < self.max_iters:\n",
    "            gradients = gradient_fn(x, y, w, self.batch_size)   \n",
    "            \n",
    "            for c in range(len(y[0])):\n",
    "                if(t==1):\n",
    "                    w[c] = w[c] - self.learning_rate * gradients[c]\n",
    "                else:\n",
    "                    delta_w = (self.momentum)*(self.deltas[-(len(y[0]))]) + (1-self.momentum)*gradients[c]\n",
    "                    w[c] = w[c] - (self.learning_rate)*(delta_w)\n",
    "                self.deltas.append(w[c])\n",
    "            \n",
    "            a = np.asarray(vx)\n",
    "            b = np.asarray(w)\n",
    "    \n",
    "            vyh=[]\n",
    "        \n",
    "            for i, vx_c in enumerate(a):\n",
    "                vyh_x=[]\n",
    "\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  b[c] @ vx_c\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    \n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  b[i] @ vx_c\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    vyh_c = num/den\n",
    "                    vyh_x.append(vyh_c)\n",
    "                    \n",
    "                vyh.append(vyh_x)\n",
    "                \n",
    "            step_cost = 0\n",
    "                \n",
    "            def cost(yh, y):\n",
    "                return y * np.log1p(np.exp(-yh)) + (1-yh) * np.log1p(np.exp(yh))\n",
    "                \n",
    "            for sample_index, vyh_x in enumerate(vyh):\n",
    "                c = np.argmax(vy[sample_index])\n",
    "                cst = cost(vyh_x[c], vy[sample_index][c])\n",
    "                step_cost += cst\n",
    "            \n",
    "            for c in range(len(b)):\n",
    "                weight_history[c].append(w[c])\n",
    "            \n",
    "            error_history.append(step_cost)\n",
    "\n",
    "            if step_cost < max_cost:\n",
    "                max_cost = step_cost\n",
    "                termination_count = 0\n",
    "                print(f\"\\t\\tStep {t}: new best cost of {step_cost:.3f}\")\n",
    "            else:\n",
    "                termination_count += 1\n",
    "                print(f\"\\t\\tStep {t}\")\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        index_best = len(error_history)-self.max_termination_condition-1\n",
    "        \n",
    "        w_best = []\n",
    "        \n",
    "        for c in range(len(y[0])):\n",
    "            w_best.append(weight_history[c][index_best])\n",
    "        \n",
    "        return w_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, add_bias=True):\n",
    "        self.add_bias = add_bias\n",
    "        pass\n",
    "            \n",
    "    def fit(self, x, y, optimizer, vx, vy):\n",
    "        def gradient(x, y, w, batch_size):\n",
    "            gradients = np.zeros(len(w)).tolist()\n",
    "\n",
    "            indices = getRandomIndices(x, batch_size)\n",
    "\n",
    "            for index in indices:\n",
    "                a = np.asarray(x[index])\n",
    "                b = np.asarray(y[index])\n",
    "\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  w[c] @ a\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  w[i] @ a\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "\n",
    "                    y_c = b[c]\n",
    "                    \n",
    "                    cost_c = np.dot(yh_c - y_c, a)\n",
    "                    \n",
    "                    gradients[c] += cost_c\n",
    "\n",
    "            return gradients\n",
    "        \n",
    "        if self.add_bias:\n",
    "            x = np.asarray(x)\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "            \n",
    "            vx = np.asarray(vx)\n",
    "            vN = vx.shape[0]\n",
    "            vx = np.column_stack([vx,np.ones(vN)])\n",
    "\n",
    "        w0 = []\n",
    "        for c in range(len(y[0])):\n",
    "            w0.append(np.zeros(len(x[0])))\n",
    "            \n",
    "        self.w = optimizer.run(gradient, x, y, w0, vx, vy)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if self.add_bias:\n",
    "            x = np.asarray(x)\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "\n",
    "        a = np.asarray(x)\n",
    "        b = np.asarray(self.w)\n",
    "\n",
    "        yh=[]\n",
    "        \n",
    "        for i, x_c in enumerate(a):\n",
    "            yh_x=[]\n",
    "            \n",
    "            for c in range(len(b)):\n",
    "                w_x =  b[c] @ x_c\n",
    "                num = np.exp(w_x)\n",
    "\n",
    "                den = 0\n",
    "                \n",
    "                for i in range(len(b)):\n",
    "                    w_x =  b[i] @ x_c\n",
    "                    den += np.exp(w_x)\n",
    "\n",
    "                yh_c = num/den\n",
    "                yh_x.append(yh_c)\n",
    "                \n",
    "            yh.append(yh_x)\n",
    "        \n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def runLogisticRegression(batch_size, learning_rate, momentum):\n",
    "    def accurate(a, b):\n",
    "        return np.argmax(a) == np.argmax(b)\n",
    "\n",
    "    def cost(yh, y):\n",
    "        return y * np.log1p(np.exp(-yh)) + (1-yh) * np.log1p(np.exp(yh))\n",
    "\n",
    "    print(\"Model hyper-parameters:\")\n",
    "    print(\"\\tMini-batch size:\", batch_size)\n",
    "    print(\"\\tLearning rate:\", learning_rate)\n",
    "    print(\"\\tMomentum:\", momentum)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    digits_training_accuracy = 0\n",
    "    digits_training_cost = 0\n",
    "    digits_validation_accuracy = 0\n",
    "    digits_validation_cost = 0\n",
    "\n",
    "    print(\"Digits gradient descent:\")\n",
    "\n",
    "    for fold_index, fold in enumerate(xDigitsTrainingSets):\n",
    "        print(f\"\\tCross-validation fold {fold_index+1}\")\n",
    "\n",
    "        gradientDescentModel = GradientDescent(batch_size, learning_rate, momentum)\n",
    "        logisticRegressionModel = LogisticRegression(add_bias=True)\n",
    "\n",
    "        logisticRegressionModel.fit(fold, yDigitsTrainingSets[fold_index], gradientDescentModel, xDigitsValidationSets[fold_index], yDigitsValidationSets[fold_index])\n",
    "        yh_training = logisticRegressionModel.predict(xDigitsTrainingSets[fold_index])\n",
    "        yh_validation = logisticRegressionModel.predict(xDigitsValidationSets[fold_index])\n",
    "\n",
    "        for sample_index, yh_x in enumerate(yh_training):\n",
    "            if accurate(yh_x, yDigitsTrainingSets[fold_index][sample_index]):\n",
    "                digits_training_accuracy += 1\n",
    "            c = np.argmax(yDigitsTrainingSets[fold_index][sample_index])\n",
    "            cst = cost(yh_x[c], yDigitsTrainingSets[fold_index][sample_index][c])\n",
    "            digits_training_cost += cst\n",
    "\n",
    "        for sample_index, yh_x in enumerate(yh_validation):\n",
    "            if accurate(yh_x, yDigitsValidationSets[fold_index][sample_index]):\n",
    "                digits_validation_accuracy += 1\n",
    "            c = np.argmax(yDigitsValidationSets[fold_index][sample_index])\n",
    "            cst = cost(yh_x[c], yDigitsValidationSets[fold_index][sample_index][c])\n",
    "            digits_validation_cost += cst\n",
    "\n",
    "    digits_training_accuracy /= 4*len(digits.data)\n",
    "    digits_training_cost /= 4\n",
    "    digits_validation_accuracy /= len(digits.data)\n",
    "\n",
    "    wine_training_accuracy = 0\n",
    "    wine_training_cost = 0\n",
    "    wine_validation_accuracy = 0\n",
    "    wine_validation_cost = 0\n",
    "\n",
    "    print(\"Wine gradient descent:\")\n",
    "\n",
    "    for fold_index, fold in enumerate(xWineTrainingSets):\n",
    "        print(f\"\\tCross-validation fold {fold_index+1}\")\n",
    "\n",
    "        gradientDescentModel = GradientDescent(batch_size, learning_rate, momentum)\n",
    "        logisticRegressionModel = LogisticRegression(add_bias=True)\n",
    "\n",
    "        logisticRegressionModel.fit(fold, yWineTrainingSets[fold_index], gradientDescentModel, xWineValidationSets[fold_index], yWineValidationSets[fold_index])\n",
    "        yh_training = logisticRegressionModel.predict(xWineTrainingSets[fold_index])\n",
    "        yh_validation = logisticRegressionModel.predict(xWineValidationSets[fold_index])\n",
    "\n",
    "        for sample_index, yh_x in enumerate(yh_training):\n",
    "            if accurate(yh_x, yWineTrainingSets[fold_index][sample_index]):\n",
    "                wine_training_accuracy += 1\n",
    "            c = np.argmax(yWineTrainingSets[fold_index][sample_index])\n",
    "            cst = cost(yh_x[c], yWineTrainingSets[fold_index][sample_index][c])\n",
    "            wine_training_cost += cst\n",
    "\n",
    "        for sample_index, yh_x in enumerate(yh_validation):\n",
    "            if accurate(yh_x, yWineValidationSets[fold_index][sample_index]):\n",
    "                wine_validation_accuracy += 1\n",
    "            c = np.argmax(yWineValidationSets[fold_index][sample_index])\n",
    "            cst = cost(yh_x[c], yWineValidationSets[fold_index][sample_index][c])\n",
    "            wine_validation_cost += cst\n",
    "\n",
    "    wine_training_accuracy /= 4*len(wine.data)\n",
    "    wine_training_cost /= 4\n",
    "    wine_validation_accuracy /= len(wine.data)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(f\"Digits training accuracy: {digits_training_accuracy*100:.1f}%\")\n",
    "    print(f\"Digits training cost: {digits_training_cost:.3f}\")\n",
    "    print(f\"Digits validation accuracy: {digits_validation_accuracy*100:.1f}%\")\n",
    "    print(f\"Digits validation cost: {digits_validation_cost:.3f}\")\n",
    "    print(f\"Wine training accuracy: {wine_training_accuracy*100:.1f}%\")\n",
    "    print(f\"Wine training cost: {wine_training_cost:.3f}\")\n",
    "    print(f\"Wine validation accuracy: {wine_validation_accuracy*100:.1f}%\")\n",
    "    print(f\"Wine validation cost: {wine_validation_cost:.3f}\\n\")\n",
    "    \n",
    "    return (digits_validation_accuracy, digits_validation_cost, wine_validation_accuracy, wine_validation_cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hyper-parameters:\n",
      "\tMini-batch size: 30\n",
      "\tLearning rate: 0.04\n",
      "\tMomentum: 0.2\n",
      "\n",
      "\n",
      "Digits gradient descent:\n",
      "\tCross-validation fold 1\n",
      "\t\tStep 1: new best cost of 463.555\n",
      "\t\tStep 2: new best cost of 458.599\n",
      "\t\tStep 3: new best cost of 450.982\n",
      "\t\tStep 4: new best cost of 443.064\n",
      "\t\tStep 5: new best cost of 434.781\n",
      "\t\tStep 6: new best cost of 426.115\n",
      "\t\tStep 7: new best cost of 420.631\n",
      "\t\tStep 8: new best cost of 414.877\n",
      "\t\tStep 9: new best cost of 410.495\n",
      "\t\tStep 10: new best cost of 398.468\n",
      "\t\tStep 11: new best cost of 392.025\n",
      "\t\tStep 12: new best cost of 390.199\n",
      "\t\tStep 13: new best cost of 378.232\n",
      "\t\tStep 14: new best cost of 366.557\n",
      "\t\tStep 15: new best cost of 365.457\n",
      "\t\tStep 16\n",
      "\t\tStep 17: new best cost of 363.380\n",
      "\t\tStep 18: new best cost of 355.650\n",
      "\t\tStep 19: new best cost of 347.422\n",
      "\t\tStep 20: new best cost of 346.393\n",
      "\t\tStep 21: new best cost of 341.839\n",
      "\t\tStep 22: new best cost of 336.713\n",
      "\t\tStep 23\n",
      "\t\tStep 24: new best cost of 331.677\n",
      "\t\tStep 25\n",
      "\t\tStep 26: new best cost of 330.858\n",
      "\t\tStep 27\n",
      "\t\tStep 28: new best cost of 330.151\n",
      "\t\tStep 29\n",
      "\t\tStep 30: new best cost of 321.136\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33: new best cost of 310.973\n",
      "\t\tStep 34: new best cost of 309.862\n",
      "\t\tStep 35: new best cost of 302.528\n",
      "\t\tStep 36\n",
      "\t\tStep 37: new best cost of 299.315\n",
      "\t\tStep 38\n",
      "\t\tStep 39: new best cost of 296.069\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45: new best cost of 290.391\n",
      "\t\tStep 46\n",
      "\t\tStep 47: new best cost of 288.338\n",
      "\t\tStep 48: new best cost of 287.712\n",
      "\t\tStep 49: new best cost of 286.310\n",
      "\t\tStep 50: new best cost of 281.069\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54: new best cost of 280.542\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57: new best cost of 275.204\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60: new best cost of 273.899\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66: new best cost of 269.550\n",
      "\t\tStep 67\n",
      "\t\tStep 68: new best cost of 268.134\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72: new best cost of 267.732\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78: new best cost of 264.775\n",
      "\t\tStep 79: new best cost of 264.763\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87: new best cost of 260.917\n",
      "\t\tStep 88: new best cost of 260.835\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92: new best cost of 259.716\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115: new best cost of 259.006\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\t\tStep 118\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123: new best cost of 258.032\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "\t\tStep 126\n",
      "\t\tStep 127\n",
      "\t\tStep 128\n",
      "\t\tStep 129\n",
      "\t\tStep 130\n",
      "\t\tStep 131\n",
      "\t\tStep 132\n",
      "\t\tStep 133\n",
      "\t\tStep 134\n",
      "\t\tStep 135\n",
      "\t\tStep 136\n",
      "\t\tStep 137\n",
      "\t\tStep 138: new best cost of 257.865\n",
      "\t\tStep 139\n",
      "\t\tStep 140\n",
      "\t\tStep 141\n",
      "\t\tStep 142: new best cost of 255.926\n",
      "\t\tStep 143\n",
      "\t\tStep 144\n",
      "\t\tStep 145\n",
      "\t\tStep 146\n",
      "\t\tStep 147\n",
      "\t\tStep 148\n",
      "\t\tStep 149: new best cost of 255.216\n",
      "\t\tStep 150: new best cost of 253.709\n",
      "\t\tStep 151\n",
      "\t\tStep 152\n",
      "\t\tStep 153\n",
      "\t\tStep 154\n",
      "\t\tStep 155: new best cost of 253.472\n",
      "\t\tStep 156\n",
      "\t\tStep 157\n",
      "\t\tStep 158\n",
      "\t\tStep 159\n",
      "\t\tStep 160\n",
      "\t\tStep 161\n",
      "\t\tStep 162\n",
      "\t\tStep 163\n",
      "\t\tStep 164\n",
      "\t\tStep 165\n",
      "\t\tStep 166\n",
      "\t\tStep 167: new best cost of 252.929\n",
      "\t\tStep 168\n",
      "\t\tStep 169\n",
      "\t\tStep 170\n",
      "\t\tStep 171\n",
      "\t\tStep 172\n",
      "\t\tStep 173\n",
      "\t\tStep 174\n",
      "\t\tStep 175\n",
      "\t\tStep 176\n",
      "\t\tStep 177\n",
      "\t\tStep 178\n",
      "\t\tStep 179\n",
      "\t\tStep 180\n",
      "\t\tStep 181\n",
      "\t\tStep 182\n",
      "\t\tStep 183\n",
      "\t\tStep 184\n",
      "\t\tStep 185\n",
      "\t\tStep 186\n",
      "\t\tStep 187\n",
      "\t\tStep 188\n",
      "\t\tStep 189\n",
      "\t\tStep 190\n",
      "\t\tStep 191\n",
      "\t\tStep 192\n",
      "\tCross-validation fold 2\n",
      "\t\tStep 1: new best cost of 461.486\n",
      "\t\tStep 2: new best cost of 458.053\n",
      "\t\tStep 3: new best cost of 446.501\n",
      "\t\tStep 4\n",
      "\t\tStep 5: new best cost of 436.722\n",
      "\t\tStep 6: new best cost of 430.614\n",
      "\t\tStep 7: new best cost of 422.262\n",
      "\t\tStep 8\n",
      "\t\tStep 9: new best cost of 410.211\n",
      "\t\tStep 10: new best cost of 407.743\n",
      "\t\tStep 11: new best cost of 401.169\n",
      "\t\tStep 12: new best cost of 397.977\n",
      "\t\tStep 13: new best cost of 385.623\n",
      "\t\tStep 14: new best cost of 381.942\n",
      "\t\tStep 15: new best cost of 375.301\n",
      "\t\tStep 16: new best cost of 373.089\n",
      "\t\tStep 17\n",
      "\t\tStep 18: new best cost of 368.612\n",
      "\t\tStep 19\n",
      "\t\tStep 20: new best cost of 367.418\n",
      "\t\tStep 21: new best cost of 362.894\n",
      "\t\tStep 22: new best cost of 355.869\n",
      "\t\tStep 23\n",
      "\t\tStep 24: new best cost of 347.810\n",
      "\t\tStep 25: new best cost of 346.528\n",
      "\t\tStep 26\n",
      "\t\tStep 27: new best cost of 338.132\n",
      "\t\tStep 28: new best cost of 336.389\n",
      "\t\tStep 29\n",
      "\t\tStep 30: new best cost of 331.798\n",
      "\t\tStep 31: new best cost of 327.777\n",
      "\t\tStep 32\n",
      "\t\tStep 33: new best cost of 326.508\n",
      "\t\tStep 34: new best cost of 325.992\n",
      "\t\tStep 35\n",
      "\t\tStep 36: new best cost of 321.813\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40: new best cost of 319.201\n",
      "\t\tStep 41: new best cost of 317.219\n",
      "\t\tStep 42\n",
      "\t\tStep 43: new best cost of 316.357\n",
      "\t\tStep 44\n",
      "\t\tStep 45: new best cost of 311.962\n",
      "\t\tStep 46: new best cost of 311.460\n",
      "\t\tStep 47\n",
      "\t\tStep 48: new best cost of 309.294\n",
      "\t\tStep 49\n",
      "\t\tStep 50: new best cost of 307.279\n",
      "\t\tStep 51\n",
      "\t\tStep 52: new best cost of 305.279\n",
      "\t\tStep 53: new best cost of 302.411\n",
      "\t\tStep 54\n",
      "\t\tStep 55: new best cost of 301.981\n",
      "\t\tStep 56\n",
      "\t\tStep 57: new best cost of 300.229\n",
      "\t\tStep 58: new best cost of 299.387\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63: new best cost of 297.085\n",
      "\t\tStep 64: new best cost of 295.227\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68: new best cost of 293.307\n",
      "\t\tStep 69: new best cost of 292.532\n",
      "\t\tStep 70: new best cost of 292.492\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73: new best cost of 289.255\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78: new best cost of 287.788\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85: new best cost of 287.764\n",
      "\t\tStep 86: new best cost of 287.659\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89: new best cost of 286.553\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99: new best cost of 285.593\n",
      "\t\tStep 100\n",
      "\t\tStep 101: new best cost of 283.082\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\t\tStep 118\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123: new best cost of 279.781\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "\t\tStep 126\n",
      "\t\tStep 127\n",
      "\t\tStep 128\n",
      "\t\tStep 129: new best cost of 279.435\n",
      "\t\tStep 130\n",
      "\t\tStep 131\n",
      "\t\tStep 132\n",
      "\t\tStep 133\n",
      "\t\tStep 134\n",
      "\t\tStep 135\n",
      "\t\tStep 136\n",
      "\t\tStep 137\n",
      "\t\tStep 138\n",
      "\t\tStep 139\n",
      "\t\tStep 140\n",
      "\t\tStep 141: new best cost of 279.365\n",
      "\t\tStep 142\n",
      "\t\tStep 143\n",
      "\t\tStep 144\n",
      "\t\tStep 145\n",
      "\t\tStep 146\n",
      "\t\tStep 147: new best cost of 279.140\n",
      "\t\tStep 148: new best cost of 277.929\n",
      "\t\tStep 149\n",
      "\t\tStep 150\n",
      "\t\tStep 151\n",
      "\t\tStep 152\n",
      "\t\tStep 153\n",
      "\t\tStep 154\n",
      "\t\tStep 155\n",
      "\t\tStep 156: new best cost of 277.830\n",
      "\t\tStep 157\n",
      "\t\tStep 158\n",
      "\t\tStep 159\n",
      "\t\tStep 160\n",
      "\t\tStep 161\n",
      "\t\tStep 162\n",
      "\t\tStep 163\n",
      "\t\tStep 164\n",
      "\t\tStep 165\n",
      "\t\tStep 166\n",
      "\t\tStep 167\n",
      "\t\tStep 168\n",
      "\t\tStep 169\n",
      "\t\tStep 170\n",
      "\t\tStep 171\n",
      "\t\tStep 172\n",
      "\t\tStep 173\n",
      "\t\tStep 174\n",
      "\t\tStep 175\n",
      "\t\tStep 176\n",
      "\t\tStep 177\n",
      "\t\tStep 178\n",
      "\t\tStep 179\n",
      "\t\tStep 180\n",
      "\t\tStep 181\n",
      "\tCross-validation fold 3\n",
      "\t\tStep 1: new best cost of 462.448\n",
      "\t\tStep 2: new best cost of 457.485\n",
      "\t\tStep 3: new best cost of 449.022\n",
      "\t\tStep 4: new best cost of 443.746\n",
      "\t\tStep 5: new best cost of 437.416\n",
      "\t\tStep 6: new best cost of 429.290\n",
      "\t\tStep 7: new best cost of 420.948\n",
      "\t\tStep 8: new best cost of 411.211\n",
      "\t\tStep 9: new best cost of 409.044\n",
      "\t\tStep 10: new best cost of 398.054\n",
      "\t\tStep 11: new best cost of 389.649\n",
      "\t\tStep 12: new best cost of 387.557\n",
      "\t\tStep 13: new best cost of 377.215\n",
      "\t\tStep 14: new best cost of 372.733\n",
      "\t\tStep 15\n",
      "\t\tStep 16: new best cost of 367.152\n",
      "\t\tStep 17: new best cost of 365.312\n",
      "\t\tStep 18\n",
      "\t\tStep 19: new best cost of 353.796\n",
      "\t\tStep 20: new best cost of 347.301\n",
      "\t\tStep 21: new best cost of 341.823\n",
      "\t\tStep 22\n",
      "\t\tStep 23: new best cost of 331.795\n",
      "\t\tStep 24: new best cost of 329.326\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27: new best cost of 322.547\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30: new best cost of 319.872\n",
      "\t\tStep 31: new best cost of 314.161\n",
      "\t\tStep 32\n",
      "\t\tStep 33: new best cost of 307.985\n",
      "\t\tStep 34: new best cost of 304.208\n",
      "\t\tStep 35\n",
      "\t\tStep 36: new best cost of 301.331\n",
      "\t\tStep 37: new best cost of 300.249\n",
      "\t\tStep 38: new best cost of 299.956\n",
      "\t\tStep 39: new best cost of 295.520\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43: new best cost of 295.245\n",
      "\t\tStep 44\n",
      "\t\tStep 45: new best cost of 294.824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48: new best cost of 285.706\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53: new best cost of 281.357\n",
      "\t\tStep 54: new best cost of 280.632\n",
      "\t\tStep 55: new best cost of 280.161\n",
      "\t\tStep 56: new best cost of 279.049\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64: new best cost of 277.792\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67: new best cost of 277.629\n",
      "\t\tStep 68\n",
      "\t\tStep 69: new best cost of 274.516\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75: new best cost of 272.937\n",
      "\t\tStep 76\n",
      "\t\tStep 77: new best cost of 272.894\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82: new best cost of 271.005\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88: new best cost of 270.990\n",
      "\t\tStep 89: new best cost of 268.107\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92: new best cost of 266.229\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97: new best cost of 266.037\n",
      "\t\tStep 98: new best cost of 265.084\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\t\tStep 101: new best cost of 262.955\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115: new best cost of 262.891\n",
      "\t\tStep 116\n",
      "\t\tStep 117: new best cost of 262.866\n",
      "\t\tStep 118\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "\t\tStep 126\n",
      "\t\tStep 127\n",
      "\t\tStep 128\n",
      "\t\tStep 129\n",
      "\t\tStep 130\n",
      "\t\tStep 131: new best cost of 261.322\n",
      "\t\tStep 132\n",
      "\t\tStep 133\n",
      "\t\tStep 134\n",
      "\t\tStep 135\n",
      "\t\tStep 136\n",
      "\t\tStep 137\n",
      "\t\tStep 138\n",
      "\t\tStep 139\n",
      "\t\tStep 140\n",
      "\t\tStep 141\n",
      "\t\tStep 142\n",
      "\t\tStep 143\n",
      "\t\tStep 144\n",
      "\t\tStep 145\n",
      "\t\tStep 146\n",
      "\t\tStep 147: new best cost of 260.040\n",
      "\t\tStep 148\n",
      "\t\tStep 149\n",
      "\t\tStep 150\n",
      "\t\tStep 151\n",
      "\t\tStep 152\n",
      "\t\tStep 153\n",
      "\t\tStep 154\n",
      "\t\tStep 155\n",
      "\t\tStep 156\n",
      "\t\tStep 157\n",
      "\t\tStep 158\n",
      "\t\tStep 159: new best cost of 260.009\n",
      "\t\tStep 160\n",
      "\t\tStep 161\n",
      "\t\tStep 162\n",
      "\t\tStep 163\n",
      "\t\tStep 164\n",
      "\t\tStep 165: new best cost of 259.607\n",
      "\t\tStep 166: new best cost of 259.592\n",
      "\t\tStep 167: new best cost of 259.206\n",
      "\t\tStep 168\n",
      "\t\tStep 169\n",
      "\t\tStep 170\n",
      "\t\tStep 171: new best cost of 258.179\n",
      "\t\tStep 172: new best cost of 257.489\n",
      "\t\tStep 173\n",
      "\t\tStep 174\n",
      "\t\tStep 175\n",
      "\t\tStep 176\n",
      "\t\tStep 177\n",
      "\t\tStep 178\n",
      "\t\tStep 179\n",
      "\t\tStep 180\n",
      "\t\tStep 181\n",
      "\t\tStep 182\n",
      "\t\tStep 183\n",
      "\t\tStep 184\n",
      "\t\tStep 185\n",
      "\t\tStep 186\n",
      "\t\tStep 187\n",
      "\t\tStep 188\n",
      "\t\tStep 189\n",
      "\t\tStep 190\n",
      "\t\tStep 191\n",
      "\t\tStep 192\n",
      "\t\tStep 193\n",
      "\t\tStep 194\n",
      "\t\tStep 195\n",
      "\t\tStep 196\n",
      "\t\tStep 197\n",
      "\tCross-validation fold 4\n",
      "\t\tStep 1: new best cost of 461.852\n",
      "\t\tStep 2: new best cost of 452.842\n",
      "\t\tStep 3: new best cost of 451.941\n",
      "\t\tStep 4: new best cost of 442.121\n",
      "\t\tStep 5: new best cost of 438.334\n",
      "\t\tStep 6: new best cost of 432.207\n",
      "\t\tStep 7: new best cost of 423.232\n",
      "\t\tStep 8: new best cost of 420.114\n",
      "\t\tStep 9: new best cost of 407.193\n",
      "\t\tStep 10\n",
      "\t\tStep 11: new best cost of 396.335\n",
      "\t\tStep 12\n",
      "\t\tStep 13: new best cost of 383.218\n",
      "\t\tStep 14: new best cost of 374.944\n",
      "\t\tStep 15\n",
      "\t\tStep 16: new best cost of 363.618\n",
      "\t\tStep 17\n",
      "\t\tStep 18: new best cost of 358.718\n",
      "\t\tStep 19: new best cost of 342.801\n",
      "\t\tStep 20: new best cost of 341.617\n",
      "\t\tStep 21\n",
      "\t\tStep 22: new best cost of 332.868\n",
      "\t\tStep 23: new best cost of 326.996\n",
      "\t\tStep 24: new best cost of 326.037\n",
      "\t\tStep 25: new best cost of 322.548\n",
      "\t\tStep 26: new best cost of 322.272\n",
      "\t\tStep 27: new best cost of 317.360\n",
      "\t\tStep 28\n",
      "\t\tStep 29: new best cost of 310.958\n",
      "\t\tStep 30: new best cost of 305.681\n",
      "\t\tStep 31\n",
      "\t\tStep 32: new best cost of 304.684\n",
      "\t\tStep 33: new best cost of 303.883\n",
      "\t\tStep 34\n",
      "\t\tStep 35: new best cost of 300.932\n",
      "\t\tStep 36: new best cost of 300.170\n",
      "\t\tStep 37\n",
      "\t\tStep 38: new best cost of 293.745\n",
      "\t\tStep 39\n",
      "\t\tStep 40: new best cost of 292.251\n",
      "\t\tStep 41: new best cost of 292.162\n",
      "\t\tStep 42: new best cost of 288.985\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45: new best cost of 286.174\n",
      "\t\tStep 46: new best cost of 285.030\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49: new best cost of 282.583\n",
      "\t\tStep 50: new best cost of 282.096\n",
      "\t\tStep 51: new best cost of 279.927\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54: new best cost of 277.477\n",
      "\t\tStep 55: new best cost of 276.839\n",
      "\t\tStep 56: new best cost of 275.306\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59: new best cost of 274.672\n",
      "\t\tStep 60\n",
      "\t\tStep 61: new best cost of 272.702\n",
      "\t\tStep 62: new best cost of 271.301\n",
      "\t\tStep 63\n",
      "\t\tStep 64: new best cost of 269.867\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68: new best cost of 266.524\n",
      "\t\tStep 69: new best cost of 264.720\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72: new best cost of 263.829\n",
      "\t\tStep 73: new best cost of 259.282\n",
      "\t\tStep 74: new best cost of 258.947\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83: new best cost of 257.830\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89: new best cost of 256.570\n",
      "\t\tStep 90: new best cost of 255.561\n",
      "\t\tStep 91: new best cost of 253.791\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96: new best cost of 252.777\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106: new best cost of 251.865\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110: new best cost of 250.632\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115: new best cost of 249.873\n",
      "\t\tStep 116\n",
      "\t\tStep 117: new best cost of 249.794\n",
      "\t\tStep 118\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "\t\tStep 126\n",
      "\t\tStep 127\n",
      "\t\tStep 128\n",
      "\t\tStep 129\n",
      "\t\tStep 130\n",
      "\t\tStep 131\n",
      "\t\tStep 132\n",
      "\t\tStep 133\n",
      "\t\tStep 134\n",
      "\t\tStep 135\n",
      "\t\tStep 136\n",
      "\t\tStep 137\n",
      "\t\tStep 138\n",
      "\t\tStep 139\n",
      "\t\tStep 140\n",
      "\t\tStep 141\n",
      "\t\tStep 142\n",
      "\tCross-validation fold 5\n",
      "\t\tStep 1: new best cost of 464.053\n",
      "\t\tStep 2: new best cost of 458.603\n",
      "\t\tStep 3: new best cost of 452.423\n",
      "\t\tStep 4: new best cost of 445.594\n",
      "\t\tStep 5: new best cost of 441.916\n",
      "\t\tStep 6: new best cost of 428.742\n",
      "\t\tStep 7: new best cost of 428.155\n",
      "\t\tStep 8: new best cost of 418.571\n",
      "\t\tStep 9: new best cost of 413.076\n",
      "\t\tStep 10: new best cost of 407.628\n",
      "\t\tStep 11: new best cost of 401.917\n",
      "\t\tStep 12: new best cost of 394.845\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15: new best cost of 373.477\n",
      "\t\tStep 16: new best cost of 369.938\n",
      "\t\tStep 17\n",
      "\t\tStep 18: new best cost of 360.950\n",
      "\t\tStep 19\n",
      "\t\tStep 20: new best cost of 358.315\n",
      "\t\tStep 21: new best cost of 349.585\n",
      "\t\tStep 22\n",
      "\t\tStep 23: new best cost of 344.016\n",
      "\t\tStep 24: new best cost of 341.860\n",
      "\t\tStep 25: new best cost of 340.868\n",
      "\t\tStep 26: new best cost of 331.643\n",
      "\t\tStep 27: new best cost of 328.072\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30: new best cost of 321.891\n",
      "\t\tStep 31: new best cost of 316.166\n",
      "\t\tStep 32: new best cost of 316.013\n",
      "\t\tStep 33\n",
      "\t\tStep 34: new best cost of 311.643\n",
      "\t\tStep 35\n",
      "\t\tStep 36: new best cost of 311.008\n",
      "\t\tStep 37: new best cost of 309.517\n",
      "\t\tStep 38: new best cost of 306.305\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41: new best cost of 298.754\n",
      "\t\tStep 42\n",
      "\t\tStep 43: new best cost of 297.461\n",
      "\t\tStep 44: new best cost of 297.305\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48: new best cost of 294.660\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53: new best cost of 288.555\n",
      "\t\tStep 54: new best cost of 288.260\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60: new best cost of 282.476\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65: new best cost of 282.172\n",
      "\t\tStep 66: new best cost of 277.575\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72: new best cost of 275.969\n",
      "\t\tStep 73: new best cost of 275.050\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76: new best cost of 274.621\n",
      "\t\tStep 77: new best cost of 273.958\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85: new best cost of 272.696\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90: new best cost of 272.614\n",
      "\t\tStep 91: new best cost of 272.541\n",
      "\t\tStep 92\n",
      "\t\tStep 93: new best cost of 271.326\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99\n",
      "\t\tStep 100: new best cost of 268.648\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111: new best cost of 267.081\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\t\tStep 118\n",
      "\t\tStep 119: new best cost of 266.244\n",
      "\t\tStep 120: new best cost of 266.167\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123\n",
      "\t\tStep 124\n",
      "\t\tStep 125\n",
      "\t\tStep 126\n",
      "\t\tStep 127\n",
      "\t\tStep 128\n",
      "\t\tStep 129\n",
      "\t\tStep 130: new best cost of 265.233\n",
      "\t\tStep 131\n",
      "\t\tStep 132\n",
      "\t\tStep 133\n",
      "\t\tStep 134\n",
      "\t\tStep 135\n",
      "\t\tStep 136\n",
      "\t\tStep 137: new best cost of 264.948\n",
      "\t\tStep 138\n",
      "\t\tStep 139: new best cost of 264.386\n",
      "\t\tStep 140\n",
      "\t\tStep 141\n",
      "\t\tStep 142\n",
      "\t\tStep 143\n",
      "\t\tStep 144\n",
      "\t\tStep 145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tStep 146\n",
      "\t\tStep 147\n",
      "\t\tStep 148\n",
      "\t\tStep 149\n",
      "\t\tStep 150\n",
      "\t\tStep 151\n",
      "\t\tStep 152\n",
      "\t\tStep 153\n",
      "\t\tStep 154\n",
      "\t\tStep 155: new best cost of 263.934\n",
      "\t\tStep 156\n",
      "\t\tStep 157\n",
      "\t\tStep 158\n",
      "\t\tStep 159\n",
      "\t\tStep 160\n",
      "\t\tStep 161\n",
      "\t\tStep 162\n",
      "\t\tStep 163\n",
      "\t\tStep 164\n",
      "\t\tStep 165\n",
      "\t\tStep 166: new best cost of 262.718\n",
      "\t\tStep 167\n",
      "\t\tStep 168\n",
      "\t\tStep 169\n",
      "\t\tStep 170\n",
      "\t\tStep 171\n",
      "\t\tStep 172\n",
      "\t\tStep 173\n",
      "\t\tStep 174\n",
      "\t\tStep 175\n",
      "\t\tStep 176\n",
      "\t\tStep 177\n",
      "\t\tStep 178\n",
      "\t\tStep 179\n",
      "\t\tStep 180\n",
      "\t\tStep 181\n",
      "\t\tStep 182\n",
      "\t\tStep 183\n",
      "\t\tStep 184\n",
      "\t\tStep 185\n",
      "\t\tStep 186\n",
      "\t\tStep 187\n",
      "\t\tStep 188\n",
      "\t\tStep 189\n",
      "\t\tStep 190\n",
      "\t\tStep 191\n",
      "Wine gradient descent:\n",
      "\tCross-validation fold 1\n",
      "\t\tStep 1: new best cost of 46.143\n",
      "\t\tStep 2: new best cost of 42.666\n",
      "\t\tStep 3\n",
      "\t\tStep 4: new best cost of 40.352\n",
      "\t\tStep 5\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8: new best cost of 40.095\n",
      "\t\tStep 9: new best cost of 39.760\n",
      "\t\tStep 10\n",
      "\t\tStep 11\n",
      "\t\tStep 12: new best cost of 37.734\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18: new best cost of 37.085\n",
      "\t\tStep 19\n",
      "\t\tStep 20: new best cost of 36.362\n",
      "\t\tStep 21\n",
      "\t\tStep 22: new best cost of 31.863\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35: new best cost of 31.049\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38: new best cost of 29.086\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57: new best cost of 28.036\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\tCross-validation fold 2\n",
      "\t\tStep 1: new best cost of 41.079\n",
      "\t\tStep 2\n",
      "\t\tStep 3: new best cost of 37.633\n",
      "\t\tStep 4\n",
      "\t\tStep 5\n",
      "\t\tStep 6\n",
      "\t\tStep 7: new best cost of 34.473\n",
      "\t\tStep 8\n",
      "\t\tStep 9\n",
      "\t\tStep 10\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14: new best cost of 34.370\n",
      "\t\tStep 15\n",
      "\t\tStep 16: new best cost of 32.699\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21: new best cost of 29.073\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40: new best cost of 28.805\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50: new best cost of 28.510\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58: new best cost of 28.441\n",
      "\t\tStep 59\n",
      "\t\tStep 60: new best cost of 27.625\n",
      "\t\tStep 61\n",
      "\t\tStep 62: new best cost of 27.096\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71: new best cost of 26.149\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88: new best cost of 25.046\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\tCross-validation fold 3\n",
      "\t\tStep 1: new best cost of 45.321\n",
      "\t\tStep 2: new best cost of 44.454\n",
      "\t\tStep 3: new best cost of 34.167\n",
      "\t\tStep 4\n",
      "\t\tStep 5\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8\n",
      "\t\tStep 9\n",
      "\t\tStep 10\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19: new best cost of 32.199\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25: new best cost of 29.601\n",
      "\t\tStep 26\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47: new best cost of 28.417\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51: new best cost of 25.209\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75: new best cost of 23.167\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\tCross-validation fold 4\n",
      "\t\tStep 1: new best cost of 42.933\n",
      "\t\tStep 2: new best cost of 31.774\n",
      "\t\tStep 3\n",
      "\t\tStep 4\n",
      "\t\tStep 5\n",
      "\t\tStep 6\n",
      "\t\tStep 7\n",
      "\t\tStep 8\n",
      "\t\tStep 9\n",
      "\t\tStep 10\n",
      "\t\tStep 11: new best cost of 30.173\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24\n",
      "\t\tStep 25\n",
      "\t\tStep 26\n",
      "\t\tStep 27: new best cost of 25.541\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35\n",
      "\t\tStep 36\n",
      "\t\tStep 37\n",
      "\t\tStep 38\n",
      "\t\tStep 39\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\tCross-validation fold 5\n",
      "\t\tStep 1: new best cost of 47.220\n",
      "\t\tStep 2: new best cost of 47.202\n",
      "\t\tStep 3\n",
      "\t\tStep 4: new best cost of 46.736\n",
      "\t\tStep 5: new best cost of 45.613\n",
      "\t\tStep 6\n",
      "\t\tStep 7: new best cost of 41.870\n",
      "\t\tStep 8\n",
      "\t\tStep 9\n",
      "\t\tStep 10\n",
      "\t\tStep 11\n",
      "\t\tStep 12\n",
      "\t\tStep 13\n",
      "\t\tStep 14\n",
      "\t\tStep 15\n",
      "\t\tStep 16\n",
      "\t\tStep 17\n",
      "\t\tStep 18\n",
      "\t\tStep 19\n",
      "\t\tStep 20\n",
      "\t\tStep 21\n",
      "\t\tStep 22\n",
      "\t\tStep 23\n",
      "\t\tStep 24: new best cost of 41.075\n",
      "\t\tStep 25: new best cost of 40.323\n",
      "\t\tStep 26: new best cost of 38.837\n",
      "\t\tStep 27\n",
      "\t\tStep 28\n",
      "\t\tStep 29\n",
      "\t\tStep 30\n",
      "\t\tStep 31\n",
      "\t\tStep 32\n",
      "\t\tStep 33\n",
      "\t\tStep 34\n",
      "\t\tStep 35: new best cost of 38.691\n",
      "\t\tStep 36\n",
      "\t\tStep 37: new best cost of 37.390\n",
      "\t\tStep 38\n",
      "\t\tStep 39: new best cost of 37.155\n",
      "\t\tStep 40\n",
      "\t\tStep 41\n",
      "\t\tStep 42\n",
      "\t\tStep 43\n",
      "\t\tStep 44\n",
      "\t\tStep 45\n",
      "\t\tStep 46\n",
      "\t\tStep 47\n",
      "\t\tStep 48\n",
      "\t\tStep 49\n",
      "\t\tStep 50: new best cost of 35.528\n",
      "\t\tStep 51\n",
      "\t\tStep 52\n",
      "\t\tStep 53\n",
      "\t\tStep 54\n",
      "\t\tStep 55\n",
      "\t\tStep 56: new best cost of 32.809\n",
      "\t\tStep 57\n",
      "\t\tStep 58\n",
      "\t\tStep 59\n",
      "\t\tStep 60\n",
      "\t\tStep 61\n",
      "\t\tStep 62\n",
      "\t\tStep 63\n",
      "\t\tStep 64\n",
      "\t\tStep 65\n",
      "\t\tStep 66: new best cost of 32.512\n",
      "\t\tStep 67\n",
      "\t\tStep 68\n",
      "\t\tStep 69\n",
      "\t\tStep 70\n",
      "\t\tStep 71\n",
      "\t\tStep 72\n",
      "\t\tStep 73\n",
      "\t\tStep 74\n",
      "\t\tStep 75\n",
      "\t\tStep 76\n",
      "\t\tStep 77\n",
      "\t\tStep 78\n",
      "\t\tStep 79\n",
      "\t\tStep 80\n",
      "\t\tStep 81\n",
      "\t\tStep 82\n",
      "\t\tStep 83\n",
      "\t\tStep 84\n",
      "\t\tStep 85\n",
      "\t\tStep 86: new best cost of 32.144\n",
      "\t\tStep 87\n",
      "\t\tStep 88\n",
      "\t\tStep 89\n",
      "\t\tStep 90\n",
      "\t\tStep 91\n",
      "\t\tStep 92\n",
      "\t\tStep 93\n",
      "\t\tStep 94\n",
      "\t\tStep 95\n",
      "\t\tStep 96\n",
      "\t\tStep 97\n",
      "\t\tStep 98: new best cost of 29.964\n",
      "\t\tStep 99\n",
      "\t\tStep 100\n",
      "\t\tStep 101\n",
      "\t\tStep 102\n",
      "\t\tStep 103\n",
      "\t\tStep 104\n",
      "\t\tStep 105\n",
      "\t\tStep 106\n",
      "\t\tStep 107\n",
      "\t\tStep 108\n",
      "\t\tStep 109\n",
      "\t\tStep 110\n",
      "\t\tStep 111\n",
      "\t\tStep 112\n",
      "\t\tStep 113\n",
      "\t\tStep 114\n",
      "\t\tStep 115\n",
      "\t\tStep 116\n",
      "\t\tStep 117\n",
      "\t\tStep 118\n",
      "\t\tStep 119\n",
      "\t\tStep 120\n",
      "\t\tStep 121\n",
      "\t\tStep 122\n",
      "\t\tStep 123\n",
      "\n",
      "\n",
      "Digits training accuracy: 94.5%\n",
      "Digits training cost: 1243.475\n",
      "Digits validation accuracy: 91.5%\n",
      "Digits validation cost: 1300.760\n",
      "Wine training accuracy: 78.8%\n",
      "Wine training cost: 144.712\n",
      "Wine validation accuracy: 84.8%\n",
      "Wine validation cost: 131.754\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "default_run = runLogisticRegression(30, 0.04, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean = lambda x1, x2: np.sqrt(np.sum((x1 - x2)**2, axis=-1))\n",
    "manhattan = lambda x1, x2: np.sum(np.abs(x1 - x2), axis=-1)\n",
    "\n",
    "class KNN:\n",
    "\n",
    "    def __init__(self, K=1, dist_fn= euclidean):\n",
    "        self.dist_fn = dist_fn\n",
    "        self.K = K\n",
    "        return\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.C = len(y[0])\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        num_test = x_test.shape[0]\n",
    "        distances = self.dist_fn(self.x[None,:,:], x_test[:,None,:])\n",
    "        knns = np.zeros((num_test, self.K), dtype=int)\n",
    "        y_prob = np.zeros((num_test),dtype=int)\n",
    "        counts = np.zeros((num_test, self.C))\n",
    "        \n",
    "        for i in range(num_test):\n",
    "            knns[i,:] = np.argsort(distances[i])[:self.K]\n",
    "            k_count=np.zeros(self.K, dtype=int)\n",
    "            \n",
    "            for s, arr in enumerate(self.y[knns[i,:]]):\n",
    "                k_count[s] = np.argmax(arr)\n",
    "            \n",
    "            y_prob_i, counts_i = np.unique(k_count, return_counts=True)\n",
    "            y_prob[i] = int(y_prob_i[np.argmax(counts_i)])\n",
    "        \n",
    "        return y_prob, knns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN digits validation accuracy: 95.7%\n",
      "KNN wine validation accuracy: 85.4%\n"
     ]
    }
   ],
   "source": [
    "KNNmodel = KNN(K=11)\n",
    "\n",
    "digits_knn_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    y_prob, knns = KNNmodel.fit(np.asarray(xDigitsTrainingSets[fold]), np.asarray(yDigitsTrainingSets[fold])).predict(np.asarray(xDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, prob in enumerate(y_prob):\n",
    "        if prob == np.argmax(yDigitsValidationSets[fold][i]):\n",
    "            digits_knn_accuracy += 1\n",
    "\n",
    "digits_knn_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"KNN digits validation accuracy: {digits_knn_accuracy*100:.1f}%\")\n",
    "\n",
    "KNNmodel = KNN(K=7)\n",
    "\n",
    "wine_knn_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    y_prob, knns = KNNmodel.fit(np.asarray(xWineTrainingSets[fold]), np.asarray(yWineTrainingSets[fold])).predict(np.asarray(xWineValidationSets[fold]))\n",
    "    \n",
    "    for i, prob in enumerate(y_prob):\n",
    "        if prob == np.argmax(yWineValidationSets[fold][i]):\n",
    "            wine_knn_accuracy += 1\n",
    "            \n",
    "wine_knn_accuracy /= len(wine.data)\n",
    "\n",
    "print(f\"KNN wine validation accuracy: {wine_knn_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive base digits validation accuracy: 81.1%\n",
      "Naive base wine validation accuracy: 93.8%\n"
     ]
    }
   ],
   "source": [
    "digits_naive_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    labels_training = np.zeros(len(yDigitsTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(np.asarray(xDigitsTrainingSets[fold]), labels_training).predict(np.asarray(xDigitsValidationSets[fold]))\n",
    "\n",
    "    for i, label in enumerate(y_pred):\n",
    "        if label == labels_validation[i]:\n",
    "            digits_naive_accuracy += 1\n",
    "\n",
    "digits_naive_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"Naive base digits validation accuracy: {digits_naive_accuracy*100:.1f}%\")\n",
    "\n",
    "wine_naive_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    labels_training = np.zeros(len(yWineTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yWineTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yWineValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yWineValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(np.asarray(xWineTrainingSets[fold]), labels_training).predict(np.asarray(xWineValidationSets[fold]))\n",
    "\n",
    "    for i, label in enumerate(y_pred):\n",
    "        if label == labels_validation[i]:\n",
    "            wine_naive_accuracy += 1\n",
    "\n",
    "wine_naive_accuracy /= len(wine.data)\n",
    "\n",
    "print(f\"Naive base wine validation accuracy: {wine_naive_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HoGFeatures(img, cellSize, blockSize, nbins):\n",
    "    cell_size = (cellSize, cellSize)\n",
    "    block_size = (blockSize, blockSize)\n",
    "    \n",
    "    hog = cv.HOGDescriptor(_winSize=(img.shape[1] // cell_size[1] * cell_size[1],\n",
    "                                     img.shape[0] // cell_size[0] * cell_size[0]),\n",
    "                           _blockSize=(block_size[1] * cell_size[1],\n",
    "                                       block_size[0] * cell_size[0]),\n",
    "                           _blockStride=(cell_size[1], cell_size[0]),\n",
    "                           _cellSize=(cell_size[1], cell_size[0]),\n",
    "                           _nbins=nbins\n",
    "    )\n",
    "    \n",
    "    return hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeHoGFeatures(imageArray):\n",
    "    HoG = HoGFeatures(imageArray[0], 2, 2, 2)\n",
    "    features = []\n",
    "    \n",
    "    for i, image in enumerate(imageArray):\n",
    "        features.append(HoG.compute((image*255).astype(np.uint8)))\n",
    "        \n",
    "    features = np.array(np.squeeze(features))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC digits validation accuracy: 89.6%\n"
     ]
    }
   ],
   "source": [
    "digits_svc_accuracy = 0\n",
    "\n",
    "for fold in range(5):\n",
    "    numbers_training = []\n",
    "    numbers_validation = []\n",
    "    \n",
    "    for i, number in enumerate(xDigitsTrainingSets[fold]):\n",
    "        numbers_training.append(np.asarray(number).reshape(8, 8))\n",
    "        \n",
    "    for i, number in enumerate(xDigitsValidationSets[fold]):\n",
    "        numbers_validation.append(np.asarray(number).reshape(8, 8))  \n",
    "        \n",
    "    HoGs_training = makeHoGFeatures(np.asarray(numbers_training))\n",
    "    HoGs_validation = makeHoGFeatures(np.asarray(numbers_validation))\n",
    "\n",
    "    clf = svm.SVC(gamma='auto', C=100) \n",
    "    \n",
    "    labels_training = np.zeros(len(yDigitsTrainingSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsTrainingSets[fold]):\n",
    "        labels_training[i] = np.argmax(arr)\n",
    "        \n",
    "    labels_validation = np.zeros(len(yDigitsValidationSets[fold]))\n",
    "    \n",
    "    for i, arr in enumerate(yDigitsValidationSets[fold]):\n",
    "        labels_validation[i] = np.argmax(arr)\n",
    "    \n",
    "    clf.fit(HoGs_training, labels_training)\n",
    "\n",
    "    labels_predicted = clf.predict(HoGs_validation)\n",
    "    \n",
    "    for i, label in enumerate(labels_predicted):\n",
    "        if label == labels_validation[i]:\n",
    "            digits_svc_accuracy += 1\n",
    "\n",
    "digits_svc_accuracy /= len(digits.data)\n",
    "\n",
    "print(f\"SVC digits validation accuracy: {digits_svc_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis of model hyper-parameters\n",
    "\n",
    "batch_size = 30\n",
    "learning_rate = 0.04\n",
    "momentum = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis of changing batch size\n",
    "if run_search:\n",
    "    batch_size_tests = np.array(range(2, 40, 2))#[30, 25, 20, 15, 10, 5]\n",
    "    batch_size_results = []\n",
    "\n",
    "    for test in batch_size_tests:\n",
    "        batch_size_results.append(runLogisticRegression(test, learning_rate, momentum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of changing learning rate\n",
    "if run_search:\n",
    "    learning_rate_tests = np.array(range(1, 20, 1))/100#[0.02, 0.04, 0.06, 0.08]\n",
    "    learning_rate_results = []\n",
    "\n",
    "    for test in learning_rate_tests:\n",
    "        learning_rate_results.append(runLogisticRegression(batch_size, test, momentum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of changing momentum\n",
    "if run_search:\n",
    "    momentum_tests = np.array(range(1, 20, 1))/20#[0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "    momentum_results = []\n",
    "\n",
    "    for test in momentum_tests:\n",
    "        momentum_results.append(runLogisticRegression(batch_size, learning_rate, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(batch_size_results[5][2])\n",
    "\n",
    "if run_search:\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_size_inches(7,7)\n",
    "\n",
    "    ax1.plot(batch_size_tests, np.asarray(batch_size_results).T[0], label = f\"Digits validation accuracy\", color='green',linewidth=4)\n",
    "    ax1.plot(batch_size_tests, np.asarray(batch_size_results).T[2], label = f\"Wine validation accuracy\", color='limegreen',linewidth=2)\n",
    "    ax1.set_ylabel('Validation Accuracy',color='green',fontsize=13)\n",
    "    ax1.tick_params(axis='y', labelcolor='green', labelsize=13)\n",
    "    ax1.set_xlabel('Batch Size',fontsize=13)\n",
    "    ax2 = ax1.twinx() \n",
    "\n",
    "    ax2.plot(batch_size_tests, np.asarray(batch_size_results).T[1]/len(digits.data), label = f\"Digits cost\", color='red',linewidth=4)\n",
    "    ax2.plot(batch_size_tests, np.asarray(batch_size_results).T[3]/len(wine.data), label = f\"Wine cost\", color='maroon',linewidth=2)\n",
    "    ax2.set_ylabel('Cost per sample',color='maroon',fontsize=13)\n",
    "    ax2.tick_params(axis='y', labelcolor='maroon', labelsize=13)\n",
    "\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "    plt.legend(handles+handles2, labels+labels2,loc='lower right')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(batch_size_results[5][2])\n",
    "\n",
    "if run_search:\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_size_inches(7,7)\n",
    "\n",
    "    ax1.plot(learning_rate_tests, np.asarray(learning_rate_results).T[0], label = f\"Digits validation accuracy\", color='green',linewidth=4)\n",
    "    ax1.plot(learning_rate_tests, np.asarray(learning_rate_results).T[2], label = f\"Wine validation accuracy\", color='limegreen',linewidth=2)\n",
    "    ax1.set_ylabel('Validation Accuracy',color='green',fontsize=13)\n",
    "    ax1.tick_params(axis='y', labelcolor='green', labelsize=13)\n",
    "    ax1.set_xlabel('Learning Rate',fontsize=13)\n",
    "    ax2 = ax1.twinx() \n",
    "\n",
    "    ax2.plot(learning_rate_tests, np.asarray(learning_rate_results).T[1]/len(digits.data), label = f\"Digits cost\", color='red',linewidth=4)\n",
    "    ax2.plot(learning_rate_tests, np.asarray(learning_rate_results).T[3]/len(wine.data), label = f\"Wine cost\", color='maroon',linewidth=2)\n",
    "    ax2.set_ylabel('Cost per sample',color='maroon',fontsize=13)\n",
    "    ax2.tick_params(axis='y', labelcolor='maroon', labelsize=13)\n",
    "\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "    plt.legend(handles+handles2, labels+labels2,loc='lower left')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(batch_size_results[5][2])\n",
    "\n",
    "if run_search:\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_size_inches(7,7)\n",
    "\n",
    "    ax1.plot(momentum_tests, np.asarray(momentum_results).T[0], label = f\"Digits validation accuracy\", color='green',linewidth=4)\n",
    "    ax1.plot(momentum_tests, np.asarray(momentum_results).T[2], label = f\"Wine validation accuracy\", color='limegreen',linewidth=2)\n",
    "    ax1.set_ylabel('Validation Accuracy',color='green',fontsize=13)\n",
    "    ax1.tick_params(axis='y', labelcolor='green', labelsize=13)\n",
    "    ax1.set_xlabel('Momentum',fontsize=13)\n",
    "    ax2 = ax1.twinx() \n",
    "\n",
    "    ax2.plot(momentum_tests, np.asarray(momentum_results).T[1]/len(digits.data), label = f\"Digits cost\", color='red',linewidth=4)\n",
    "    ax2.plot(momentum_tests, np.asarray(momentum_results).T[3]/len(wine.data), label = f\"Wine cost\", color='maroon',linewidth=2)\n",
    "    ax2.set_ylabel('Cost per sample',color='maroon',fontsize=13)\n",
    "    ax2.tick_params(axis='y', labelcolor='maroon', labelsize=13)\n",
    "\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "    plt.legend(handles+handles2, labels+labels2,loc='lower right')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits_validation_accuracies = [default_run[0],digits_knn_accuracy,digits_naive_accuracy,digits_svc_accuracy]\n",
    "wine_validation_accuracies = [default_run[2],wine_knn_accuracy,wine_naive_accuracy,0]\n",
    "\n",
    "labels = [\"Logistic Regression\",\"KNN\",\"Naive Bayes\",\"SVC\"]\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, digits_validation_accuracies, width, label='Digits')\n",
    "rects2 = ax.bar(x + width/2, wine_validation_accuracies, width, label='Wine')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Validation Accuracies')\n",
    "ax.set_title('Comparison between different classifiers')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
