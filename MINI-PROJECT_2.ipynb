{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lia\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797\n",
      "1797\n"
     ]
    }
   ],
   "source": [
    "print(len(digits.data))\n",
    "print(digits.target.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "wine = fetch_openml(name='wine', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n",
      "178\n"
     ]
    }
   ],
   "source": [
    "print(len(wine.data))\n",
    "print(wine.target.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalising wine data\n",
    "wine_data_norm = []\n",
    "for col in wine.data.T:\n",
    "    col_norm = col/np.amax(col)\n",
    "    wine_data_norm.append(col_norm)\n",
    "    \n",
    "wine.data = np.asarray(wine_data_norm).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation for digits dataset\n",
    "\n",
    "digitsTrainingSetSize = int(np.ceil(0.8 * len(digits.data)))\n",
    "digitsValidationSetSize = int(len(digits.data) - digitsTrainingSetSize)\n",
    "\n",
    "xDigitsTrainingSets = []\n",
    "yDigitsTrainingSets = []\n",
    "xDigitsValidationSets = []\n",
    "yDigitsValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data[foldIndex*digitsValidationSetSize:((foldIndex*digitsValidationSetSize)+digitsValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(digits.target[index])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "\n",
    "    for index, data in enumerate(digits.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(digits.target[index])\n",
    "            \n",
    "    xDigitsTrainingSets.append(xTrainingSet)\n",
    "    yDigitsTrainingSets.append(yTrainingSet)\n",
    "    xDigitsValidationSets.append(xValidationSet)\n",
    "    yDigitsValidationSets.append(yValidationSet)\n",
    "    \n",
    "# 5-fold cross validation for wine dataset\n",
    "\n",
    "wineTrainingSetSize = int(np.ceil(0.8 * len(wine.data)))\n",
    "wineValidationSetSize = int(len(wine.data) - wineTrainingSetSize)\n",
    "\n",
    "xWineTrainingSets = []\n",
    "yWineTrainingSets = []\n",
    "xWineValidationSets = []\n",
    "yWineValidationSets = []\n",
    "\n",
    "for foldIndex in range(5):\n",
    "\n",
    "    xValidationSet = []\n",
    "    yValidationSet = []\n",
    "    for index, data in enumerate(wine.data[foldIndex*wineValidationSetSize:((foldIndex*wineValidationSetSize)+wineValidationSetSize)]):\n",
    "        xValidationSet.append(data.tolist())\n",
    "        yValidationSet.append(wine.target[index+(foldIndex*wineValidationSetSize)])\n",
    "    \n",
    "    xTrainingSet = []\n",
    "    yTrainingSet = []\n",
    "    \n",
    "    for index, data in enumerate(wine.data.tolist()):\n",
    "        if data not in xValidationSet:\n",
    "            xTrainingSet.append(data)\n",
    "            yTrainingSet.append(wine.target[index])\n",
    "            \n",
    "    xWineTrainingSets.append(xTrainingSet)\n",
    "    yWineTrainingSets.append(yTrainingSet)\n",
    "    xWineValidationSets.append(xValidationSet)\n",
    "    yWineValidationSets.append(yValidationSet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of y for digits dataset\n",
    "\n",
    "numberOfDigitsTargets = 10\n",
    "numberOfWineTargets = 3\n",
    "\n",
    "for index, fold in enumerate(yDigitsTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yDigitsValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfDigitsTargets)\n",
    "        encoding[y] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yDigitsValidationSets[index] = encodedFold\n",
    "\n",
    "# one-hot encoding of y for wine dataset\n",
    "\n",
    "for index, fold in enumerate(yWineTrainingSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineTrainingSets[index] = encodedFold\n",
    "    \n",
    "for index, fold in enumerate(yWineValidationSets):\n",
    "    encodedFold = []\n",
    "    for i, y in enumerate(fold):\n",
    "        encoding = np.zeros(numberOfWineTargets)\n",
    "        encoding[int(y)-1] = 1\n",
    "        encodedFold.append(encoding.tolist())\n",
    "    yWineValidationSets[index] = encodedFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomIndices(arr, batch_size):\n",
    "    indices = []\n",
    "    \n",
    "    if batch_size > len(arr):\n",
    "        print(\"Error: batch size larger than size of dataset.\")\n",
    "        return\n",
    "    \n",
    "    while batch_size > 0:\n",
    "        x = np.floor(np.random.random() * len(arr))\n",
    "        if x not in indices:\n",
    "            indices.append(int(x))\n",
    "            batch_size -= 1\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent class\n",
    " \n",
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, batch_size, learning_rate=0.5, momentum=0.9, max_iters=20, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iters = max_iters\n",
    "        self.epsilon = epsilon\n",
    "            \n",
    "    def run(self, gradient_fn, x, y, w):\n",
    "        # TODO: use epsilon\n",
    "        \n",
    "        grad = np.inf\n",
    "        t = 1\n",
    "        \n",
    "        while np.linalg.norm(grad) > self.epsilon and t < self.max_iters:\n",
    "            # TODO: implement momentum here\n",
    "            \n",
    "            print(\"gradient descent step:\", t)\n",
    "            \n",
    "            gradients = gradient_fn(x, y, w, self.batch_size)\n",
    "            \n",
    "            for c in range(len(y[0])):\n",
    "                w[c] = w[c] - self.learning_rate * gradients[c]\n",
    "#                 print(\"w for class: \", w[c])\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, add_bias=True):\n",
    "        self.add_bias = add_bias\n",
    "        pass\n",
    "            \n",
    "    def fit(self, x, y, optimizer):\n",
    "        # TODO: add bias\n",
    "        \n",
    "        def gradient(x, y, w, batch_size):\n",
    "            gradients = np.zeros(len(w)).tolist()\n",
    "\n",
    "            indices = getRandomIndices(x, batch_size)\n",
    "\n",
    "            for index in indices:\n",
    "                a = np.asarray(x[index])\n",
    "                b = np.asarray(y[index])\n",
    "                \n",
    "#                 print(\"x:\", a.astype(int))\n",
    "#                 print(\"y:\", b)\n",
    "\n",
    "                for c in range(len(b)):\n",
    "                    w_x =  w[c] @ a\n",
    "                    num = np.exp(w_x)\n",
    "\n",
    "                    den = 0\n",
    "                    for i in range(len(b)):\n",
    "                        w_x =  w[i] @ a\n",
    "                        den += np.exp(w_x)\n",
    "\n",
    "                    yh_c = num/den\n",
    "\n",
    "                    y_c = b[c]\n",
    "                    \n",
    "                    # TODO: may change, see slide 27 of logistic slideshow\n",
    "                    cost_c = np.dot(yh_c - y_c, a)\n",
    "                    \n",
    "                    gradients[c] += cost_c\n",
    "                    \n",
    "#                     print(\"class:\", c)\n",
    "#                     print(\"softmax numerator:\", num)\n",
    "#                     print(\"softmax denominator:\", den)\n",
    "#                     print(\"y hat for class:\", yh_c)\n",
    "#                     print(\"y actual for class:\", y_c)\n",
    "#                     print(\"x gradient:\", cost_c)\n",
    "#                     print(\"new gradient for class:\", gradients[c])\n",
    "\n",
    "            return gradients\n",
    "        \n",
    "        w0 = []\n",
    "        for c in range(len(y[0])):\n",
    "            w0.append(np.zeros(len(x[0])))\n",
    "            \n",
    "        self.w = optimizer.run(gradient, x, y, w0)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        a = np.asarray(x)\n",
    "        b = np.asarray(self.w)\n",
    "        \n",
    "#         if self.add_bias:\n",
    "#             x = np.column_stack([x,np.ones(N)])\n",
    "\n",
    "        yh=[]\n",
    "        for i,x_c in enumerate(a):\n",
    "            yh_x=[]\n",
    "            for c in range(len(b)):\n",
    "                w_x =  b[c] @ x_c\n",
    "                num = np.exp(w_x)\n",
    "\n",
    "                den = 0\n",
    "                for i in range(len(b)):\n",
    "                    w_x =  b[i] @ x_c\n",
    "                    den += np.exp(w_x)\n",
    "\n",
    "                yh_c = num/den\n",
    "                yh_x.append(yh_c)\n",
    "            yh.append(yh_x)\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient descent step: 1\n",
      "gradient descent step: 2\n",
      "gradient descent step: 3\n",
      "gradient descent step: 4\n",
      "gradient descent step: 5\n",
      "gradient descent step: 6\n",
      "gradient descent step: 7\n",
      "gradient descent step: 8\n",
      "gradient descent step: 9\n",
      "gradient descent step: 10\n",
      "gradient descent step: 11\n",
      "gradient descent step: 12\n",
      "gradient descent step: 13\n",
      "gradient descent step: 14\n",
      "gradient descent step: 15\n",
      "gradient descent step: 16\n",
      "gradient descent step: 17\n",
      "gradient descent step: 18\n",
      "gradient descent step: 19\n",
      "[1.565189640712024e-08, 0.9999999843474122, 6.91257132190561e-13] [1.0, 0.0, 0.0]\n",
      "1.3862943502708225\n",
      "[2.3379919434497103e-07, 0.9999997661767702, 2.4035374118972948e-11] [1.0, 0.0, 0.0]\n",
      "1.3862941990626245\n",
      "[4.8928836799608573e-08, 0.9999999510657016, 5.461573387595366e-12] [1.0, 0.0, 0.0]\n",
      "1.3862943272050048\n",
      "[8.577307332455636e-07, 0.9999991422536193, 1.564749463701114e-11] [1.0, 0.0, 0.0]\n",
      "1.3862937665860673\n",
      "[3.0051144050577218e-09, 0.9999999969889753, 5.910278366283938e-12] [1.0, 0.0, 0.0]\n",
      "1.3862943590369041\n",
      "[3.878337732442881e-07, 0.9999996121460796, 2.0147076843440193e-11] [1.0, 0.0, 0.0]\n",
      "1.3862940922939666\n",
      "[1.291659489735627e-07, 0.9999998708199194, 1.4131568131707338e-11] [1.0, 0.0, 0.0]\n",
      "1.386294271588873\n",
      "[5.62743317213751e-08, 0.9999999437137077, 1.1960516725586465e-11] [1.0, 0.0, 0.0]\n",
      "1.3862943221134956\n",
      "[1.2244603770933534e-07, 0.9999998775372806, 1.6681655730603062e-11] [1.0, 0.0, 0.0]\n",
      "1.386294276246761\n",
      "[1.8079465535927825e-07, 0.9999998191935495, 1.1795158794108598e-11] [1.0, 0.0, 0.0]\n",
      "1.3862942358025767\n",
      "[1.5892382513015993e-07, 0.9999998410729061, 3.2688866121995766e-12] [1.0, 0.0, 0.0]\n",
      "1.386294250962283\n",
      "[2.156940174837251e-07, 0.9999997842496319, 5.635068011687768e-11] [1.0, 0.0, 0.0]\n",
      "1.3862942116121788\n",
      "[2.575482626848711e-07, 0.9999997424140417, 3.769576459322614e-11] [1.0, 0.0, 0.0]\n",
      "1.3862941826010218\n",
      "[2.556242550523491e-08, 0.9999999744366013, 9.731611566604507e-13] [1.0, 0.0, 0.0]\n",
      "1.3862943434013673\n",
      "[5.879211796273894e-07, 0.9999994120732845, 5.5359516284209575e-12] [1.0, 0.0, 0.0]\n",
      "1.386293953603896\n",
      "[1.5218368597837745e-07, 0.9999998477882005, 2.8113503807603997e-11] [1.0, 0.0, 0.0]\n",
      "1.386294255634192\n",
      "[7.491412375960472e-08, 0.9999999250658697, 2.000658857622101e-11] [1.0, 0.0, 0.0]\n",
      "1.3862943091933755\n",
      "[2.9140314344866257e-08, 0.9999999708496873, 9.998273617525291e-12] [1.0, 0.0, 0.0]\n",
      "1.3862943409213635\n",
      "[8.626328330018335e-07, 0.9999991373504069, 1.6759989588028906e-11] [1.0, 0.0, 0.0]\n",
      "1.3862937631881884\n",
      "[1.7062615683843918e-07, 0.9999998293219031, 5.1939961126233513e-11] [1.0, 0.0, 0.0]\n",
      "1.3862942428508438\n",
      "[9.439784225890437e-09, 0.9999999905595748, 6.409467707296602e-13] [1.0, 0.0, 0.0]\n",
      "1.386294354576731\n",
      "[2.6828256869417536e-08, 0.9999999731314787, 4.02645469319881e-11] [1.0, 0.0, 0.0]\n",
      "1.3862943425239598\n",
      "[9.965465729754075e-09, 0.9999999900339118, 6.222987944759966e-13] [1.0, 0.0, 0.0]\n",
      "1.3862943542123563\n",
      "[2.055040082488555e-08, 0.999999979445189, 4.4101853832267025e-12] [1.0, 0.0, 0.0]\n",
      "1.386294346875438\n",
      "[2.485270759095078e-09, 0.9999999975141454, 5.838496926166611e-13] [1.0, 0.0, 0.0]\n",
      "1.3862943593972321\n",
      "[1.6485262061343956e-10, 0.9999999998348909, 2.565316819178621e-13] [1.0, 0.0, 0.0]\n",
      "1.3862943610056235\n",
      "[1.249566445200656e-07, 0.9999998750199056, 2.344988423992922e-11] [1.0, 0.0, 0.0]\n",
      "1.386294274506541\n",
      "[4.336766445299126e-07, 0.9999995661485533, 1.7480202939564458e-10] [1.0, 0.0, 0.0]\n",
      "1.3862940605181002\n",
      "[1.4922715233883256e-09, 0.9999999985074075, 3.2085716174381266e-13] [1.0, 0.0, 0.0]\n",
      "1.3862943600855266\n",
      "[4.5177562389686145e-08, 0.9999999548156455, 6.792029252216978e-12] [1.0, 0.0, 0.0]\n",
      "1.3862943298051902\n",
      "[2.36661776637515e-08, 0.9999999763314822, 2.3402208009290445e-12] [1.0, 0.0, 0.0]\n",
      "1.386294344715746\n",
      "[7.903196763711323e-07, 0.9999992096346585, 4.5665088211849166e-11] [1.0, 0.0, 0.0]\n",
      "1.386293813311879\n",
      "[5.6744521450645105e-09, 0.999999994322949, 2.598966406958766e-12] [1.0, 0.0, 0.0]\n",
      "1.3862943571866602\n",
      "[3.64370077660289e-09, 0.999999996354913, 1.3861744016386113e-12] [1.0, 0.0, 0.0]\n",
      "1.3862943585942697\n",
      "[3.658009991174768e-08, 0.999999963400786, 1.911396069492032e-11] [1.0, 0.0, 0.0]\n",
      "1.386294335764497\n",
      "gradient descent step: 1\n",
      "gradient descent step: 2\n",
      "gradient descent step: 3\n",
      "gradient descent step: 4\n",
      "gradient descent step: 5\n",
      "gradient descent step: 6\n",
      "gradient descent step: 7\n",
      "gradient descent step: 8\n",
      "gradient descent step: 9\n",
      "gradient descent step: 10\n",
      "gradient descent step: 11\n",
      "gradient descent step: 12\n",
      "gradient descent step: 13\n",
      "gradient descent step: 14\n",
      "gradient descent step: 15\n",
      "gradient descent step: 16\n",
      "gradient descent step: 17\n",
      "gradient descent step: 18\n",
      "gradient descent step: 19\n",
      "[4.865118145847235e-07, 0.9890712242098846, 0.010928289278300871] [1.0, 0.0, 0.0]\n",
      "1.3862940238955388\n",
      "[1.0208303578426505e-06, 0.9328644288617803, 0.06713455030786175] [1.0, 0.0, 0.0]\n",
      "1.3862936535339458\n",
      "[2.6389340922094787e-06, 0.9060870465918709, 0.09391031447403672] [1.0, 0.0, 0.0]\n",
      "1.3862925319484238\n",
      "[4.8700717564482335e-06, 0.9762172061219251, 0.02377792380631849] [1.0, 0.0, 0.0]\n",
      "1.386290985437454\n",
      "[2.918273634634742e-07, 0.950791707115138, 0.04920800105749857] [1.0, 0.0, 0.0]\n",
      "1.386294158840555\n",
      "[5.988660816236297e-07, 0.9843992362043196, 0.015600164929598828] [1.0, 0.0, 0.0]\n",
      "1.386293946017465\n",
      "[5.383087627513568e-07, 0.781204863934864, 0.2187945977563733] [1.0, 0.0, 0.0]\n",
      "1.3862939879926168\n",
      "[5.666326519066095e-06, 0.9915544632872917, 0.008439870386189362] [1.0, 0.0, 0.0]\n",
      "1.386290433513613\n",
      "[1.1138508360545421e-07, 0.7930186757310265, 0.2069812128838901] [1.0, 0.0, 0.0]\n",
      "1.3862942839136307\n",
      "[1.419380707100762e-06, 0.9869838137964206, 0.013014766822872243] [1.0, 0.0, 0.0]\n",
      "1.3862933772796517\n",
      "[1.9221649639125368e-07, 0.5847497156441389, 0.4152500921393648] [1.0, 0.0, 0.0]\n",
      "1.3862942278855588\n",
      "[4.394655958829499e-07, 0.9690141378682812, 0.030985422666123044] [1.0, 0.0, 0.0]\n",
      "1.3862940565055037\n",
      "[4.501659708674917e-06, 0.9872778426006168, 0.012717655739674597] [1.0, 0.0, 0.0]\n",
      "1.3862912408020893\n",
      "[9.239346637936816e-07, 0.8900371913099347, 0.10996188475540139] [1.0, 0.0, 0.0]\n",
      "1.3862937206969699\n",
      "[5.574999199093163e-06, 0.7683784037795922, 0.2316160212212086] [1.0, 0.0, 0.0]\n",
      "1.386290496817144\n",
      "[3.0349860806472625e-05, 0.9711096390654478, 0.02886001107374568] [1.0, 0.0, 0.0]\n",
      "1.38627332396916\n",
      "[3.149300277092963e-06, 0.9855657889939287, 0.014431061705794185] [1.0, 0.0, 0.0]\n",
      "1.3862921781888031\n",
      "[6.195534284708336e-06, 0.9659823122661279, 0.034011492199587615] [1.0, 0.0, 0.0]\n",
      "1.386290066693173\n",
      "[2.028977794710429e-06, 0.7208262601398923, 0.279171710882313] [1.0, 0.0, 0.0]\n",
      "1.3862929547386236\n",
      "[5.722867364015877e-06, 0.9424760852659513, 0.05751819186668483] [1.0, 0.0, 0.0]\n",
      "1.3862903943223248\n",
      "[1.6760911987609742e-06, 0.9562097468995499, 0.04378857700925129] [1.0, 0.0, 0.0]\n",
      "1.3862931993412995\n",
      "[1.9797283986552517e-06, 0.9645248514164878, 0.03547316885511345] [1.0, 0.0, 0.0]\n",
      "1.386292988875753\n",
      "[4.181217133402487e-06, 0.8838093671625858, 0.11618645162028073] [1.0, 0.0, 0.0]\n",
      "1.3862914629166525\n",
      "[1.8687485916921686e-05, 0.9583427900940922, 0.04163852241999093] [1.0, 0.0, 0.0]\n",
      "1.3862814078544092\n",
      "[1.6584014986668715e-05, 0.8191316008402262, 0.1808518151447871] [0.0, 1.0, 0.0]\n",
      "0.5794181198032197\n",
      "[1.3806535725932505e-07, 0.26108753397263607, 0.7389123279620066] [0.0, 1.0, 0.0]\n",
      "1.1860140193814879\n",
      "[8.234436922805458e-08, 0.07862533368260294, 0.9213745839730277] [0.0, 1.0, 0.0]\n",
      "1.3301888075354027\n",
      "[9.86546318277138e-07, 0.9246149422691302, 0.0753840711845515] [0.0, 1.0, 0.0]\n",
      "0.42898932347228386\n",
      "[4.6491425926845094e-07, 0.9993612221290074, 0.0006383129567334655] [0.0, 1.0, 0.0]\n",
      "0.3142721056945844\n",
      "[5.2521480316381945e-08, 0.9528116726133686, 0.04718827486515113] [0.0, 1.0, 0.0]\n",
      "0.38652621932793024\n",
      "[4.814667555279662e-07, 0.9682502634481654, 0.031749255085079045] [0.0, 1.0, 0.0]\n",
      "0.36286202056645506\n",
      "[1.5552661979402193e-06, 0.9986199010173997, 0.0013785437164024249] [0.0, 1.0, 0.0]\n",
      "0.31544407953038867\n",
      "[5.99667132157461e-07, 0.9887392651443896, 0.011260135188478316] [0.0, 1.0, 0.0]\n",
      "0.33099838377231533\n",
      "[2.975457057674494e-07, 0.42893117903431416, 0.5710685234199802] [0.0, 1.0, 0.0]\n",
      "1.0328483579954755\n",
      "[9.228858283278811e-07, 0.996562096803888, 0.0034369803102836994] [0.0, 1.0, 0.0]\n",
      "0.318693674638873\n",
      "gradient descent step: 1\n",
      "gradient descent step: 2\n",
      "gradient descent step: 3\n",
      "gradient descent step: 4\n",
      "gradient descent step: 5\n",
      "gradient descent step: 6\n",
      "gradient descent step: 7\n",
      "gradient descent step: 8\n",
      "gradient descent step: 9\n",
      "gradient descent step: 10\n",
      "gradient descent step: 11\n",
      "gradient descent step: 12\n",
      "gradient descent step: 13\n",
      "gradient descent step: 14\n",
      "gradient descent step: 15\n",
      "gradient descent step: 16\n",
      "gradient descent step: 17\n",
      "gradient descent step: 18\n",
      "gradient descent step: 19\n",
      "[0.7065927983737369, 0.05162052976132653, 0.2417866718649366] [0.0, 1.0, 0.0]\n",
      "1.34983030062343\n",
      "[0.9747732553885142, 0.025109550919400248, 0.00011719369208564877] [0.0, 1.0, 0.0]\n",
      "1.3687301413058202\n",
      "[0.8024608287881184, 0.1654079768095804, 0.03213119440230121] [0.0, 1.0, 0.0]\n",
      "1.2642295079506716\n",
      "[0.999061125414951, 0.0009283778269461267, 1.049675810283602e-05] [0.0, 1.0, 0.0]\n",
      "1.385650643075274\n",
      "[0.9996741875828618, 0.0003185787294262809, 7.233687712045297e-06] [0.0, 1.0, 0.0]\n",
      "1.386073513794659\n",
      "[0.8057767351411074, 0.13592013822003346, 0.05830312663885916] [0.0, 1.0, 0.0]\n",
      "1.287145941621324\n",
      "[0.9800326396200186, 0.01850177365886556, 0.0014655867211157867] [0.0, 1.0, 0.0]\n",
      "1.3733835370756684\n",
      "[0.36879785852290264, 0.3199639056142102, 0.31123823586288724] [0.0, 1.0, 0.0]\n",
      "1.1347323528408748\n",
      "[0.996162121172734, 0.0033096328448758133, 0.000528245982390151] [0.0, 1.0, 0.0]\n",
      "1.3839975554945585\n",
      "[0.7686573492883823, 0.22007044636539705, 0.011272204346220642] [0.0, 1.0, 0.0]\n",
      "1.2202914478425049\n",
      "[0.868443234717063, 0.13116124098374018, 0.00039552429919697423] [0.0, 1.0, 0.0]\n",
      "1.2907945712853028\n",
      "[0.9968020632280554, 0.0030253149012524775, 0.00017262187069210923] [0.0, 1.0, 0.0]\n",
      "1.3841950810311865\n",
      "[0.7349216426530393, 0.2539291148616779, 0.011149242485282804] [0.0, 1.0, 0.0]\n",
      "1.1920797951269222\n",
      "[0.01491860871744701, 0.0675893160919249, 0.917492075190628] [0.0, 1.0, 0.0]\n",
      "1.338264132175932\n",
      "[0.9910070356512394, 0.008791446117949854, 0.0002015182308108492] [0.0, 1.0, 0.0]\n",
      "1.3801811876511891\n",
      "[0.9466667192358843, 0.052639775764891206, 0.0006935049992245779] [0.0, 1.0, 0.0]\n",
      "1.3490962018568626\n",
      "[0.4865155575993823, 0.46785867788686086, 0.04562576451375667] [0.0, 1.0, 0.0]\n",
      "0.9940983444587308\n",
      "[0.5512953131621929, 0.43169026433595603, 0.01701442250185109] [0.0, 1.0, 0.0]\n",
      "1.030144116577171\n",
      "[0.69243873796883, 0.24424300848885278, 0.06331825354231721] [0.0, 1.0, 0.0]\n",
      "1.200230654078652\n",
      "[0.7216796572657606, 0.2714625694094845, 0.006857773324754787] [0.0, 1.0, 0.0]\n",
      "1.1771586390401936\n",
      "[0.34313131104843814, 0.5269420810729459, 0.12992660787861593] [0.0, 1.0, 0.0]\n",
      "0.9327587908783745\n",
      "[0.2265114579492577, 0.5799189856951058, 0.19356955635563652] [0.0, 1.0, 0.0]\n",
      "0.8750516221981688\n",
      "[0.1388997990299142, 0.5315485672785636, 0.32955163369152213] [0.0, 1.0, 0.0]\n",
      "0.9278444733157295\n",
      "[0.9215218026794907, 0.07742788950677862, 0.0010503078137307819] [0.0, 1.0, 0.0]\n",
      "1.3310682852655416\n",
      "[0.981911961334313, 0.017799113189969514, 0.0002889254757174522] [0.0, 1.0, 0.0]\n",
      "1.3738770479884002\n",
      "[0.9992363376108169, 0.0007255933064549153, 3.806908272835358e-05] [0.0, 1.0, 0.0]\n",
      "1.385791286496122\n",
      "[0.9090074174621464, 0.043511229254478145, 0.047481353283375514] [0.0, 1.0, 0.0]\n",
      "1.3556510348723414\n",
      "[0.9822219358797573, 0.017574660727589727, 0.0002034033926530121] [0.0, 1.0, 0.0]\n",
      "1.3740346378945199\n",
      "[0.999602555296549, 0.0003949047008215807, 2.540002629465918e-06] [0.0, 1.0, 0.0]\n",
      "1.3860205950447972\n",
      "[0.9329764724153722, 0.06646466230209358, 0.0005588652825342716] [0.0, 1.0, 0.0]\n",
      "1.3390832821569192\n",
      "[0.9951854344795495, 0.004501405589673991, 0.00031315993077659835] [0.0, 1.0, 0.0]\n",
      "1.3831691474582113\n",
      "[0.904602218165805, 0.08369066263068087, 0.011707119203514133] [0.0, 1.0, 0.0]\n",
      "1.3264596206150632\n",
      "[0.8354025560783328, 0.1560007224408106, 0.008596721480856518] [0.0, 1.0, 0.0]\n",
      "1.271598606379201\n",
      "[0.719021196047308, 0.2660347241581683, 0.014944079794523751] [0.0, 1.0, 0.0]\n",
      "1.181800940489225\n",
      "[0.9766325048622774, 0.022530687386006755, 0.0008368077517160234] [0.0, 1.0, 0.0]\n",
      "1.3705489383978553\n",
      "gradient descent step: 1\n",
      "gradient descent step: 2\n",
      "gradient descent step: 3\n",
      "gradient descent step: 4\n",
      "gradient descent step: 5\n",
      "gradient descent step: 6\n",
      "gradient descent step: 7\n",
      "gradient descent step: 8\n",
      "gradient descent step: 9\n",
      "gradient descent step: 10\n",
      "gradient descent step: 11\n",
      "gradient descent step: 12\n",
      "gradient descent step: 13\n",
      "gradient descent step: 14\n",
      "gradient descent step: 15\n",
      "gradient descent step: 16\n",
      "gradient descent step: 17\n",
      "gradient descent step: 18\n",
      "gradient descent step: 19\n",
      "[0.4094863535419138, 1.2707069531863586e-05, 0.5905009393885542] [0.0, 1.0, 0.0]\n",
      "1.3862855532101037\n",
      "[0.9935809425647069, 1.6674259962212526e-06, 0.0064173900092969315] [0.0, 1.0, 0.0]\n",
      "1.3862932053475676\n",
      "[0.6940624264430183, 8.996807120901792e-06, 0.3059285767498609] [0.0, 1.0, 0.0]\n",
      "1.386288124988165\n",
      "[0.9964990573664195, 2.8309330059750074e-06, 0.003498111700574522] [0.0, 1.0, 0.0]\n",
      "1.3862923988646556\n",
      "[0.9999810690805125, 1.3887839055121154e-08, 1.89170316483545e-05] [0.0, 1.0, 0.0]\n",
      "1.386294351493574\n",
      "[0.9998185021136398, 3.562384515628333e-09, 0.00018149432397573568] [0.0, 1.0, 0.0]\n",
      "1.386294358650634\n",
      "[0.9958741138656817, 1.4877466494675237e-06, 0.004124398387668828] [0.0, 1.0, 0.0]\n",
      "1.3862933298919418\n",
      "[0.6965722861943398, 1.5360220373619094e-06, 0.303426177783623] [0.0, 1.0, 0.0]\n",
      "1.3862932964299564\n",
      "[0.9921890157431767, 5.438516775209124e-06, 0.007805545740047958] [0.0, 1.0, 0.0]\n",
      "1.386290591419927\n",
      "[0.9931424162166627, 2.8313395035922236e-06, 0.006854752443833747] [0.0, 1.0, 0.0]\n",
      "1.386292398582892\n",
      "[0.9983924622975764, 1.1185751674974057e-05, 0.001596351950748477] [0.0, 1.0, 0.0]\n",
      "1.3862866077163742\n",
      "[0.9974461709983224, 4.288253825278645e-06, 0.002549540747852369] [0.0, 1.0, 0.0]\n",
      "1.3862913887242447\n",
      "[0.9932751903711298, 5.583365465183356e-06, 0.0067192262634050105] [0.0, 1.0, 0.0]\n",
      "1.386290491018067\n",
      "[0.16749503704457208, 2.262409510700807e-06, 0.8325027005459172] [0.0, 1.0, 0.0]\n",
      "1.3862927929358373\n",
      "[0.9842462232015257, 1.1478902395246845e-06, 0.015752628908234735] [0.0, 1.0, 0.0]\n",
      "1.3862935654626782\n",
      "[0.9998012704859961, 1.789731330443518e-08, 0.00019871161669051678] [0.0, 1.0, 0.0]\n",
      "1.3862943487144181\n",
      "[0.9999080913567869, 1.210788131912546e-09, 9.190743242498413e-05] [0.0, 1.0, 0.0]\n",
      "1.3862943602806364\n",
      "[0.7459619961419431, 7.41548583873005e-07, 0.25403726230947293] [0.0, 1.0, 0.0]\n",
      "1.3862938471174429\n",
      "[0.9660270404604783, 3.0119673216655225e-08, 0.03397292941984851] [0.0, 1.0, 0.0]\n",
      "1.3862943402425238\n",
      "[0.9997755423430792, 5.135075112204666e-09, 0.00022445252184573778] [0.0, 1.0, 0.0]\n",
      "1.3862943575605278\n",
      "[0.9979070537893577, 6.450897019342314e-07, 0.0020923011209404573] [0.0, 1.0, 0.0]\n",
      "1.3862939139776786\n",
      "[0.9981090136232822, 2.121831835617363e-07, 0.0018907741935343602] [0.0, 1.0, 0.0]\n",
      "1.3862942140457042\n",
      "[0.667770793960048, 4.509729127816148e-06, 0.33222469631082424] [0.0, 1.0, 0.0]\n",
      "1.3862912352087762\n",
      "[0.9898115008984134, 3.887182662279977e-06, 0.01018461191892432] [0.0, 1.0, 0.0]\n",
      "1.3862916667264105\n",
      "[0.6564650767767991, 5.223460640961465e-07, 0.34353440087713677] [0.0, 1.0, 0.0]\n",
      "1.3862939990571208\n",
      "[0.822707417989247, 1.888560378377444e-06, 0.17729069345037468] [0.0, 0.0, 1.0]\n",
      "1.2548418849399272\n",
      "[0.14036729463264802, 3.064885534223896e-07, 0.8596323988787985] [0.0, 0.0, 1.0]\n",
      "0.5232035025661359\n",
      "[0.05275968094293919, 3.6561166457628183e-07, 0.9472399534453962] [0.0, 0.0, 1.0]\n",
      "0.3949942192400228\n",
      "[0.27078231421405446, 2.0545578344334733e-07, 0.7292174803301622] [0.0, 0.0, 1.0]\n",
      "0.6976211148617752\n",
      "[0.08614412988069221, 1.1984582450914964e-06, 0.9138546716610627] [0.0, 0.0, 1.0]\n",
      "0.44493869244453466\n",
      "[0.006314571221208929, 3.760087615867283e-08, 0.9936853911779149] [0.0, 0.0, 1.0]\n",
      "0.32322747925729517\n",
      "[0.0013128637904983897, 1.310248344921034e-08, 0.9986871231070181] [0.0, 0.0, 1.0]\n",
      "0.3153378350353127\n",
      "[0.00020855280889579345, 1.951302166611316e-09, 0.999791445239802] [0.0, 0.0, 1.0]\n",
      "0.3135916359875801\n",
      "[0.0036299766163151947, 4.1755499928116e-08, 0.9963699816281848] [0.0, 0.0, 1.0]\n",
      "0.3189967814981009\n",
      "[0.04091970880992115, 2.5512027803821614e-07, 0.9590800360698009] [0.0, 0.0, 1.0]\n",
      "0.37695368454921097\n",
      "gradient descent step: 1\n",
      "gradient descent step: 2\n",
      "gradient descent step: 3\n",
      "gradient descent step: 4\n",
      "gradient descent step: 5\n",
      "gradient descent step: 6\n",
      "gradient descent step: 7\n",
      "gradient descent step: 8\n",
      "gradient descent step: 9\n",
      "gradient descent step: 10\n",
      "gradient descent step: 11\n",
      "gradient descent step: 12\n",
      "gradient descent step: 13\n",
      "gradient descent step: 14\n",
      "gradient descent step: 15\n",
      "gradient descent step: 16\n",
      "gradient descent step: 17\n",
      "gradient descent step: 18\n",
      "gradient descent step: 19\n",
      "[0.07192037702783984, 0.08178152335377355, 0.8462980996183865] [0.0, 0.0, 1.0]\n",
      "0.5419203357078721\n",
      "[0.486768718977164, 0.013564486921986895, 0.49966679410084913] [0.0, 0.0, 1.0]\n",
      "0.9614620901492347\n",
      "[0.03192475703502748, 0.12611150706783864, 0.8419637358971339] [0.0, 0.0, 1.0]\n",
      "0.5479599179411757\n",
      "[0.16013853617438695, 0.03764192697548399, 0.8022195368501291] [0.0, 0.0, 1.0]\n",
      "0.6023369016248914\n",
      "[0.8529184349561071, 0.00048250950508198025, 0.1465990555388109] [0.0, 0.0, 1.0]\n",
      "1.2789085409396783\n",
      "[0.3186805422049102, 0.01637893685739529, 0.6649405209376946] [0.0, 0.0, 1.0]\n",
      "0.776785572163265\n",
      "[0.030502463187455274, 0.040162714662011795, 0.929334822150533] [0.0, 0.0, 1.0]\n",
      "0.42194902803695294\n",
      "[0.17192256116801222, 0.008798431787734469, 0.8192790070442534] [0.0, 0.0, 1.0]\n",
      "0.5792169339458634\n",
      "[0.38602889758925096, 0.005762891044945434, 0.6082082113658036] [0.0, 0.0, 1.0]\n",
      "0.8431457729693624\n",
      "[0.5026825561873091, 0.0030507514633060727, 0.49426669234938475] [0.0, 0.0, 1.0]\n",
      "0.9670656983640331\n",
      "[0.651442831039867, 0.004226246144789298, 0.34433092281534383] [0.0, 0.0, 1.0]\n",
      "1.1127579885495356\n",
      "[0.7601733760397937, 0.0014705772922648983, 0.23835604666794136] [0.0, 0.0, 1.0]\n",
      "1.205152900888994\n",
      "[0.5898446997157253, 0.018337949871416834, 0.3918173504128579] [0.0, 0.0, 1.0]\n",
      "1.0686126318054234\n",
      "[0.34529811604397065, 0.0025662739752515124, 0.6521356099807779] [0.0, 0.0, 1.0]\n",
      "0.7920457777811873\n",
      "[0.22869975662666778, 0.022618182408823566, 0.7486820609645086] [0.0, 0.0, 1.0]\n",
      "0.6727851853321984\n",
      "[0.09814767853409957, 0.006224047799669524, 0.8956282736662309] [0.0, 0.0, 1.0]\n",
      "0.4716366716816248\n",
      "[0.26571066514945824, 0.007001906467361792, 0.72728742838318] [0.0, 0.0, 1.0]\n",
      "0.7000615848966426\n",
      "[0.09051271463725342, 0.015574201952513954, 0.8939130834102326] [0.0, 0.0, 1.0]\n",
      "0.4741286128979538\n",
      "[0.85409146924411, 0.0005643983723934497, 0.1453441323834967] [0.0, 0.0, 1.0]\n",
      "1.2798801538227278\n",
      "[0.7588840240370472, 0.0014163001194679396, 0.23969967584348476] [0.0, 0.0, 1.0]\n",
      "1.2040315512724284\n",
      "[0.22377753932501354, 0.012788675672095032, 0.7634337850028914] [0.0, 0.0, 1.0]\n",
      "0.6536891221655733\n",
      "[0.17713711957635436, 0.023148628787779072, 0.7997142516358666] [0.0, 0.0, 1.0]\n",
      "0.6057045508505652\n",
      "[0.043217690655907984, 0.07156491792087631, 0.8852173914232156] [0.0, 0.0, 1.0]\n",
      "0.4867084277872138\n",
      "[0.3250142470405016, 0.014975553770298484, 0.6600101991892] [0.0, 0.0, 1.0]\n",
      "0.7826810024814785\n",
      "[0.3159805138231039, 0.007405456144154414, 0.6766140300327416] [0.0, 0.0, 1.0]\n",
      "0.762727833236027\n",
      "[0.04222216509448762, 0.03713962581195389, 0.9206382090935584] [0.0, 0.0, 1.0]\n",
      "0.43490013884095546\n",
      "[0.4829880620872082, 0.0022205055363529215, 0.514791432376439] [0.0, 0.0, 1.0]\n",
      "0.9456285971069288\n",
      "[0.524023001607375, 0.002133392562836163, 0.4738436058297888] [0.0, 0.0, 1.0]\n",
      "0.9880253610517046\n",
      "[0.524648216616506, 0.005531119221048498, 0.4698206641624455] [0.0, 0.0, 1.0]\n",
      "0.9921109213818329\n",
      "[0.6077087850106443, 0.0027640568191782244, 0.38952715817017747] [0.0, 0.0, 1.0]\n",
      "1.070782741856502\n",
      "[0.14434577448796973, 0.04408316503539007, 0.8115710604766401] [0.0, 0.0, 1.0]\n",
      "0.5897039589287851\n",
      "[0.13523458980979003, 0.013083493002637357, 0.8516819171875727] [0.0, 0.0, 1.0]\n",
      "0.5343881454838393\n",
      "[0.4866466541379404, 0.003416477425987815, 0.5099368684360718] [0.0, 0.0, 1.0]\n",
      "0.9507330893048438\n",
      "[0.17164763492000246, 0.0038843457086366784, 0.8244680193713609] [0.0, 0.0, 1.0]\n",
      "0.5721189976199826\n",
      "[0.37638226746933134, 0.00617078360001425, 0.6174469489306543] [0.0, 0.0, 1.0]\n",
      "0.8325573320745799\n"
     ]
    }
   ],
   "source": [
    "def cost(yh, y):\n",
    "    return y * np.log1p(np.exp(-yh)) + (1-yh) * np.log1p(np.exp(yh))\n",
    "\n",
    "for fold_index, fold in enumerate(xWineTrainingSets):\n",
    "    gradientDescentModel = GradientDescent(10)\n",
    "    logisticRegressionModel = LogisticRegression(False)\n",
    "    \n",
    "    logisticRegressionModel.fit(fold, yWineTrainingSets[fold_index], gradientDescentModel)\n",
    "    yh = logisticRegressionModel.predict(xWineValidationSets[fold_index])\n",
    "    \n",
    "    for sample_index, yh_x in enumerate(yh):\n",
    "        c = np.argmax(yWineValidationSets[fold_index][sample_index])\n",
    "        cst = cost(yh_x[c], yWineValidationSets[fold_index][sample_index][c])\n",
    "        print(yh_x, yWineValidationSets[fold_index][sample_index])\n",
    "        print(cst)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#xWineTrainingSets = []\n",
    "#yWineTrainingSets = []\n",
    "#xWineValidationSets = []\n",
    "#yWineValidationSets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
